[{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":null,"dir":"","previous_headings":"","what":"CLAUDE.md","title":"CLAUDE.md","text":"file provides guidance Claude Code (claude.ai/code) working code repository.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":"repository-overview","dir":"","previous_headings":"","what":"Repository Overview","title":"CLAUDE.md","text":"fmrilss R package implementing efficient Least Squares Separate (LSS) analysis functional magnetic resonance imaging (fMRI) data. LSS used estimate trial--trial activation patterns event-related fMRI designs, critical multivariate pattern analysis (MVPA) connectivity studies. package provides multiple backends, R implementations highly optimized C++ OpenMP parallelization.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":"lss-implementations","dir":"","previous_headings":"Core Architecture","what":"LSS Implementations","title":"CLAUDE.md","text":"package provides multiple implementations LSS algorithm, accessible unified interface: - r_optimized: Optimized R implementation (default, recommended) - cpp_optimized: Parallelized C++ using OpenMP (fastest large datasets) - r_vectorized: Standard vectorized R - cpp: Standard C++ implementation - naive: Simple loop-based R (reference implementation testing) - oasis: Mathematically equivalent reformulation ridge regularization support Key design pattern: methods use lss(Y, X, Z, Nuisance) signature : - Y: data matrix (timepoints x voxels) - X: trial design matrix (one column per trial) - Z: experimental regressors (intercept, trends, blocks) - Nuisance: regressors project (motion, physiology)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":"advanced-features","dir":"","previous_headings":"Core Architecture","what":"Advanced Features","title":"CLAUDE.md","text":"OASIS Method: Reformulates LSS single matrix operation, eliminating per-trial GLM redundancy. Supports: - Multi-basis HRF models (K > 1) - Ridge regularization (absolute fractional modes) - Automatic design construction event onsets via fmrihrf - Standard error computation - AR(1) prewhitening Voxel-wise HRF Estimation: estimate_voxel_hrf() fits per-voxel HRF basis coefficients, enabling data-driven HRF modeling LSS. Shared-Basis HRF Models (SBHM): Learn low-rank shared time bases parameterized HRF libraries via SVD, enabling efficient multi-voxel HRF estimation reduced parameters. Prewhitening: Integration fmriAR package AR/ARMA noise modeling flexible pooling strategies (global, voxel-wise, run-aware, parcel-based).","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":"essential-commands","dir":"","previous_headings":"","what":"Essential Commands","title":"CLAUDE.md","text":"","code":"# Build and install package devtools::build() devtools::install()  # Run tests devtools::test()                    # Run all tests devtools::test_active_file()       # Test current file testthat::test_file(\"tests/testthat/test-lss-equivalence.R\")  # Single test file  # Check package devtools::check()                  # Full R CMD check devtools::check_examples()         # Check examples only  # Documentation devtools::document()               # Update roxygen documentation pkgdown::build_site()             # Build package website  # Load for development devtools::load_all()              # Load package without installing"},{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":"c-compilation","dir":"","previous_headings":"","what":"C++ Compilation","title":"CLAUDE.md","text":"package uses Rcpp Armadillo matrix operations. OpenMP configured via src/Makevars: - Automatic OpenMP detection configuration - Falls back gracefully OpenMP unavailable - Links: Rcpp, RcppArmadillo, roptim, bigmemory, BH","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":"testing-strategy","dir":"","previous_headings":"","what":"Testing Strategy","title":"CLAUDE.md","text":"Tests verify: 1. Mathematical equivalence across implementations 2. Correctness ground truth 3. Edge cases (missing data, singular matrices) 4. HRF convolution integration 5. OASIS deconvolution methods Test files follow pattern test-{feature}.R tests/testthat/.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":"key-dependencies","dir":"","previous_headings":"","what":"Key Dependencies","title":"CLAUDE.md","text":"fmrihrf: HRF modeling convolution (GitHub: bbuchsbaum/fmrihrf) fmriAR: AR/ARMA prewhitening (GitHub: bbuchsbaum/fmriAR) RcppArmadillo: Matrix operations C++ roptim: Optimization routines HRF parameter estimation MASS: Statistical functions (used mixed models) bigmemory: Memory-efficient matrix operations","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/CLAUDE.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"CLAUDE.md","text":"Five main vignettes demonstrate package usage: - getting_started.Rmd: Basic LSS concepts usage - oasis_method.Rmd: OASIS method ridge regularization - oasis_theory.Rmd: Mathematical foundations OASIS - voxel-wise-hrf.Rmd: Voxel-wise HRF estimation integration LSS - sbhm.Rmd: Shared-Basis HRF Matching efficient voxel-specific HRF estimation","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"the-challenge-of-event-related-fmri-analysis","dir":"Articles","previous_headings":"","what":"The Challenge of Event-Related fMRI Analysis","title":"Getting Started with fmrilss","text":"event-related fMRI experiments rapid stimulus presentation, hemodynamic responses consecutive trials overlap substantially due slow temporal dynamics BOLD signal (10-15 seconds). Traditional general linear models (GLMs) estimate trials simultaneously suffer collinearity inter-stimulus intervals short (< 4 seconds), resulting unstable trial-specific activation estimates. unstable estimates compromise downstream analyses including multivariate pattern analysis (MVPA), representational similarity analysis (RSA), trial--trial connectivity methods. Least Squares Separate (LSS) approach (Mumford et al., 2012) addresses collinearity problem fitting separate GLM trial. model, one regressor represents trial interest second regressor aggregates trials, reducing collinearity improving estimate stability. fmrilss package provides optimized implementations LSS extensions including automatic HRF estimation, ridge regularization, parallel computation across multiple CPU cores.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"understanding-the-lss-approach","dir":"Articles","previous_headings":"","what":"Understanding the LSS Approach","title":"Getting Started with fmrilss","text":"vignette assumes familiarity fMRI general linear models, design matrices, hemodynamic response functions, R matrix operations. Standard GLM analysis (Least Squares , LSA) estimates trial coefficients simultaneously solving: beta = (X’X)^(-1)X’Y. trials occur rapid succession, design matrix columns exhibit high correlation (condition number > 30), producing unstable trial estimates inflated variance. LSS reformulates problem fitting N separate models, N number trials. trial , design matrix contains: (1) regressor trial , (2) single regressor aggregating trials, (3) nuisance baseline regressors. reduces effective condition number produces stable single-trial estimates. Mathematically, trial : beta_i = (X_i’X_i)^(-1)X_i’Y X_i = [x_i | Σ(j≠) x_j | Z | N], x_i trial regressor, Z experimental covariates, N nuisance regressors. Beyond two core regressors, LSS models can include experimental regressors capture session-wide effects like linear trends block effects, want model don’t need trial-specific estimates . model can also incorporate nuisance regressors motion parameters physiological noise, projected analysis begins.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"your-first-lss-analysis","dir":"Articles","previous_headings":"","what":"Your First LSS Analysis","title":"Getting Started with fmrilss","text":"following example demonstrates LSS analysis using synthetic data rapid event-related design.","code":"library(fmrihrf) library(fmrilss)  set.seed(42) n_timepoints <- 150 n_trials <- 12 n_voxels <- 25"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"creating-the-experimental-design","dir":"Articles","previous_headings":"Your First LSS Analysis","what":"Creating the Experimental Design","title":"Getting Started with fmrilss","text":"design matrix encodes stimulus timing expected hemodynamic responses. ’ll create proper event-related design convolving trial onsets HRF. Matrix X contains HRF-convolved trial regressors. Matrix Z contains experimental covariates (intercept linear trend). nuisance matrix models unwanted variance (e.g., motion parameters).","code":"# Define trial onsets (in TRs) onsets <- round(seq(from = 10, to = n_timepoints - 12, length.out = n_trials))  # Create HRF-convolved design matrix using a simple gamma HRF # In real analyses, use fmrihrf package for more sophisticated HRF models simple_hrf <- function(t) {   # Gamma function parameters for a canonical HRF shape   ifelse(t >= 0, dgamma(t, shape = 6, rate = 1), 0) }  # Build trial design matrix (X) with HRF convolution X <- matrix(0, n_timepoints, n_trials) hrf_kernel <- simple_hrf(0:20)  # HRF spanning ~20 TRs for(i in 1:n_trials) {   onset <- onsets[i]   hrf_length <- min(length(hrf_kernel), n_timepoints - onset + 1)   X[onset:(onset + hrf_length - 1), i] <- hrf_kernel[1:hrf_length] }  # Experimental regressors (Z) - intercept and linear trend # These are regressors we want to model but not trial-wise Z <- cbind(Intercept = 1, LinearTrend = as.vector(scale(1:n_timepoints, center = TRUE, scale = FALSE)))  # Nuisance regressors - e.g., 6 motion parameters Nuisance <- matrix(rnorm(n_timepoints * 6), n_timepoints, 6)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"visualizing-regressors","dir":"Articles","previous_headings":"Your First LSS Analysis","what":"Visualizing Regressors","title":"Getting Started with fmrilss","text":"following visualizations show experimental regressors (excluding intercept) nuisance regressors, standardized comparability.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"single-trial-regressors","dir":"Articles","previous_headings":"Your First LSS Analysis","what":"Single-Trial Regressors","title":"Getting Started with fmrilss","text":"Trial regressor structure visualized time x trial heatmap:","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"generating-realistic-data","dir":"Articles","previous_headings":"Your First LSS Analysis","what":"Generating Realistic Data","title":"Getting Started with fmrilss","text":"Now ’ll create synthetic fMRI data includes contributions components, plus noise make realistic:","code":"# Simulate effects for each component true_trial_betas <- matrix(rnorm(n_trials * n_voxels, 0, 1.2), n_trials, n_voxels) true_fixed_effects <- matrix(rnorm(2 * n_voxels, c(5, -0.1), 0.5), 2, n_voxels) true_nuisance_effects <- matrix(rnorm(6 * n_voxels, 0, 2), 6, n_voxels)  # Combine signals and add noise Y <- (Z %*% true_fixed_effects) +      (X %*% true_trial_betas) +      (Nuisance %*% true_nuisance_effects) +      matrix(rnorm(n_timepoints * n_voxels, 0, 1), n_timepoints, n_voxels)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"running-the-analysis","dir":"Articles","previous_headings":"Your First LSS Analysis","what":"Running the Analysis","title":"Getting Started with fmrilss","text":"lss() function provides clean, modern interface adapts needs. simplest, can provide just data trial matrix, function automatically includes intercept term: complete analysis accounts experimental effects removes nuisance signals, include Z Nuisance matrices. function handles nuisance regression efficiently, projecting signals data design matrix estimating trial-specific betas:","code":"beta_basic <- lss(Y, X) # The result is a trials-by-voxels matrix dim(beta_basic) #> [1] 12 25 beta_full <- lss(Y, X, Z = Z, Nuisance = Nuisance) # The output dimensions remain the same dim(beta_full) #> [1] 12 25"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"handling-temporal-autocorrelation-with-prewhitening","dir":"Articles","previous_headings":"","what":"Handling Temporal Autocorrelation with Prewhitening","title":"Getting Started with fmrilss","text":"fMRI time series exhibit temporal autocorrelation can affect statistical inference. fmrilss package now integrates fmriAR provide sophisticated prewhitening capabilities: advanced noise modeling, can use voxel-specific parameters ARMA models: Pooling strategies: - pooling = \"global\" (default): Single AR model voxels (fast) - pooling = \"voxel\": Per-voxel AR models (accurate slow) - pooling = \"run\": Separate model per run (multi-run data)","code":"# Basic AR(1) prewhitening beta_ar1 <- lss(Y, X, Z = Z, Nuisance = Nuisance,                 prewhiten = list(method = \"ar\", p = 1))  # Automatic AR order selection beta_auto <- lss(Y, X, Z = Z, Nuisance = Nuisance,                 prewhiten = list(method = \"ar\", p = \"auto\", p_max = 4))  # The prewhitened estimates account for temporal dependencies dim(beta_ar1) #> [1] 12 25 # Voxel-specific AR parameters (slower but more accurate) # Default pooling=\"global\" estimates one AR model across all voxels # pooling=\"voxel\" fits separate AR models per voxel (~100x slower) beta_voxel <- lss(Y, X, Z = Z, Nuisance = Nuisance,                   prewhiten = list(method = \"ar\", p = 1, pooling = \"voxel\"))  # ARMA model for complex noise (AR + moving average components) # Use when residuals show both autocorrelation and moving average patterns beta_arma <- lss(Y, X, Z = Z, Nuisance = Nuisance,                 prewhiten = list(method = \"arma\", p = 2, q = 1))"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"choosing-the-right-computational-backend","dir":"Articles","previous_headings":"","what":"Choosing the Right Computational Backend","title":"Getting Started with fmrilss","text":"fmrilss package offers multiple computational backends, optimized different scenarios. default R implementation well-optimized readable, making excellent understanding algorithm debugging, ’ll often want use high-performance C++ backend real analyses, especially large datasets. C++ implementation leverages Armadillo efficient linear algebra OpenMP parallel processing across multiple CPU cores: modular design allows seamless switching backends without code changes. R implementation facilitates debugging development, C++ version provides production performance.","code":"# Run the same analysis with the high-performance C++ engine beta_fast <- lss(Y, X, Z = Z, Nuisance = Nuisance, method = \"cpp_optimized\")  # The results are numerically identical to the R version all.equal(beta_full, beta_fast, tolerance = 1e-8) #> [1] TRUE  # Combine prewhitening with fast computation beta_fast_ar <- lss(Y, X, Z = Z, Nuisance = Nuisance,                     method = \"cpp_optimized\",                     prewhiten = list(method = \"ar\", p = 1))"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"lss-versus-traditional-glm-when-each-shines","dir":"Articles","previous_headings":"","what":"LSS versus Traditional GLM: When Each Shines","title":"Getting Started with fmrilss","text":"Comparison LSS standard Least Squares (LSA) methods. LSA traditional GLM approach estimates trial coefficients simultaneously single model, making computationally faster susceptible collinearity rapid designs.  LSS produces beta estimates different variance characteristics LSA. Method selection criteria: Use LSS : - MVPA pattern classification analyses - Rapid event-related designs (ISI < 4 seconds) - Trial--trial connectivity analyses - Representational similarity analysis (RSA) Use LSA : - Block designs well-separated trials - Group-level contrast estimation - Designs ISI > 6 seconds - computational efficiency critical","code":"# LSA: Standard GLM (Least Squares All) estimates all trials simultaneously # This is computationally faster but suffers from collinearity in rapid designs beta_lsa <- lsa(Y, X, Z = Z, Nuisance = Nuisance)  # Compare dimensions cat(\"LSS beta dimensions:\", dim(beta_full), \"\\n\") #> LSS beta dimensions: 12 25 cat(\"LSA beta dimensions:\", dim(beta_lsa), \"\\n\") #> LSA beta dimensions: 12 25  # Compare variance in beta estimates var_lss <- apply(beta_full, 2, var) var_lsa <- apply(beta_lsa, 2, var)  cat(\"\\nMean variance across voxels:\\n\") #>  #> Mean variance across voxels: cat(\"  LSS:\", mean(var_lss), \"\\n\") #>   LSS: 10.27741 cat(\"  LSA:\", mean(var_lsa), \"\\n\") #>   LSA: 195.8418  # Plot comparison par(mfrow = c(1, 2)) hist(beta_full[, 1], main = \"LSS: Beta distribution (Voxel 1)\",      xlab = \"Beta values\", col = \"lightblue\") hist(beta_lsa[, 1], main = \"LSA: Beta distribution (Voxel 1)\",      xlab = \"Beta values\", col = \"lightgreen\")"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"introducing-the-oasis-method","dir":"Articles","previous_headings":"","what":"Introducing the OASIS Method","title":"Getting Started with fmrilss","text":"OASIS (Optimized Analytic Single-pass Inverse Solution) method extends LSS : automatic HRF estimation, ridge regularization stability, single-pass computation trials, multi-basis HRF model support. X = NULL? OASIS can build design matrix internally design_spec, ensuring HRF settings stay synchronized design. especially useful exploring different HRFs using multi-basis models. Alternatively, can provide HRF-convolved design via X.","code":"# Basic OASIS usage with automatic design construction # fmrihrf is now imported automatically beta_oasis <- lss(   Y = Y,   X = NULL,  # Let OASIS build design from design_spec   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sampling_frame(blocklens = nrow(Y), TR = 1),       cond = list(         onsets = onsets,        # reuse onsets from above         hrf = HRF_SPMG1,        # SPM canonical HRF         span = 30               # HRF duration in seconds       )     ),     ridge_mode = \"fractional\",  # Regularization for numerical stability     ridge_x = 0.01,             # Ridge for design matrix     ridge_b = 0.01              # Ridge for coefficients   ) )  cat(\"OASIS beta dimensions:\", dim(beta_oasis), \"\\n\") #> OASIS beta dimensions: 12 25  # Alternative: Provide a pre-built design matrix directly # beta_oasis_manual <- lss(Y, X = my_hrf_convolved_design, method = \"oasis\", ...)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"working-with-different-computational-backends","dir":"Articles","previous_headings":"","what":"Working with Different Computational Backends","title":"Getting Started with fmrilss","text":"package provides multiple backends match computational needs resources. backend implements algorithm different optimization strategies. naive implementation offers clearest code understanding algorithm, r_vectorized r_optimized provide good performance pure R code. production work large datasets, cpp_optimized offers best performance, especially combined parallel processing.","code":"# Benchmark different methods library(microbenchmark)  methods <- c(\"naive\", \"r_vectorized\", \"r_optimized\", \"cpp_optimized\") timings <- list()  for (m in methods) {   timings[[m]] <- system.time({     lss(Y, X, method = m)   })[3] }  # Display timing comparison timing_df <- data.frame(   Method = methods,   Time = unlist(timings) ) print(timing_df)  # For large datasets, consider threading for C++ backends if (require(\"parallel\")) {   n_cores <- parallel::detectCores() - 1   # Set OpenMP threads for C++ backend if supported   Sys.setenv(OMP_NUM_THREADS = n_cores) }"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"handling-complex-experimental-designs","dir":"Articles","previous_headings":"","what":"Handling Complex Experimental Designs","title":"Getting Started with fmrilss","text":"Real experiments often involve multiple conditions, parametric modulations, various covariates. fmrilss package handles complexities gracefully. working multiple conditions, can create separate design matrices condition include condition labels experimental regressors. allows model condition-specific effects still obtaining trial-wise estimates within condition. Parametric modulations, trial responses weighted continuous variables like reaction time stimulus intensity, also straightforward implement. quantitative demo, simulate parametric effect data show recovery without modulator:","code":"# Create design with multiple conditions n_cond <- 3 X_multi <- matrix(0, n_timepoints, n_trials * n_cond)  # Generate a simple HRF for demonstration hrf <- c(0, 0.2, 0.5, 0.8, 1, 0.9, 0.7, 0.5, 0.3, 0.1)  for (c in 1:n_cond) {   trial_idx <- ((c-1) * n_trials + 1):(c * n_trials)   for (i in 1:n_trials) {     onset <- 10 + (trial_idx[i] - 1) * 5     if (onset + 9 <= n_timepoints) {       X_multi[onset:(onset + 9), trial_idx[i]] <- hrf     }   } }  # Add condition labels as experimental regressors Z_cond <- matrix(0, n_timepoints, n_cond) for (c in 1:n_cond) {   trial_idx <- ((c-1) * n_trials + 1):(c * n_trials)   Z_cond[, c] <- rowSums(X_multi[, trial_idx, drop = FALSE]) }  # Run LSS with condition regressors beta_multi <- lss(Y, X_multi, Z = Z_cond, method = \"r_optimized\") # Add parametric modulator (e.g., reaction time, stimulus intensity) modulator <- scale(rnorm(n_trials, mean = 0, sd = 1), center = TRUE, scale = FALSE)  # Create parametrically modulated design (scale each trial by its modulator) X_param <- sweep(X, 2, as.numeric(modulator), `*`)  # Simulate data with a parametric effect (reuse fixed and nuisance parts) Y_mod <- (Z %*% true_fixed_effects) +          (X_param %*% true_trial_betas) +          (Nuisance %*% true_nuisance_effects) +          matrix(rnorm(n_timepoints * n_voxels, 0, 1), n_timepoints, n_voxels)  # Fit models with and without the parametric modulator beta_unmod <- lss(Y_mod, X,       Z = Z, method = \"r_optimized\") beta_param <- lss(Y_mod, X_param, Z = Z, method = \"r_optimized\")  # Ground truth depends on which design matrix we use: # - When fitting with X (unmodulated):        recovers modulator * true_trial_betas # - When fitting with X_param (premodulated): recovers true_trial_betas directly true_unmod_coefs <- sweep(true_trial_betas, 1, as.numeric(modulator), `*`)  cor_unmod <- cor(as.vector(beta_unmod), as.vector(true_unmod_coefs)) cor_param <- cor(as.vector(beta_param), as.vector(true_trial_betas))  cat(\"Correlation with true coefficients (parametric effect simulated):\\n\") #> Correlation with true coefficients (parametric effect simulated): cat(\"  Using X (no modulator):\\t\", round(cor_unmod, 3), \"\\n\") #>   Using X (no modulator):     0.138 cat(\"  Using X_param (with modulator):\", round(cor_param, 3), \"\\n\") #>   Using X_param (with modulator): 0.103"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"optimizing-performance-for-large-scale-analyses","dir":"Articles","previous_headings":"","what":"Optimizing Performance for Large-Scale Analyses","title":"Getting Started with fmrilss","text":"working whole-brain data containing hundreds thousands voxels, memory management computational efficiency become critical. package provides several strategies handling large datasets effectively. large datasets exceed available memory, can process voxels chunks. approach maintains reasonable memory usage still benefiting vectorized operations within chunk: Another optimization strategy involves preprocessing data remove nuisance signals running LSS. LSS can handle nuisance regressors internally, preprocessing can efficient running multiple analyses: Choosing right backend data size crucial optimal performance: Performance guidelines (based benchmarks): practice, pick backend matches needs: R debugging, C++ maximum throughput, OASIS HRF-aware workflows. Always validate quick benchmark data.","code":"# For very large datasets, process in chunks chunk_size <- 1000 n_chunks <- ceiling(ncol(Y) / chunk_size)  beta_chunks <- list() for (chunk in 1:n_chunks) {   voxel_idx <- ((chunk - 1) * chunk_size + 1):min(chunk * chunk_size, ncol(Y))   beta_chunks[[chunk]] <- lss(Y[, voxel_idx], X, method = \"cpp_optimized\") }  # Combine results beta_full <- do.call(cbind, beta_chunks) # Project out nuisance before LSS (when appropriate) # project_confounds returns the projection matrix Q; apply it to both Y and X Q_nuis <- project_confounds(Nuisance) Y_clean <- Q_nuis %*% Y X_clean <- Q_nuis %*% X  # This can be faster than including Nuisance in each LSS iteration beta_preprocessed <- lss(Y_clean, X_clean, Z = Z, method = \"r_optimized\")"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"troubleshooting-common-challenges","dir":"Articles","previous_headings":"","what":"Troubleshooting Common Challenges","title":"Getting Started with fmrilss","text":"Even robust implementations, certain data characteristics can cause issues. Understanding diagnose address problems help get analyses. design matrices become singular near-singular due high collinearity regressors, standard least squares solutions become unstable. can detect examining correlation matrix design: Memory limitations can also pose challenges large datasets. starting analysis, ’s wise estimate memory requirements adjust approach accordingly: encounter unexpectedly slow performance, profiling can help identify bottlenecks:","code":"# Check for collinearity in design matrix cor_matrix <- cor(X) high_cor <- which(abs(cor_matrix) > 0.9 & cor_matrix != 1, arr.ind = TRUE)  if (nrow(high_cor) > 0) {   warning(\"High correlation between regressors detected\")    # Option 1: Use OASIS with ridge and automatic design construction   beta_ridge <- lss(     Y = Y,     X = NULL,  # Let OASIS build design     method = \"oasis\",     oasis = list(       design_spec = list(         sframe = sampling_frame(blocklens = nrow(Y), TR = 1),         cond = list(onsets = onsets, hrf = HRF_SPMG1, span = 30)       ),       ridge_mode = \"absolute\",       ridge_x = 0.1  # Stronger ridge for collinearity     )   )    # Option 2: For standard LSS without OASIS, increase ISI or use prewhitening   # Ridge is not directly available for non-OASIS backends } # Estimate peak memory requirements mem_Y <- object.size(Y) mem_designs_approx <- mem_Y * n_trials  # n_trials copies of design during LSS mem_results <- mem_Y  # Output size similar to Y mem_required <- mem_Y + mem_designs_approx + mem_results  cat(\"Estimated memory needed:\", round(mem_required / 1e9, 2), \"GB\\n\")  # If this exceeds available RAM, use chunking approach (see example above) # Or consider OASIS which has lower memory overhead for large datasets # Profile code to find bottlenecks Rprof(\"lss_profile.out\") beta_slow <- lss(Y, X, method = \"naive\") Rprof(NULL) summaryRprof(\"lss_profile.out\")"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"where-to-go-from-here","dir":"Articles","previous_headings":"","what":"Where to Go From Here","title":"Getting Started with fmrilss","text":"vignette introduced core concepts capabilities fmrilss package. ’ve learned LSS addresses collinearity problem rapid event-related designs, choose different computational backends, strategies handling complex experimental designs large datasets. deepen understanding explore advanced features, recommend: Voxel-wise HRF vignette: Model spatial variation hemodynamic responses across brain using fully flexible HRF estimation SBHM vignette: Learn efficient voxel-specific HRF estimation using Shared-Basis HRF Matching, library-constrained approach balances flexibility computational efficiency OASIS method vignette: Comprehensive coverage OASIS features including automatic HRF construction, ridge regularization, multi-basis models Mixed models vignette: Combine LSS mixed-effects modeling frameworks hierarchical analyses fmrilss package represents comprehensive toolkit trial-wise beta estimation, providing options range simple, interpretable implementations highly optimized solutions large-scale analyses. Whether ’re conducting exploratory analyses laptop processing massive datasets computing cluster, package offers flexibility performance need modern fMRI analysis.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Getting Started with fmrilss","text":"Mumford, J. ., Turner, B. O., Ashby, F. G., & Poldrack, R. . (2012). Deconvolving BOLD activation event-related designs multivoxel pattern classification analyses. NeuroImage, 59(3), 2636-2643.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/getting_started.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Getting Started with fmrilss","text":"","code":"#> R version 4.5.1 (2025-06-13) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] fmrilss_0.1.0          fmrihrf_0.1.0.9000     RcppArmadillo_15.0.2-2 #> [4] Rcpp_1.1.0             #>  #> loaded via a namespace (and not attached): #>  [1] Matrix_1.7-3        gtable_0.3.6        jsonlite_2.0.0      #>  [4] compiler_4.5.1      assertthat_0.2.1    jquerylib_0.1.4     #>  [7] splines_4.5.1       systemfonts_1.3.1   scales_1.4.0        #> [10] textshaping_1.0.4   uuid_1.2-1          yaml_2.3.10         #> [13] fastmap_1.2.0       lattice_0.22-7      ggplot2_4.0.0       #> [16] R6_2.6.1            knitr_1.50          desc_1.4.3          #> [19] RColorBrewer_1.1-3  bslib_0.9.0         rlang_1.1.6         #> [22] cachem_1.1.0        xfun_0.53           S7_0.2.0            #> [25] fs_1.6.6            sass_0.4.10         memoise_2.0.1       #> [28] cli_3.6.5           pkgdown_2.1.3       magrittr_2.0.4      #> [31] digest_0.6.37       grid_4.5.1          bigmemory.sri_0.1.8 #> [34] lifecycle_1.0.4     bigmemory_4.6.4     vctrs_0.6.5         #> [37] evaluate_1.0.5      glue_1.8.0          farver_2.1.2        #> [40] numDeriv_2016.8-1.1 fmriAR_0.1.0        ragg_1.5.0          #> [43] rmarkdown_2.30      purrr_1.1.0         tools_4.5.1         #> [46] htmltools_0.5.8.1"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"the-computational-challenge-of-modern-fmri","dir":"Articles","previous_headings":"","what":"The Computational Challenge of Modern fMRI","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Modern fMRI datasets sub-millimeter resolution thousands trials require millions model fits using standard LSS approaches. datasets 100,000 voxels 500 trials, traditional LSS requires 50 million separate GLM fits, involving matrix inversion parameter estimation. OASIS (Optimized Analytic Single-pass Inverse Solution) addresses computational challenge mathematical reformulation. Instead iterating trials sequentially, OASIS exploits shared structure across LSS models compute trial estimates simultaneously. Compared naive per-trial GLM approach (O(N·T³)), factored approach reduces dominant work single decomposition plus shared projections (roughly O(T³ + N·T·V)). Note fmrilss’s optimized LSS backends already exploit factoring; OASIS packages estimator extends HRF-aware design building multi-basis solves. OASIS also provides: automatic HRF estimation multi-basis functions, ridge regularization numerical stability, efficient handling complex experimental designs, integration prewhitening autocorrelation correction. Prerequisites: familiarity LSS concepts, HRF models basis functions, ridge regression principles, fmrihrf package.","code":"library(fmrihrf) library(fmrilss) set.seed(42)  # Note: we build designs inline with fmrihrf primitives (regressor_set + evaluate) # to keep examples short and readable."},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"understanding-the-oasis-innovation","dir":"Articles","previous_headings":"","what":"Understanding the OASIS Innovation","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"OASIS exploits mathematical structure LSS: N separate GLMs share computational components can factored reused across trials. Note factoring already exploited fmrilss’s optimized backends (e.g., fused single-pass solver residualizes reuses totals cross-products across trials). Importantly, single-basis HRF (K = 1) ridge (ridge_x = ridge_b = 0), OASIS mathematically identical core LSS estimator; simply computes betas one batched pass. OASIS wraps core solve richer, HRF-aware workflow: automatic design construction event specs, optional ridge scaling, AR(1) whitening, SE/diagnostic reporting, native support multi-basis FIR HRFs. Beyond computational efficiency, OASIS adds features generic LSS path expose: built-ridge regularization (absolute fractional) two-regressor Gram per trial, optional AR(1) whitening, analytical standard errors diagnostics, blocked multi-basis solver treats K>1 HRF bases 2Kx2K closed-form systems per trial. Finally, OASIS integrates directly fmrihrf: builds trial-wise designs event specs (including multi-condition aggregates), auto-detects basis dimension K, supports FIR/non-parametric bases, can run fast HRF grid selection fitting. removes manual design construction preserving exact LSS equivalence.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"what-oasis-adds-over-optimized-lss","dir":"Articles","previous_headings":"Understanding the OASIS Innovation","what":"What OASIS Adds Over Optimized LSS","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"HRF-aware design: Builds X events via fmrihrf (single multi-basis), optional HRF grid search. Stabilization: Ridge regularization per‑trial Gram (absolute fractional scaling design energy). Inference: Optional standard errors design diagnostics without extra passes. Whitening: Optional AR(1) prewhitening applied consistently data design. Multi-basis efficiency: Blocked products 2Kx2K closed-form solves K>1 bases.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"equivalence-to-lss-k-1-no-ridge","dir":"Articles","previous_headings":"Understanding the OASIS Innovation","what":"Equivalence to LSS (K = 1, no ridge)","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"already single-basis trial-wise design X use ridge, lss(..., method = \"oasis\") returns coefficients optimized LSS path. quick check verifies numerical equivalence floating-point tolerance: Use plain LSS backends fixed single-basis X want leanest dependency surface. Prefer OASIS want built-HRF design construction, multi-basis HRFs (K > 1), ridge control, standard errors/diagnostics one call.","code":"set.seed(1) n <- 120; n_trials <- 15; V <- 8 X <- matrix(rnorm(n * n_trials), n, n_trials) Z <- cbind(1, scale(1:n)) N <- matrix(rnorm(n * 3), n, 3) Y <- matrix(rnorm(n * V), n, V)  b_lss  <- lss(Y, X, Z = Z, Nuisance = N, method = \"cpp_optimized\") b_oasis <- lss(Y, X, Z = Z, Nuisance = N, method = \"oasis\", oasis = list(ridge_x = 0, ridge_b = 0))  max(abs(b_lss - b_oasis))"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"when-to-use-which","dir":"Articles","previous_headings":"Understanding the OASIS Innovation","what":"When To Use Which","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"already trial-wise single-basis X. need ridge, SEs, HRF grid search. want minimal dependency surface maximum simplicity. want build X directly event specs/HRFs call. need multi-basis HRFs (K > 1) FIR. want ridge regularization, SEs/diagnostics, AR(1) whitening one pass. want optional HRF grid search design diagnostics.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"starting-with-oasis-a-simple-example","dir":"Articles","previous_headings":"","what":"Starting with OASIS: A Simple Example","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Let’s begin straightforward example demonstrates OASIS action. ’ll create synthetic data known ground truth, see OASIS recovers signal:","code":"# Create synthetic data n_time <- 300 n_voxels <- 100 TR <- 1.0  # Generate event onsets (rapid, jittered ISIs ~ U(3, 9) s) start_time <- 10 isi <- runif(500, min = 3, max = 9)                  # generous upper bound onsets_cont <- c(start_time, start_time + cumsum(isi)) onsets <- onsets_cont[onsets_cont < (n_time - 20)]    # leave tail for HRF span n_trials <- length(onsets)  # Create sampling frame sframe <- fmrihrf::sampling_frame(blocklens = n_time, TR = TR)  # Generate synthetic fMRI data Y <- matrix(rnorm(n_time * n_voxels), n_time, n_voxels)  # Add signal to data true_betas <- matrix(rnorm(n_trials * n_voxels, mean = 1, sd = 0.5),                      n_trials, n_voxels)  # Create signal using canonical HRF grid <- fmrihrf::samples(sframe, global = TRUE) rset <- fmrihrf::regressor_set(   onsets   = onsets,   fac      = factor(seq_len(n_trials)),  # one column per trial   hrf      = fmrihrf::HRF_SPMG1,   duration = 0,   span     = 30,   summate  = FALSE ) X_trials <- fmrihrf::evaluate(rset, grid = grid, precision = 0.1, method = \"conv\")  Y <- Y + X_trials %*% true_betas  # Run OASIS beta_oasis <- lss(   Y = Y,   X = NULL,  # OASIS builds design internally   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(         onsets = onsets,         hrf = fmrihrf::HRF_SPMG1,         span = 30  # HRF duration       )     )   ) )  cat(\"OASIS results:\\n\") #> OASIS results: cat(\"  Beta dimensions:\", dim(beta_oasis), \"\\n\") #>   Beta dimensions: 42 100 cat(\"  Mean beta:\", round(mean(beta_oasis), 3), \"\\n\") #>   Mean beta: 1.012 cat(\"  Beta SD:\", round(sd(beta_oasis), 3), \"\\n\") #>   Beta SD: 0.613"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"visualizing-the-design","dir":"Articles","previous_headings":"Starting with OASIS: A Simple Example","what":"Visualizing the Design","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"","code":"if (requireNamespace(\"ggplot2\", quietly = TRUE)) {   dfX <- data.frame(     Time  = rep(seq_len(nrow(X_trials)), times = ncol(X_trials)),     Trial = factor(rep(seq_len(ncol(X_trials)), each = nrow(X_trials))),     Value = as.vector(X_trials)   )   ggplot2::ggplot(dfX, ggplot2::aes(Time, Trial, fill = Value)) +     ggplot2::geom_tile() +     ggplot2::scale_fill_viridis_c(option = \"C\") +     ggplot2::labs(title = \"Trial-wise design (SPMG1)\", x = \"Time (TR)\", y = \"Trial\", fill = \"\") +     ggplot2::theme_minimal(base_size = 12) }"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Notice OASIS takes design specification rather pre-built design matrix. allows construct optimal internal representation efficient computation. results identical ’d get standard LSS, computed much efficiently.","code":"#> R version 4.5.1 (2025-06-13) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] fmrilss_0.1.0      fmrihrf_0.1.0.9000 #>  #> loaded via a namespace (and not attached): #>  [1] Matrix_1.7-3        gtable_0.3.6        jsonlite_2.0.0      #>  [4] compiler_4.5.1      Rcpp_1.1.0          assertthat_0.2.1    #>  [7] jquerylib_0.1.4     splines_4.5.1       systemfonts_1.3.1   #> [10] scales_1.4.0        textshaping_1.0.4   uuid_1.2-1          #> [13] yaml_2.3.10         fastmap_1.2.0       lattice_0.22-7      #> [16] ggplot2_4.0.0       R6_2.6.1            labeling_0.4.3      #> [19] knitr_1.50          desc_1.4.3          RColorBrewer_1.1-3  #> [22] bslib_0.9.0         rlang_1.1.6         cachem_1.1.0        #> [25] xfun_0.53           S7_0.2.0            fs_1.6.6            #> [28] sass_0.4.10         viridisLite_0.4.2   memoise_2.0.1       #> [31] cli_3.6.5           withr_3.0.2         pkgdown_2.1.3       #> [34] magrittr_2.0.4      digest_0.6.37       grid_4.5.1          #> [37] bigmemory.sri_0.1.8 lifecycle_1.0.4     bigmemory_4.6.4     #> [40] vctrs_0.6.5         evaluate_1.0.5      glue_1.8.0          #> [43] farver_2.1.2        numDeriv_2016.8-1.1 fmriAR_0.1.0        #> [46] ragg_1.5.0          rmarkdown_2.30      purrr_1.1.0         #> [49] tools_4.5.1         htmltools_0.5.8.1"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"the-power-of-ridge-regularization","dir":"Articles","previous_headings":"","what":"The Power of Ridge Regularization","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"One OASIS’s valuable features built-ridge regularization. rapid event-related designs, close temporal spacing trials can lead highly correlated regressors unstable estimates. Ridge regression adds small penalty term “shrinks” estimates toward zero, trading small amount bias large reduction variance. OASIS offers two approaches ridge regularization, suited different scenarios. Let’s explore :","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"absolute-ridge-regularization","dir":"Articles","previous_headings":"The Power of Ridge Regularization","what":"Absolute Ridge Regularization","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"absolute ridge, specify fixed penalty values added diagonal normal equations. approach gives direct control regularization strength:","code":"# Absolute ridge: fixed penalty values beta_ridge_abs <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(onsets = onsets, hrf = HRF_SPMG1, span = 30)     ),     ridge_mode = \"absolute\",     ridge_x = 0.1,  # Penalty on trial regressor     ridge_b = 0.1   # Penalty on aggregator regressor   ) )  # Compare with unregularized cat(\"Comparison with unregularized:\\n\") #> Comparison with unregularized: cat(\"  Correlation:\", round(cor(as.vector(beta_oasis),                                as.vector(beta_ridge_abs)), 3), \"\\n\") #>   Correlation: 1 cat(\"  Mean absolute difference:\",     round(mean(abs(beta_oasis - beta_ridge_abs)), 4), \"\\n\") #>   Mean absolute difference: 0.0096"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"fractional-ridge-regularization","dir":"Articles","previous_headings":"The Power of Ridge Regularization","what":"Fractional Ridge Regularization","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Fractional ridge scales penalty relative “energy” (sum squares) design matrix. adaptive approach automatically adjusts scale data: variance reduction demonstrates ridge regression’s stabilizing effect. practice, translates reliable estimates, especially noisy data complex designs.","code":"# Fractional ridge: penalty as fraction of design energy beta_ridge_frac <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(onsets = onsets, hrf = HRF_SPMG1, span = 30)     ),     ridge_mode = \"fractional\",     ridge_x = 0.05,  # 5% of mean design energy     ridge_b = 0.05   # 5% of mean aggregator energy   ) )  # Ridge typically reduces variance in estimates var_unreg <- apply(beta_oasis, 2, var) var_ridge <- apply(beta_ridge_frac, 2, var)  cat(\"\\nVariance reduction with ridge:\\n\") #>  #> Variance reduction with ridge: cat(\"  Mean variance (unregularized):\", round(mean(var_unreg), 4), \"\\n\") #>   Mean variance (unregularized): 0.3664 cat(\"  Mean variance (ridge):\", round(mean(var_ridge), 4), \"\\n\") #>   Mean variance (ridge): 0.3319 cat(\"  Reduction:\", round((1 - mean(var_ridge)/mean(var_unreg)) * 100, 1), \"%\\n\") #>   Reduction: 9.4 %"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"working-with-multi-basis-hrfs","dir":"Articles","previous_headings":"","what":"Working with Multi-Basis HRFs","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Real hemodynamic responses rarely match canonical HRF perfectly. might peak earlier later, wider narrower, different undershoot characteristics. Multi-basis HRF models capture variability representing response weighted combination basis functions. OASIS handles multi-basis HRFs naturally, returning separate estimates basis component:  multi-basis approach reveals hemodynamic response varies across trials. Large temporal derivative values suggest timing variability, dispersion derivative values indicate width changes. information can valuable understanding physiological variations task-related modulations hemodynamic response.","code":"# Use SPMG3: canonical + temporal derivative + dispersion derivative beta_spmg3 <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(         onsets = onsets,         hrf = HRF_SPMG3,  # 3 basis functions         span = 30       )     )   ) )  # SPMG3 returns KxN rows where K=number of basis functions, N=number of trials # The results are organized in blocks: [trial1_basis1, trial1_basis2, trial1_basis3, #                                       trial2_basis1, trial2_basis2, trial2_basis3, ...] cat(\"Multi-basis results:\\n\") #> Multi-basis results: cat(\"  Beta dimensions:\", dim(beta_spmg3), \"\\n\") #>   Beta dimensions: 126 100 cat(\"  Basis functions (K):\", 3, \"\\n\") #>   Basis functions (K): 3 cat(\"  Trials (N):\", n_trials, \"\\n\") #>   Trials (N): 42 cat(\"  Total rows (KxN):\", nrow(beta_spmg3), \"\\n\") #>   Total rows (KxN): 126  # Extract components using the K-stride pattern # For K=3 basis functions, every 3rd row starting from position k gives basis k K <- 3  # Number of basis functions in SPMG3 canonical_betas <- beta_spmg3[seq(1, nrow(beta_spmg3), by = K), ]    # Basis 1: Canonical temporal_betas <- beta_spmg3[seq(2, nrow(beta_spmg3), by = K), ]     # Basis 2: Temporal derivative dispersion_betas <- beta_spmg3[seq(3, nrow(beta_spmg3), by = K), ]   # Basis 3: Dispersion derivative  # Verify dimensions: each should be n_trials x n_voxels cat(\"\\nExtracted component dimensions:\\n\") #>  #> Extracted component dimensions: cat(\"  Each basis matrix:\", dim(canonical_betas), \"\\n\") #>   Each basis matrix: 42 100  # Analyze contributions cat(\"\\nBasis function contributions:\\n\") #>  #> Basis function contributions: cat(\"  Canonical mean |beta|:\", round(mean(abs(canonical_betas)), 3), \"\\n\") #>   Canonical mean |beta|: 1.065 cat(\"  Temporal deriv mean |beta|:\", round(mean(abs(temporal_betas)), 3), \"\\n\") #>   Temporal deriv mean |beta|: 0.861 cat(\"  Dispersion deriv mean |beta|:\", round(mean(abs(dispersion_betas)), 3), \"\\n\") #>   Dispersion deriv mean |beta|: 1.463  # Visualize first voxel par(mfrow = c(1, 3)) plot(canonical_betas[, 1], type = \"b\", main = \"Canonical\",      ylab = \"Beta\", xlab = \"Trial\") plot(temporal_betas[, 1], type = \"b\", main = \"Temporal Derivative\",      ylab = \"Beta\", xlab = \"Trial\") plot(dispersion_betas[, 1], type = \"b\", main = \"Dispersion Derivative\",      ylab = \"Beta\", xlab = \"Trial\")"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"pervoxel-hrfs-with-oasisvoxhrf","dir":"Articles","previous_headings":"","what":"Per‑Voxel HRFs with OASIS‑VOXHRF","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"datasets benefit estimating HRF shape per voxel computing trial amplitudes. OASIS‑VOXHRF mode adds fast, two‑pass workflow stays entirely within OASIS idiom: Pass : estimate voxel‑wise HRF shape chosen basis solving tiny K×K ridge system using aggregate per‑basis design. can optionally nudge shapes toward reference (shape prior) add small roughness penalty. Pass B: compute single‑trial betas using voxel‑specific HRFs via existing lss_with_hrf() C++ engine. Enable setting oasis$hrf_mode = \"voxel_ridge\" , optionally, tuning knobs . Defaults keep constraints maximal flexibility. differs HRF grid search - Grid search () selects single global HRF whole dataset/ROI scoring small set candidate shapes, refitting winner. fast robust expect largely homogeneous HRF, capture voxel‑‑voxel shape differences. - VOXHRF estimates HRF shape per voxel chosen basis (e.g., SPMG3, FIR) fits trial betas voxel‑specific shapes. captures spatial heterogeneity lets regularize toward reference via shape prior (lambda_shape), smooth roughness penalty (mu_rough), optionally shrink weights across voxels. strong shape prior, VOXHRF reduces canonical‑HRF OASIS solution. - practice: start global HRF via grid search stability speed; use VOXHRF need spatially varying shapes multi‑basis/FIR modeling important. Notes - Uses confound residualization optional whitening standard OASIS path. - Defaults: shape prior, roughness, shrink; opt needed rapid designs. - strong shape prior, results approach canonical‑HRF OASIS.","code":"beta_voxhrf <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(onsets = onsets, hrf = HRF_SPMG3, span = 30)     ),     hrf_mode      = \"voxel_ridge\",     lambda_shape  = 5,       # pull toward reference shape (e.g., canonical)     mu_rough      = 0,       # optional roughness (0 = off)     shrink_global = 0.05,    # small global shrink for stability     orient_ref    = TRUE     # flip shapes to have positive orientation   ) ) dim(beta_voxhrf)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"sharedbasis-hrf-matching-sbhm","dir":"Articles","previous_headings":"","what":"Shared‑Basis HRF Matching (SBHM)","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"SBHM learns single low‑rank time basis large HRF library fits voxels coordinate system. Matching fast cosine lookup r‑dimensional space, trial‑wise amplitudes come projecting r‑coefficients onto voxel’s matched coordinates. provides design‑aware HRF selection without FIRs manifold alignment. Note: section provides quick demonstration SBHM. comprehensive documentation including parameter guidance, performance considerations, troubleshooting, see dedicated SBHM vignette (vignette(\"sbhm\", package = \"fmrilss\")). Workflow - Build library via fmrihrf::hrf_library() parameter grid; learn rank‑r basis B SVD. - Prepass: fit r coefficients per voxel SBHM basis, match closest library member whitened, L2‑normalized coefficient space. - LSS: run OASIS K=r (SBHM HRF) get r×trial coefficients; project onto matched coordinates scalar amplitudes. Notes - library data live coordinate system: library HRF h_k = B α_k, trial regressor exactly X α_k X per‑basis regressors, comparing β̄_v α_k principled. - Whitening divides coefficients singular values (Σ_r^{-1}), equalizing basis directions cosine matching; recommended. - Confidence can read margin top‑1 top‑2 cosine matches; small margins indicate ambiguity.","code":"# 1) Build an HRF library on the TR grid using fmrihrf::hrf_library param_grid <- expand.grid(shape = c(6, 8, 10), rate = c(0.9, 1.0, 1.1)) gamma_fun  <- function(shape, rate) {   f <- function(t) fmrihrf::hrf_gamma(t, shape = shape, rate = rate)   fmrihrf::as_hrf(f, span = 32) }  sbhm <- sbhm_build(   library_spec = list(fun = gamma_fun, pgrid = param_grid, span = 32),   r = 6, sframe = sframe, baseline = c(0, 0.5), normalize = TRUE )  # 2) Define design (SBHM HRF is injected automatically by lss_sbhm) design_spec <- list(   sframe = sframe,   cond   = list(onsets = onsets, duration = 0, span = 32) )  # 3) Run end-to-end SBHM: prepass -> match -> OASIS(K=r) -> projection res_sbhm <- lss_sbhm(   Y = Y,   sbhm = sbhm,   design_spec = design_spec,   Nuisance = NULL,   prewhiten = list(method = \"ar\", exact_first = \"ar1\"),   match = list(shrink = list(tau = 0.0, ref = sbhm$ref$alpha_ref), whiten = TRUE),   oasis = list(ridge_mode = \"fractional\", ridge_x = 0, ridge_b = 0),   return = \"amplitude\" )  cat(\"SBHM results:\\n\") cat(\"  Amplitude dims:\", dim(res_sbhm$amplitude), \"\\n\") cat(\"  Mean amplitude:\", round(mean(res_sbhm$amplitude), 3), \"\\n\") cat(\"  Top-1/-2 margin (median):\", round(median(res_sbhm$margin), 3), \"\\n\")  # Optional: compare against canonical LSS amplitudes for reference beta_spmg1 <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(sframe = sframe, cond = list(onsets = onsets, hrf = fmrihrf::HRF_SPMG1, span = 30))   ) ) cat(\"  Corr(SBHM amp, canonical beta):\",     round(cor(as.vector(res_sbhm$amplitude), as.vector(beta_spmg1)), 3), \"\\n\")"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"whitening-semantics","dir":"Articles","previous_headings":"","what":"Whitening Semantics","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"oasis$whiten = \"ar1\" prewhiten = list(...) used, OASIS: Estimates AR parameter deflating nuisance space used model (intercept Z, user Nuisance, design_spec$others). gives stable rho. Applies linear transform W data Y, trial design X, nuisance design, whitened GLS algebraically equivalent unwhitened model. Implications - Consistency matters: manually whiten outside OASIS, use nuisance space rho estimation apply W Y, X, nuisance. Otherwise may see small systematic differences. - Coefficients identical floating‑point whitening applied consistently. Standard errors t‑statistics computed whitened space automatically OASIS. - Whitening change model degrees freedom; re‑weights timepoints account autocorrelation. - behavior shared VOXHRF path; whitening applied consistently model components.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"non-parametric-hrf-modeling-with-fir","dir":"Articles","previous_headings":"","what":"Non-Parametric HRF Modeling with FIR","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Sometimes want make assumptions HRF shape . Finite Impulse Response (FIR) basis provides completely non-parametric approach, estimating response time point independently:  FIR approach reveals actual shape hemodynamic response without parametric constraints. trial now contributes one coefficient per time bin, individual voxel-level estimates can look jagged—variance simply much higher single-parameter canonical fit. Averaging across trials voxels (adding modest ridge penalty) recovers smooth, HRF-like curve still allowing deviations canonical shape. practice typically pool information across voxels/regions apply additional smoothing/regularization working FIR bases.","code":"# Create FIR basis with 15 time bins (2s width over a 30s window) fir_hrf <- hrf_fir_generator(nbasis = 15, span = 30)  # Robustly derive bin width from attributes. Some fmrihrf versions # do not expose bin_width via attr(params). n_bins   <- as.integer(attr(fir_hrf, \"nbasis\")) span_fir <- as.numeric(attr(fir_hrf, \"span\")) bin_width <- span_fir / n_bins  beta_fir <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(         onsets = onsets,         hrf = fir_hrf,         span = 30       )     ),     ridge_mode = \"fractional\",     ridge_x = 0.2,  # Moderate ridge improves stability for high-dimensional FIR fits     ridge_b = 0.2   ) )  cat(\"FIR basis results:\\n\") #> FIR basis results: cat(\"  Beta dimensions:\", dim(beta_fir), \"\\n\") #>   Beta dimensions: 630 100 cat(\"  Time bins per trial:\", nrow(beta_fir) / n_trials, \"\\n\") #>   Time bins per trial: 15  # Organise coefficients as (time bin x trial x voxel) fir_array <- array(beta_fir, dim = c(n_bins, n_trials, ncol(beta_fir)))  # Average across trials and voxels to recover a smooth HRF estimate fir_mean <- apply(fir_array, 1, mean) fir_se <- apply(fir_array, 1, sd) / sqrt(n_trials * ncol(beta_fir)) time_points <- seq(0, (n_bins - 1) * bin_width, by = bin_width)  # Plot mean HRF with +/- 1 SE ribbon upper <- fir_mean + fir_se lower <- fir_mean - fir_se  plot(time_points, fir_mean, type = \"l\",      ylim = range(c(lower, upper)),      main = \"FIR-derived HRF (mean across trials/voxels)\",      xlab = \"Time (seconds)\", ylab = \"Response\",      col = \"navy\", lwd = 2) polygon(c(time_points, rev(time_points)), c(upper, rev(lower)),         col = grDevices::adjustcolor(\"navy\", alpha.f = 0.2), border = NA) lines(time_points, fir_mean, col = \"navy\", lwd = 2)  # Add canonical HRF sampled on the same grid for reference canonical <- evaluate(HRF_SPMG1, time_points) scale_factor <- max(fir_mean) / max(canonical) lines(time_points, canonical * scale_factor, col = \"firebrick\", lty = 2, lwd = 2) legend(\"topright\",        c(\"FIR mean\", \"+/- 1 SE\", \"Canonical (scaled)\"),        col = c(\"navy\", NA, \"firebrick\"),        lty = c(1, NA, 2),        lwd = c(2, NA, 2),        pch = c(NA, 15, NA),        pt.cex = c(NA, 2, NA),        pt.bg = c(NA, grDevices::adjustcolor(\"navy\", alpha.f = 0.2), NA),        bty = \"n\")"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"optimizing-hrf-models-through-grid-search","dir":"Articles","previous_headings":"","what":"Optimizing HRF Models Through Grid Search","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"’re unsure HRF model best fits data, OASIS can efficiently evaluate multiple candidates. vignette perform global HRF selection: choose single HRF shape shared voxels (ROI) refit model using HRF. keeps selection fast stable. estimate voxel‑specific HRFs; workflow, see “Voxel‑wise HRF” vignette use multi‑basis/FIR model. two ways drive selection: - Manual scoring (shown ): loop candidates, score fit, pick best, refit full voxel set. - Built‑selection: pass grid via oasis$design_spec$hrf_grid let OASIS pick single best HRF internally using matched‑filter score. Grid search (global) reveals HRF parameters best explain data dataset/ROI level. complementary voxel‑wise HRF modeling: start shared HRF stability, zoom multi‑basis/FIR needed. prefer let OASIS pick HRF internally, pass grid directly. Note path also selects single global HRF proceeds immediately fitting; expose chosen parameters return value:","code":"# Create a grid of HRF models with varying parameters hrf_grid <- create_lwu_grid(   tau_range = c(4, 8),      # Peak time   sigma_range = c(2, 3.5),  # Width   rho_range = c(0.2, 0.5),  # Undershoot   n_tau = 3,   n_sigma = 2,   n_rho = 2 )  cat(\"Testing\", length(hrf_grid$hrfs), \"different HRF models\\n\\n\") #> Testing 12 different HRF models  # Global HRF grid search (single HRF for all voxels): # Score each candidate on a small voxel subset, then refit all voxels with the best. best_fit <- -Inf best_idx <- 1  for (i in seq_along(hrf_grid$hrfs)) {   # Create HRF object   tau_val <- hrf_grid$parameters$tau[i]   sigma_val <- hrf_grid$parameters$sigma[i]   rho_val <- hrf_grid$parameters$rho[i]    hrf_obj <- structure(     function(t) {       hrf_lwu(t, tau = tau_val, sigma = sigma_val, rho = rho_val, normalize = \"height\")     },     class = c(\"hrf\", \"function\"),     span = 30   )    # Fit OASIS with this HRF   beta_test <- lss(     Y = Y[, 1:10],  # Test on subset for speed     X = NULL,     method = \"oasis\",     oasis = list(       design_spec = list(         sframe = sframe,         cond = list(onsets = onsets, hrf = hrf_obj, span = 30)       ),       ridge_mode = \"fractional\",       ridge_x = 0.01,       ridge_b = 0.01     )   )    # Calculate fit (simplified - residual sum of squares)   grid <- fmrihrf::samples(sframe, global = TRUE)   rset <- fmrihrf::regressor_set(onsets = onsets,                                  fac = factor(seq_along(onsets)),                                  hrf = hrf_obj,                                  duration = 0,                                  span = 30,                                  summate = FALSE)   X_test <- fmrihrf::evaluate(rset, grid = grid, precision = 0.1, method = \"conv\")    fitted <- X_test %*% beta_test   rss <- sum((Y[, 1:10] - fitted)^2)    if (-rss > best_fit) {     best_fit <- -rss     best_idx <- i   } }  cat(\"Best HRF parameters:\\n\") #> Best HRF parameters: cat(\"  tau:\", hrf_grid$parameters$tau[best_idx], \"\\n\") #>   tau: 4 cat(\"  sigma:\", hrf_grid$parameters$sigma[best_idx], \"\\n\") #>   sigma: 2 cat(\"  rho:\", hrf_grid$parameters$rho[best_idx], \"\\n\") #>   rho: 0.2  # Refit all voxels with the selected global HRF best_hrf <- hrf_grid$hrfs[[best_idx]] beta_global <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(onsets = onsets, hrf = best_hrf, span = 30)     ),     ridge_mode = \"fractional\",     ridge_x = 0.01,     ridge_b = 0.01   ) ) cat(\"Refit with global HRF complete. Beta dims:\", paste(dim(beta_global), collapse = \" x \"), \"\\n\") #> Refit with global HRF complete. Beta dims: 42 x 100 beta_auto_grid <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(onsets = onsets, span = 30),       hrf_grid = hrf_grid$hrfs     ),     ridge_mode = \"fractional\",     ridge_x = 0.01,     ridge_b = 0.01   ) ) dim(beta_auto_grid)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"handling-complex-experimental-designs","dir":"Articles","previous_headings":"","what":"Handling Complex Experimental Designs","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Real experiments often involve multiple conditions, trials interest others serving nuisance events. OASIS handles scenarios structured condition specification: approach ensures variance conditions properly accounted without conflating trials interest.","code":"# Create design with two conditions onsets_cond1 <- seq(10, 280, by = 30) onsets_cond2 <- seq(25, 280, by = 30)  # Condition 1 as target, Condition 2 as nuisance beta_multi <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(         onsets = onsets_cond1,         hrf = HRF_SPMG1,         span = 30       ),       others = list(         list(onsets = onsets_cond2)  # Other conditions as nuisance       )     )   ) )  cat(\"Multi-condition OASIS:\\n\") #> Multi-condition OASIS: cat(\"  Analyzing condition 1 trials:\", length(onsets_cond1), \"\\n\") #>   Analyzing condition 1 trials: 10 cat(\"  Condition 2 included as nuisance\\n\") #>   Condition 2 included as nuisance cat(\"  Beta dimensions:\", dim(beta_multi), \"\\n\") #>   Beta dimensions: 10 100"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"beyond-point-estimates-standard-errors-and-diagnostics","dir":"Articles","previous_headings":"","what":"Beyond Point Estimates: Standard Errors and Diagnostics","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"beta estimates valuable, understanding uncertainty crucial proper inference. OASIS can provide standard errors diagnostic information:  Standard errors reveal estimates reliable. Trials higher standard errors might overlapping responses occurring periods higher noise.","code":"# Request standard errors result_with_se <- lss(   Y = Y[, 1:10],  # Subset for demonstration   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(onsets = onsets[1:10], hrf = HRF_SPMG1, span = 30)     ),     return_se = TRUE,     return_diag = TRUE   ) )  cat(\"Results with diagnostics:\\n\") #> Results with diagnostics: cat(\"  Beta dimensions:\", dim(result_with_se$beta), \"\\n\") #>   Beta dimensions: 10 10 cat(\"  SE dimensions:\", dim(result_with_se$se), \"\\n\") #>   SE dimensions: 10 10 cat(\"  Mean SE:\", round(mean(result_with_se$se), 4), \"\\n\") #>   Mean SE: 0.3953  # Calculate t-statistics t_stats <- result_with_se$beta / result_with_se$se cat(\"  Mean |t-statistic|:\", round(mean(abs(t_stats)), 2), \"\\n\") #>   Mean |t-statistic|: 1.33  # Visualize SE across trials plot(rowMeans(result_with_se$se), type = \"b\",      main = \"Standard Error Across Trials\",      xlab = \"Trial\", ylab = \"Mean SE\",      col = \"darkblue\", pch = 19)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"performance-in-practice","dir":"Articles","previous_headings":"","what":"Performance in Practice","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"understand OASIS’s efficiency, let’s conduct systematic benchmarks comparing traditional LSS implementations across different dataset sizes:  capability allows test hypotheses hemodynamic response characteristics adapt models software packages.","code":"# Benchmark across different dataset sizes sizes <- list(   small  = list(timepoints = 300, voxels = 500),   medium = list(timepoints = 400, voxels = 2000),   large  = list(timepoints = 600, voxels = 8000),   xlarge = list(timepoints = 800, voxels = 16000) )  benchmark_results <- data.frame()  for (size_name in names(sizes)) {   n_time <- sizes[[size_name]][['timepoints']]   n_vox  <- sizes[[size_name]][['voxels']]    # Create test data   Y_bench <- matrix(rnorm(n_time * n_vox), n_time, n_vox)    # Create sampling frame and onsets for this size   sframe_bench <- fmrihrf::sampling_frame(blocklens = n_time, TR = TR)   n_trials <- min(60, floor(n_time / 10))   onsets_bench <- seq(10, n_time - 20, length.out = n_trials)    # Standard LSS with pre-built design (shared across methods)   grid_b <- fmrihrf::samples(sframe_bench, global = TRUE)   rset_b <- fmrihrf::regressor_set(onsets = onsets_bench,                                    fac = factor(seq_along(onsets_bench)),                                    hrf = fmrihrf::HRF_SPMG1,                                    duration = 0,                                    span = 30,                                    summate = FALSE)   dm_bench <- fmrihrf::evaluate(rset_b, grid = grid_b, precision = 0.1, method = \"conv\")    # Time standard LSS (R optimized)   time_r_opt <- system.time({     beta_r <- lss(Y_bench, dm_bench, method = \"r_optimized\")   })[3]    # Time standard LSS (C++ optimized)   time_cpp <- system.time({     beta_cpp <- lss(Y_bench, dm_bench, method = \"cpp_optimized\")   })[3]    # Time OASIS using the pre-built design (fair comparison)   time_oasis <- system.time({     beta_oasis <- lss(Y = Y_bench, X = dm_bench, method = \"oasis\")   })[3]    # Time OASIS when it also has to build the design internally   time_oasis_build <- system.time({     beta_oasis_build <- lss(       Y = Y_bench,       X = NULL,       method = \"oasis\",       oasis = list(         design_spec = list(           sframe = sframe_bench,           cond = list(             onsets = onsets_bench,             hrf = fmrihrf::HRF_SPMG1,             span = 30           )         )       )     )   })[3]    # Store results   benchmark_results <- rbind(benchmark_results, data.frame(     Size = size_name,     Timepoints = n_time,     Voxels = n_vox,     Trials = n_trials,     R_Optimized = time_r_opt,     CPP_Optimized = time_cpp,     OASIS = time_oasis,     OASIS_with_design = time_oasis_build,     Speedup_vs_R = time_r_opt / time_oasis,     Speedup_vs_CPP = time_cpp / time_oasis,     Design_Build_Overhead = time_oasis_build - time_oasis   )) }  # Display rounded results for readability numeric_cols <- setdiff(names(benchmark_results), \"Size\") benchmark_display <- benchmark_results benchmark_display[numeric_cols] <- lapply(benchmark_display[numeric_cols], function(x) round(x, 3)) print(benchmark_display) #>            Size Timepoints Voxels Trials R_Optimized CPP_Optimized OASIS #> elapsed   small        300    500     30       0.005         0.008 0.004 #> elapsed1 medium        400   2000     40       0.016         0.029 0.014 #> elapsed2  large        600   8000     60       0.076         0.172 0.062 #> elapsed3 xlarge        800  16000     60       0.179         0.197 0.157 #>          OASIS_with_design Speedup_vs_R Speedup_vs_CPP Design_Build_Overhead #> elapsed              0.016        1.250          2.000                 0.012 #> elapsed1             0.034        1.143          2.071                 0.020 #> elapsed2             0.093        1.226          2.774                 0.031 #> elapsed3             0.205        1.140          1.255                 0.048  # Visualize scaling par(mfrow = c(1, 2))  # Time vs dataset size ylim_max <- max(benchmark_results[, c(\"R_Optimized\", \"CPP_Optimized\", \"OASIS_with_design\")]) plot(benchmark_results$Voxels, benchmark_results$R_Optimized,      type = \"b\", col = \"blue\", pch = 19,      xlab = \"Number of Voxels\", ylab = \"Time (seconds)\",      main = \"Computation Time Scaling\",      ylim = c(0, ylim_max)) lines(benchmark_results$Voxels, benchmark_results$CPP_Optimized,       type = \"b\", col = \"darkgreen\", pch = 15) lines(benchmark_results$Voxels, benchmark_results$OASIS,       type = \"b\", col = \"red\", pch = 17) lines(benchmark_results$Voxels, benchmark_results$OASIS_with_design,       type = \"b\", col = \"red\", lty = 2, pch = 1) legend(\"topleft\",        c(\"R Optimized\", \"C++ Optimized\", \"OASIS (pre-built X)\", \"OASIS (build design)\"),        col = c(\"blue\", \"darkgreen\", \"red\", \"red\"),        pch = c(19, 15, 17, 1),        lty = c(1, 1, 1, 2))  # Speedup factor barplot(benchmark_results$Speedup_vs_R,         names.arg = benchmark_results$Size,         main = \"OASIS Speedup vs R Optimized\",         ylab = \"Speedup Factor\",         col = \"steelblue\") abline(h = 1, lty = 2, col = \"gray\") These benchmarks show that all three optimized backends sit within a few percent of one another across the regimes we tested. The R and fused C++ paths are already very efficient, and OASIS closely tracks them even as we scale to tens of thousands of voxels and dozens of trials. The `OASIS (build design)` curve highlights another key point: if you let OASIS construct the design inside the call you pay the additional cost of HRF convolution, so for apples-to-apples comparisons you should pre-build and reuse `X` just as you would for the other methods.  The exact ordering will vary with hardware, trial spacing, and nuisance structure—on some reruns OASIS edges ahead, on others the R or C++ backend wins by a similar margin. The takeaway is that OASIS keeps pace with the existing optimized solvers while additionally providing HRF-aware design construction, ridge regularization, whitening, and diagnostics.  ## Creating Custom HRF Functions  OASIS's flexibility extends to custom HRF functions, allowing you to implement novel hemodynamic models:   ``` r # Create custom double-gamma HRF custom_hrf <- function(t, a1 = 6, a2 = 16, b1 = 1, b2 = 1, c = 1/6) {   # Double gamma function   dgamma(t, a1, b1) - c * dgamma(t, a2, b2) }  # Wrap as HRF object custom_hrf_obj <- structure(   custom_hrf,   class = c(\"hrf\", \"function\"),   span = 30 )  # Use with OASIS beta_custom <- lss(   Y = Y[, 1:10],   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(         onsets = onsets[1:10],         hrf = custom_hrf_obj,         span = 30       )     )   ) )  cat(\"Custom HRF results:\\n\") #> Custom HRF results: cat(\"  Beta dimensions:\", dim(beta_custom), \"\\n\") #>   Beta dimensions: 10 10 cat(\"  Mean beta:\", round(mean(beta_custom), 3), \"\\n\") #>   Mean beta: 1.633"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"practical-guidance-for-ridge-parameter-selection","dir":"Articles","previous_headings":"","what":"Practical Guidance for Ridge Parameter Selection","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Choosing appropriate ridge parameters crucial optimal performance. little regularization leaves estimates unstable; much biases toward zero. ’s systematic approach selection: condition number (kappa) indicates numerical stability—lower values suggest stable estimates. tradeoff bias (reduced mean beta) variance (reduced SD) guides choice.","code":"# Test different ridge values ridge_values <- c(0, 0.001, 0.01, 0.05, 0.1) ridge_results <- list()  for (ridge in ridge_values) {   beta_r <- lss(     Y = Y[, 1:10],     X = NULL,     method = \"oasis\",     oasis = list(       design_spec = list(         sframe = sframe,         cond = list(onsets = onsets, hrf = HRF_SPMG1, span = 30)       ),       ridge_mode = \"fractional\",       ridge_x = ridge,       ridge_b = ridge     )   )    ridge_results[[as.character(ridge)]] <- list(     mean_beta = mean(abs(beta_r)),     sd_beta = sd(beta_r),     condition_number = kappa(cor(beta_r))   ) }  # Display results cat(\"Ridge parameter selection:\\n\") #> Ridge parameter selection: for (r in names(ridge_results)) {   cat(sprintf(\"  Ridge = %s: mean|beta| = %.3f, SD = %.3f, kappa = %.1f\\n\",               r, ridge_results[[r]]$mean_beta,               ridge_results[[r]]$sd_beta,               ridge_results[[r]]$condition_number)) } #>   Ridge = 0: mean|beta| = 1.059, SD = 0.610, kappa = 9.3 #>   Ridge = 0.001: mean|beta| = 1.057, SD = 0.609, kappa = 9.3 #>   Ridge = 0.01: mean|beta| = 1.041, SD = 0.604, kappa = 9.2 #>   Ridge = 0.05: mean|beta| = 0.973, SD = 0.580, kappa = 8.9 #>   Ridge = 0.1: mean|beta| = 0.899, SD = 0.552, kappa = 8.5"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"memory-usage-comparison","dir":"Articles","previous_headings":"","what":"Memory Usage Comparison","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"Understanding memory trade-offs OASIS standard LSS important choosing right method computational resources:  shown, OASIS requires upfront memory due precomputed matrices, investment enables computational efficiency. memory overhead noticeable multi-basis HRFs large numbers trials. memory-constrained systems, consider using blocked processing standard LSS large datasets.","code":"# Compare memory usage across methods memory_comparison <- function(n_time, n_vox, n_trials) {    # Calculate memory requirements (in MB)   double_size <- 8  # bytes per double    # Standard LSS memory (per iteration)   # Stores: current X_trial (n_time x 2), beta result (n_trials x n_vox)   lss_per_iter <- (n_time * 2 + n_vox) * double_size / 1024^2   lss_total <- lss_per_iter  # Only peak memory matters    # OASIS memory (upfront)   # Stores: X (n_time x n_trials), precomputed terms, results   oasis_X <- n_time * n_trials * double_size / 1024^2   oasis_precomp <- (n_trials^2 + n_trials * 3) * double_size / 1024^2  # Gram matrices   oasis_results <- n_trials * n_vox * double_size / 1024^2   oasis_total <- oasis_X + oasis_precomp + oasis_results    # For multi-basis (K=3), multiply by K   oasis_multibasis <- oasis_total * 3    return(data.frame(     Timepoints = n_time,     Voxels = n_vox,     Trials = n_trials,     LSS_MB = round(lss_total, 2),     OASIS_MB = round(oasis_total, 2),     OASIS_Multi_MB = round(oasis_multibasis, 2),     Ratio = round(oasis_total / lss_total, 1),     Ratio_Multi = round(oasis_multibasis / lss_total, 1)   )) }  # Test different dataset sizes mem_results <- rbind(   memory_comparison(200, 100, 15),    # Small   memory_comparison(300, 1000, 30),   # Medium   memory_comparison(400, 10000, 50),  # Large   memory_comparison(500, 50000, 100)  # Very large )  print(mem_results) #>   Timepoints Voxels Trials LSS_MB OASIS_MB OASIS_Multi_MB Ratio Ratio_Multi #> 1        200    100     15   0.00     0.04           0.11   9.5        28.6 #> 2        300   1000     30   0.01     0.31           0.92  25.0        75.0 #> 3        400  10000     50   0.08     3.99          11.96  48.4       145.2 #> 4        500  50000    100   0.39    38.61         115.82  99.2       297.7  # Visualize memory scaling par(mfrow = c(1, 2))  # Memory usage comparison plot(mem_results$Voxels, mem_results$LSS_MB,      type = \"b\", col = \"blue\", pch = 19,      xlab = \"Number of Voxels\", ylab = \"Memory (MB)\",      main = \"Memory Usage Comparison\",      log = \"xy\") lines(mem_results$Voxels, mem_results$OASIS_MB,       type = \"b\", col = \"red\", pch = 17) lines(mem_results$Voxels, mem_results$OASIS_Multi_MB,       type = \"b\", col = \"orange\", pch = 15) legend(\"topleft\", c(\"Standard LSS\", \"OASIS\", \"OASIS Multi-basis\"),        col = c(\"blue\", \"red\", \"orange\"), pch = c(19, 17, 15), lty = 1)  # Memory ratio barplot(t(as.matrix(mem_results[, c(\"Ratio\", \"Ratio_Multi\")])),         beside = TRUE,         names.arg = paste(mem_results$Voxels, \"vox\"),         main = \"Memory Usage Ratio (OASIS/LSS)\",         ylab = \"Memory Ratio\",         col = c(\"red\", \"orange\"),         legend.text = c(\"OASIS\", \"OASIS Multi-basis\")) abline(h = 1, lty = 2, col = \"gray\")"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"advanced-features-for-production-use","dir":"Articles","previous_headings":"","what":"Advanced Features for Production Use","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"deploying OASIS production analyses, several advanced features enhance efficiency robustness.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"memory-efficient-block-processing","dir":"Articles","previous_headings":"Advanced Features for Production Use","what":"Memory-Efficient Block Processing","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"datasets large fit memory, OASIS can process voxels blocks:","code":"# For very large datasets, use blocked processing block_size <- 1000 n_blocks <- ceiling(ncol(Y) / block_size)  # OASIS with blocked voxels oasis_config <- list(   design_spec = list(     sframe = sframe,     cond = list(onsets = onsets, hrf = HRF_SPMG1, span = 30)   ),   block_cols = block_size  # Process voxels in blocks )  beta_blocked <- lss(Y, X = NULL, method = \"oasis\", oasis = oasis_config)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"temporal-autocorrelation-correction","dir":"Articles","previous_headings":"Advanced Features for Production Use","what":"Temporal Autocorrelation Correction","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"fMRI data exhibits temporal autocorrelation can bias standard errors. OASIS can apply AR(1) prewhitening: Prewhitening improves validity statistical inference, particularly standard errors hypothesis tests.","code":"# OASIS with AR(1) prewhitening using new fmriAR integration beta_whitened <- lss(   Y = Y[, 1:10],   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(onsets = onsets[1:10], hrf = HRF_SPMG1, span = 30)     )   ),   prewhiten = list(     method = \"ar\",     p = 1  # AR(1) model   ) )  cat(\"AR(1) whitening applied using fmriAR\\n\") #> AR(1) whitening applied using fmriAR cat(\"  Beta dimensions:\", dim(beta_whitened), \"\\n\") #>   Beta dimensions: 10 10  # Advanced: Auto-select AR order beta_auto_ar <- lss(   Y = Y[, 1:10],   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(onsets = onsets[1:10], hrf = HRF_SPMG1, span = 30)     )   ),   prewhiten = list(     method = \"ar\",     p = \"auto\",  # Automatically select optimal AR order     p_max = 4    # Maximum order to consider   ) )  cat(\"\\nAuto AR order selection applied\\n\") #>  #> Auto AR order selection applied cat(\"  Beta dimensions:\", dim(beta_auto_ar), \"\\n\") #>   Beta dimensions: 10 10"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"advanced-prewhitening-with-fmriar","dir":"Articles","previous_headings":"Advanced Features for Production Use","what":"Advanced Prewhitening with fmriAR","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"integration fmriAR provides sophisticated noise modeling capabilities beyond simple AR(1): advanced options provide better noise modeling complex experimental designs data structures.","code":"# Voxel-specific AR parameters beta_voxel_ar <- lss(   Y = Y,   X = X,   method = \"oasis\",   prewhiten = list(     method = \"ar\",     p = 1,     pooling = \"voxel\"  # Estimate separate AR(1) for each voxel   ) )  # Run-aware AR estimation for multi-run data runs <- rep(1:2, each = n_timepoints/2) beta_run_ar <- lss(   Y = Y,   X = X,   method = \"oasis\",   prewhiten = list(     method = \"ar\",     p = \"auto\",     pooling = \"run\",  # Separate parameters per run     runs = runs   ) )  # ARMA models for complex noise structure beta_arma <- lss(   Y = Y,   X = X,   method = \"oasis\",   prewhiten = list(     method = \"arma\",     p = 2,  # AR order     q = 1   # MA order   ) )  # Parcel-based pooling for spatial regularization parcels <- kmeans(t(Y), centers = 10)$cluster  # Example parcellation beta_parcel <- lss(   Y = Y,   X = X,   method = \"oasis\",   prewhiten = list(     method = \"ar\",     p = \"auto\",     pooling = \"parcel\",     parcels = parcels   ) )"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"when-oasis-shines-use-case-recommendations","dir":"Articles","previous_headings":"","what":"When OASIS Shines: Use Case Recommendations","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"extensive testing application, ’ve identified scenarios OASIS provides greatest benefits. Rapid event-related designs inter-stimulus intervals 10 seconds benefit enormously OASIS’s efficient handling overlapping responses. method’s speed advantage becomes crucial dealing experiments containing hundreds thousands trials. exploratory analyses optimal HRF unknown, OASIS’s ability quickly evaluate multiple HRF models enables data-driven model selection. computational efficiency makes feasible test dozens HRF variants across multiple brain regions. Large-scale analyses involving whole-brain data multiple subjects particularly benefit OASIS’s speed. might take days traditional LSS can often completed hours OASIS, enabling iterative thorough analyses. Designs prone collinearity—whether rapid presentation, long HRFs, correlated experimental factors—benefit OASIS’s integrated ridge regression. regularization happens naturally within estimation framework rather requiring post-hoc adjustments.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"limitations-and-considerations","dir":"Articles","previous_headings":"","what":"Limitations and Considerations","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"OASIS offers substantial advantages, ’s important understand limitations simpler approaches might suffice. method requires memory iterative LSS approaches since constructs larger intermediate matrices. extremely large datasets memory-constrained systems, traditional LSS careful memory management might necessary. OASIS’s design specification system, powerful, learning curve. simple analyses pre-constructed design matrices, standard LSS might straightforward implement. mathematical optimizations make OASIS fast also make less transparent. need understand modify estimation procedure, traditional LSS implementation offers clearer code paths.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"looking-ahead","dir":"Articles","previous_headings":"","what":"Looking Ahead","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"OASIS represents current state---art LSS implementation, combining theoretical rigor computational efficiency. fMRI data continues grow resolution complexity, methods like OASIS become just useful essential practical analysis. Future developments might include adaptive regularization automatically selects optimal ridge parameters, integration machine learning frameworks end--end optimization, extensions handle even complex experimental designs. fmrilss package continue evolve advances, maintaining OASIS cornerstone efficient trial-wise estimation. Whether ’re working standard datasets pushing boundaries fMRI acquisition, OASIS provides tools need rigorous, efficient analysis.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"summary-and-next-steps","dir":"Articles","previous_headings":"","what":"Summary and Next Steps","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"vignette taken full capabilities OASIS method, basic usage advanced features. ’ve seen OASIS achieves dramatic speedups mathematical reformulation, provides numerical stability integrated ridge regression, handles flexible HRF models seamlessly, scales large datasets efficiently. continue journey, explore getting started vignette foundational LSS concepts, voxel-wise HRF vignette spatial modeling hemodynamic responses, package examples directory real-world applications. theoretical details OASIS prepared publication, offering deeper insights mathematical innovations make method possible. OASIS transforms LSS computationally intensive procedure practical tool everyday fMRI analysis. combining speed, flexibility, robustness, enables analyses otherwise infeasible, opening new possibilities understanding brain function event-related fMRI.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_method.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further Reading","title":"The OASIS Method: Optimized Analytic Single-pass Inverse Solution","text":"See vignette(\"getting_started\") LSS basics See vignette(\"voxel-wise-hrf\") spatial HRF variation Review examples/oasis_example.R additional demonstrations Consult OASIS paper (preparation) theoretical details","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"OASIS Theory: Algebra and Implementation Details","text":"Optimized Analytic Single-pass Inverse Solution (OASIS) extends Least Squares Separate (LSS) estimation algebraic reformulation enables single-pass computation trial estimates. single-basis HRF (K = 1) without ridge, OASIS reduces exactly closed-form LSS solution; per‑trial 2x2 normal equations . value OASIS batching solves efficiently generalizing algebra multi‑basis HRFs (2Kx2K) optional ridge diagnostics. document provides mathematical foundation implementation details.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"computational-intuition","dir":"Articles","previous_headings":"Motivation","what":"Computational Intuition","title":"OASIS Theory: Algebra and Implementation Details","text":"Standard LSS requires N separate GLM fits N trials, involving: 1. Matrix assembly: O(T²) operations 2. QR decomposition: O(T³) operations 3. Back-substitution: O(T²) operations Total complexity: O(NT³) N trials OASIS recognizes N models share substantial structure. factoring common computations, OASIS reduces complexity : 1. Single QR decomposition: O(T³) 2. Shared projections: O(NT²) 3. Per-trial solutions: O(N) Total complexity: O(T³ + NT²), significant reduction N large.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"visual-comparison-of-computational-scaling","dir":"Articles","previous_headings":"Motivation","what":"Visual Comparison of Computational Scaling","title":"OASIS Theory: Algebra and Implementation Details","text":"Computational complexity: Classical LSS vs OASIS","code":"# Demonstrate computational scaling N_trials <- c(10, 50, 100, 200, 500, 1000) T_points <- 200  # Fixed number of timepoints  # Simplified complexity models (arbitrary units) classical_ops <- N_trials * T_points^3 / 1e6  # O(NT³) oasis_ops <- (T_points^3 + N_trials * T_points^2) / 1e6  # O(T³ + NT²)  # Create comparison plot plot(N_trials, classical_ops, type='l', col='red', lwd=2,      xlab='Number of Trials', ylab='Computational Operations (millions)',      main='Computational Complexity: Classical LSS vs OASIS',      ylim=c(0, max(classical_ops))) lines(N_trials, oasis_ops, col='blue', lwd=2) legend('topleft', c('Classical LSS', 'OASIS'),        col=c('red', 'blue'), lwd=2, bty='n')  # Add shaded region showing computational savings polygon(c(N_trials, rev(N_trials)),         c(classical_ops, rev(oasis_ops)),         col=rgb(0.2, 0.8, 0.2, 0.3), border=NA) text(500, mean(c(classical_ops[4], oasis_ops[4])),      'Computational\\nSavings', col='darkgreen')"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"prerequisites","dir":"Articles","previous_headings":"Motivation","what":"Prerequisites","title":"OASIS Theory: Algebra and Implementation Details","text":"vignette assumes familiarity : - QR decomposition orthogonal projection matrices - Ridge regression regularization - Matrix calculus linear algebra - standard LSS formulation Code references point R/oasis_glue.R src/oasis_core.cpp implementations. Notation used throughout: Y∈ℝT×VY \\\\mathbb{R}^{T \\times V}: voxel data (TT time points, VV voxels) X=[x1,…,xN]∈ℝT×NX = [x_1, \\dots, x_N] \\\\mathbb{R}^{T \\times N}: trial regressors one condition Z∈ℝT×KzZ \\\\mathbb{R}^{T \\times K_z}: nuisance/experimental regressors shared across trials R=−QQTR = - QQ^T: orthogonal projector removing nuisance effects (QQ comes QR factorisation [Z,others][Z,\\text{others}]) Inner products denoted ⟨,b⟩=aTb\\langle , b \\rangle = ^T b first treat single-basis case (one regressor per trial) generalizing multi-basis HRFs.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"classical-lss-recap","dir":"Articles","previous_headings":"","what":"Classical LSS Recap","title":"OASIS Theory: Algebra and Implementation Details","text":"Classical LSS fits, trial jj, GLM design [xj,bj,Z][x_j, b_j, Z], bj=∑≠jxib_j = \\sum_{\\neq j} x_i. Solving model independently costs 𝒪(N)\\mathcal{O}(N) QR factorizations. Algebraically, trial-specific beta can expressed β̂j=⟨Rxj,RY⟩−⟨Rxj,Rbj⟩∥Rbj∥2⟨Rbj,RY⟩∥Rxj∥2−⟨Rxj,Rbj⟩2∥Rbj∥2. \\hat{\\beta}_j = \\frac{\\langle Rx_j, RY \\rangle - \\frac{\\langle Rx_j, Rb_j \\rangle}{\\|Rb_j\\|^2} \\langle Rb_j, RY \\rangle}{\\|Rx_j\\|^2 - \\frac{\\langle Rx_j, Rb_j \\rangle^2}{\\|Rb_j\\|^2}}. OASIS extracts reuses common computational components (projections, norms, cross-products) across trials, computing .","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"single-basis-oasis-algebra","dir":"Articles","previous_headings":"","what":"Single-Basis OASIS Algebra","title":"OASIS Theory: Algebra and Implementation Details","text":"residualising nuisance regressors define: aj=Rxja_j = Rx_j s=∑j=1Najs = \\sum_{j=1}^N a_j dj=∥aj∥2d_j = \\|a_j\\|^2 αj=⟨aj,s−aj⟩\\alpha_j = \\langle a_j, s - a_j \\rangle sj=∥s−aj∥2s_j = \\|s - a_j\\|^2 Let njv=⟨aj,RY⋅v⟩n_{jv} = \\langle a_j, RY_{\\cdot v} \\rangle mv=⟨s,RY⋅v⟩m_v = \\langle s, RY_{\\cdot v} \\rangle. pair (βj,γj)(\\beta_j, \\gamma_j) solving 2x2 system trial jj voxel vv obtained Gj[βjvγjv]=[njvmv−njv],Gj=[dj+λxαjαjsj+λb], G_j \\begin{bmatrix} \\beta_{jv} \\\\ \\gamma_{jv} \\end{bmatrix} = \\begin{bmatrix} n_{jv} \\\\ m_v - n_{jv} \\end{bmatrix}, \\quad G_j = \\begin{bmatrix} d_j + \\lambda_x & \\alpha_j \\\\ \\alpha_j & s_j + \\lambda_b \\end{bmatrix}, ridge penalties λx,λb≥0\\lambda_x, \\lambda_b \\ge 0. inverse GjG_j analytic, soβjv=(sj+λb)njv−αj(mv−njv)(dj+λx)(sj+λb)−αj2. \\beta_{jv} = \\frac{(s_j + \\lambda_b) n_{jv} - \\alpha_j (m_v - n_{jv})}{(d_j + \\lambda_x)(s_j + \\lambda_b) - \\alpha_j^2}. exactly oasis_betas_closed_form() implements (C++ file src/oasis_core.cpp). precomputation step oasis_precompute_design() produces aj,s,dj,αj,sja_j, s, d_j, \\alpha_j, s_j , oasis_AtY_SY_blocked() streams voxels obtain njvn_{jv} mvm_v.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"fractional-ridge","dir":"Articles","previous_headings":"Single-Basis OASIS Algebra","what":"Fractional Ridge","title":"OASIS Theory: Algebra and Implementation Details","text":"oasis$ridge_mode = \"fractional\" sets λx=ηx⋅d‾\\lambda_x = \\eta_x \\cdot \\bar{d} λb=ηb⋅s‾\\lambda_b = \\eta_b \\cdot \\bar{s}, d‾\\bar{d} s‾\\bar{s} means djd_j sjs_j. helper .oasis_resolve_ridge() implements scaling. Absolute ridge uses supplied values directly.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"standard-errors","dir":"Articles","previous_headings":"Single-Basis OASIS Algebra","what":"Standard Errors","title":"OASIS Theory: Algebra and Implementation Details","text":"Given Gj−1G_j^{-1} residual norm ∥RY∥2\\|RY\\|^2, variance βjv\\beta_{jv} Var(β̂jv)=σjv2(Gj−1)11,σjv2=SSEjvdof, \\operatorname{Var}(\\hat{\\beta}_{jv}) = \\sigma_{jv}^2 \\left( G_j^{-1} \\right)_{11}, \\quad \\sigma_{jv}^2 = \\frac{\\text{SSE}_{jv}}{\\text{dof}}, SSEjv=∥RY⋅v∥2−2(βjvnjv+γjv(mv−njv))+djβjv2+sjγjv2+2αjβjvγjv. \\text{SSE}_{jv} = \\|RY_{\\cdot v}\\|^2 - 2 (\\beta_{jv} n_{jv} + \\gamma_{jv} (m_v - n_{jv})) + d_j \\beta_{jv}^2 + s_j \\gamma_{jv}^2 + 2 \\alpha_j \\beta_{jv} \\gamma_{jv}. .oasis_se_from_norms() realises computation, reusing njvn_{jv}, mvm_v cached design scalars.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"multi-basis-extension","dir":"Articles","previous_headings":"","what":"Multi-Basis Extension","title":"OASIS Theory: Algebra and Implementation Details","text":"HRF contributes K>1K > 1 basis functions, trial columns Aj∈ℝT×KA_j \\\\mathbb{R}^{T \\times K}. Define S=∑jAjS = \\sum_j A_j Dj=AjTAjD_j = A_j^T A_j Cj=AjT(S−Aj)C_j = A_j^T (S - A_j) Ej=(S−Aj)T(S−Aj)E_j = (S - A_j)^T (S - A_j) Per voxel need N1=ATRYN1 = ^T RY (stacked NN blocks size KK) SY=STRYSY = S^T RY. block system [Dj+λxICjCjTEj+λbI][BjvΓjv]=[N1jvSYv−N1jv], \\begin{bmatrix} D_j + \\lambda_x & C_j \\\\ C_j^T & E_j + \\lambda_b \\end{bmatrix} \\begin{bmatrix} B_{jv} \\\\ \\Gamma_{jv} \\end{bmatrix} = \\begin{bmatrix} N1_{jv} \\\\ SY_v - N1_{jv} \\end{bmatrix}, Bjv∈ℝKB_{jv} \\\\mathbb{R}^K. oasisk_betas() solves 2Kx2K system via Cholesky factorisation. Ridge adds λxI\\lambda_x λbI\\lambda_b block diagonals. Compared single-basis path, shapes cached matrices differ; solve still analytic per trial/voxel block. companion oasisk_betas_se() extends SSE/variance calculation multi-basis case, using building blocks.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"hrf-aware-design-construction","dir":"Articles","previous_headings":"","what":"HRF-Aware Design Construction","title":"OASIS Theory: Algebra and Implementation Details","text":"OASIS can construct XX fly event specifications. .oasis_build_X_from_events() uses fmrihrf::regressor_set() generate trial-wise columns (optional -condition aggregates) given: cond$onsets: per-trial onset times cond$hrf: HRF object (canonical, FIR, multi-basis, user-defined) cond$span, precision, method: convolution controls design residualised nuisance regressors fed algebra . HRF definition enters directly, switching HRFs running grid searches automatically regenerates matching design. provide explicit X, OASIS skips step assumes already encoded HRF matrix.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"ar1-whitening","dir":"Articles","previous_headings":"","what":"AR(1) Whitening","title":"OASIS Theory: Algebra and Implementation Details","text":"oasis$whiten = \"ar1\" estimates common AR(1) coefficient residualised data. .oasis_ar1_whitener() computes ρ\\rho applies Toeplitz-safe differencing: ỹt={1−ρ2y1t=1,yt−ρyt−1t>1. \\tilde{y}_t = \\begin{cases} \\sqrt{1 - \\rho^2} y_1 & t = 1, \\\\ y_t - \\rho y_{t-1} & t > 1. \\end{cases} transformation applied XX nuisance regressors standard OASIS algebra runs. Whitening preserves single-pass benefits transformed data treated exactly like original inputs.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"diagnostics-output","dir":"Articles","previous_headings":"","what":"Diagnostics Output","title":"OASIS Theory: Algebra and Implementation Details","text":"oasis$return_diag = TRUE, OASIS returns precomputed design scalars: Single-basis: dj,αj,sjd_j, \\alpha_j, s_j (oasis_precompute_design()) Multi-basis: Dj,Cj,EjD_j, C_j, E_j (oasisk_precompute_design()) matrices useful checking trial collinearity, energy, effect ridge scaling.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"algorithm-summary","dir":"Articles","previous_headings":"","what":"Algorithm Summary","title":"OASIS Theory: Algebra and Implementation Details","text":"Putting everything together, single-basis solver proceeds follows: Residualise YY XX nuisance regressors, optionally whitening. Compute aj,s,dj,αj,sja_j, s, d_j, \\alpha_j, s_j (oasis_precompute_design). Stream voxels blocks, forming NY=ATRYN_Y = ^T RY SY=sTRYS_Y = s^T RY (oasis_AtY_SY_blocked). Apply ridge scaling (absolute fractional) obtain λx,λb\\lambda_x, \\lambda_b. trial, evaluate closed-form βjv\\beta_{jv} (γjv\\gamma_{jv} SEs requested). Optionally compute SEs diagnostics. multi-basis path swaps steps 2–5 block equivalents. cases, cost dominated single projection YY matrix–vector multiplies step 3, giving 𝒪(TV)\\mathcal{O}(T V) complexity small trial-dependent overhead.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"complexity-and-memory","dir":"Articles","previous_headings":"","what":"Complexity and Memory","title":"OASIS Theory: Algebra and Implementation Details","text":"Projection / whitening: 𝒪(TVKz)\\mathcal{O}(T V K_z) arithmetic, 𝒪(TKz)\\mathcal{O}(T K_z) memory confounds Precomputation: 𝒪(TN)\\mathcal{O}(T N) Products (blocked): 𝒪(TV)\\mathcal{O}(T V) block size tuning Closed-form solves: 𝒪(NV)\\mathcal{O}(N V) negligible constants (2x2 2Kx2K systems) Compared classical LSS (NN separate regressions), OASIS shaves repeated projections linear solves, yielding substantial speedups NN VV large.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/oasis_theory.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"OASIS Theory: Algebra and Implementation Details","text":"Mumford, J. ., Turner, B. O., Ashby, F. G., & Poldrack, R. . (2012). Deconvolving BOLD activation event-related designs multivoxel pattern classification analyses. NeuroImage, 59(3), 2636–2643. fmrilss source files R/oasis_glue.R src/oasis_core.cpp (implementation alignment).","code":"sessionInfo() #> R version 4.5.1 (2025-06-13) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> loaded via a namespace (and not attached): #>  [1] digest_0.6.37     desc_1.4.3        R6_2.6.1          fastmap_1.2.0     #>  [5] xfun_0.53         cachem_1.1.0      knitr_1.50        htmltools_0.5.8.1 #>  [9] rmarkdown_2.30    lifecycle_1.0.4   cli_3.6.5         pkgdown_2.1.3     #> [13] sass_0.4.10       textshaping_1.0.4 jquerylib_0.1.4   systemfonts_1.3.1 #> [17] compiler_4.5.1    tools_4.5.1       ragg_1.5.0        evaluate_1.0.5    #> [21] bslib_0.9.0       yaml_2.3.10       jsonlite_2.0.0    rlang_1.1.6       #> [25] fs_1.6.6"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Shared-Basis HRF Matching (SBHM) provides efficient middle ground global HRF selection (one HRF voxels) fully unconstrained voxel-wise HRF estimation. SBHM learns low-rank shared time basis library candidate HRFs, matches voxel best-fitting library member reduced coefficient space.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"the-problem-voxel-specific-hrfs-at-scale","dir":"Articles","previous_headings":"Introduction","what":"The Problem: Voxel-Specific HRFs at Scale","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Real fMRI data shows substantial HRF variability across brain regions individuals. single canonical HRF (like SPM’s double-gamma) often provides poor fit data. However, estimating completely unconstrained voxel-wise HRFs using FIR bases multi-basis models : Computationally expensive: Requires fitting many parameters per voxel Data hungry: Needs many trials stable estimates High variance: Individual voxel estimates can noisy without regularization","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"the-sbhm-solution","dir":"Articles","previous_headings":"Introduction","what":"The SBHM Solution","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"SBHM addresses challenges : Learning shared basis library physiologically plausible HRFs via SVD Fitting voxels low-dimensional basis space (typically r=4-8 dimensions) Matching voxel closest library member using cosine similarity Projecting trial-wise coefficients onto matched coordinates interpretable amplitudes approach provides: - Efficiency: Fit r parameters per voxel (vs. K×trials FIR) - Stability: Constrain estimates learned manifold plausible HRFs - Interpretability: voxel maps specific library HRF known parameters","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"when-to-use-sbhm","dir":"Articles","previous_headings":"Introduction","what":"When to Use SBHM","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Use SBHM : - expect voxel-specific HRF shapes want library-constrained - library large (50-200+ candidate HRFs) covering physiological variability - want computational efficiency interpretable per-voxel HRF assignments - need compare HRF parameters across voxels conditions Use global HRF selection : - single shared HRF sufficient analysis - Maximum computational efficiency critical - ROI anatomically homogeneous Use unconstrained voxel-wise HRF : - need discover completely novel HRF shapes - many trials can afford variance - want data-driven HRF discovery without parametric constraints Use multi-basis FIR : - need capture timing/shape variability within single trials - experimental design varies trial--trial (e.g., parametric modulation)","code":"library(fmrihrf) library(fmrilss)  # Derived convenience defaults for examples n_voxels_default <- if (fast_mode) 20 else 50 n_trials_default <- if (fast_mode) 6 else 10 ranks_default    <- if (fast_mode) 2:6 else 2:10"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"sbhm-workflow-overview","dir":"Articles","previous_headings":"","what":"SBHM Workflow Overview","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"SBHM pipeline consists four main steps, coordinated lss_sbhm() function: ’ll walk step examples, show end--end workflow.","code":"1. sbhm_build()    → Learn shared basis B from HRF library via SVD                      Returns: B (time basis), S (singular values), A (library coordinates)  2. sbhm_prepass()  → Fit aggregate model in basis space per voxel                      Returns: beta_bar (r×V coefficients)  3. sbhm_match()    → Match voxels to library via cosine similarity                      Returns: matched_idx, margin, alpha_hat  4. LSS + Project   → Run OASIS with K=r, project to scalar amplitudes                      Returns: trial-wise amplitudes (ntrials×V)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"step-1-building-the-shared-basis","dir":"Articles","previous_headings":"","what":"Step 1: Building the Shared Basis","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"first step create library candidate HRFs spanning physiological variability, learn low-rank basis via SVD.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"creating-an-hrf-library","dir":"Articles","previous_headings":"Step 1: Building the Shared Basis","what":"Creating an HRF Library","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# Define a parameter grid for gamma HRFs # Shape controls peak time, rate controls width shapes <- if (fast_mode) seq(6, 10, by = 2) else seq(5, 11, by = 1.5) rates  <- if (fast_mode) seq(0.8, 1.2, by = 0.2) else seq(0.7, 1.3, by = 0.15) param_grid <- expand.grid(shape = shapes, rate = rates)  cat(\"Library size:\", nrow(param_grid), \"HRFs\\n\") #> Library size: 9 HRFs  # Function to create gamma HRF with given parameters gamma_fun <- function(shape, rate) {   # Important: close over parameters so evaluation uses (shape, rate)   f <- function(t) fmrihrf::hrf_gamma(t, shape = shape, rate = rate)   fmrihrf::as_hrf(f, name = sprintf(\"gamma(s=%.2f,r=%.2f)\", shape, rate), span = 32) }  # Set up time grid sframe <- sampling_frame(blocklens = n_time, TR = TR)  # Build SBHM with rank r=6 sbhm <- sbhm_build(   library_spec = list(     fun = gamma_fun,     pgrid = param_grid,     span = 32,           # HRF duration in seconds     precision = 0.1,     # Time resolution for evaluation (0.1s steps)     method = \"conv\"      # Convolution method (vs. \"interp\")   ),   r = 6,                 # Target rank (number of basis functions)   sframe = sframe,   baseline = c(0, 0.5),  # Remove mean in first 0.5s (before response)   normalize = TRUE,      # L2 normalize library columns before SVD   shifts = NULL,         # Optional: time shifts to augment library (experimental)   ref = \"mean\"           # Reference HRF: \"mean\" (library average) or \"spmg1\" (SPM canonical) )  cat(\"\\nSBHM basis dimensions:\\n\") #>  #> SBHM basis dimensions: cat(\"  B (time basis):\", dim(sbhm$B), \"\\n\") #>   B (time basis): 160 6 cat(\"  S (singular values):\", length(sbhm$S), \"\\n\") #>   S (singular values): 6 cat(\"  A (library coords):\", dim(sbhm$A), \"\\n\") #>   A (library coords): 6 9  # Estimate total library variance via a higher-rank SVD for a more meaningful percent max_r <- min(12, n_time, nrow(sbhm$A)) sbhm_full <- sbhm_build(   library_spec = list(fun = gamma_fun, pgrid = param_grid, span = 32),   r = max_r,   sframe = sframe,   baseline = c(0, 0.5),   normalize = TRUE ) cat(\"  Variance explained by r=6:\",     round(100 * sum(sbhm$S^2) / sum(sbhm_full$S^2), 1), \"%\\n\") #>   Variance explained by r=6: 100 %"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"visualizing-the-learned-basis","dir":"Articles","previous_headings":"Step 1: Building the Shared Basis","what":"Visualizing the Learned Basis","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# Plot the learned time basis functions par(mfrow = c(2, 3), mar = c(3, 3, 2, 1)) for (i in 1:ncol(sbhm$B)) {   plot(sbhm$tgrid, sbhm$B[, i], type = \"l\", col = \"navy\", lwd = 2,        main = paste0(\"Basis \", i, \" (σ=\", round(sbhm$S[i], 2), \")\"),        xlab = \"Time (s)\", ylab = \"Amplitude\")   abline(h = 0, col = \"gray\", lty = 2)   grid() }"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"understanding-the-basis","dir":"Articles","previous_headings":"Step 1: Building the Shared Basis","what":"Understanding the Basis","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"basis functions capture principal modes variation HRF library: Basis 1 typically captures main hemodynamic response shape Basis 2-3 capture timing variations (earlier vs. later peaks) Basis 4-6 capture width undershoot variations singular values (S) indicate importance basis function. rapid drop-suggests redundancy library.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"choosing-the-rank-r","dir":"Articles","previous_headings":"Step 1: Building the Shared Basis","what":"Choosing the Rank (r)","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# Build SBHM with different ranks to see variance explained ranks <- ranks_default var_explained <- numeric(length(ranks))  for (i in seq_along(ranks)) {   sbhm_test <- sbhm_build(     library_spec = list(fun = gamma_fun, pgrid = param_grid, span = 32),     r = ranks[i],     sframe = sframe,     normalize = TRUE   )   var_explained[i] <- sum(sbhm_test$S^2) }  plot(ranks, var_explained / max(var_explained) * 100,      type = \"b\", pch = 19, col = \"navy\", lwd = 2,      xlab = \"Rank (r)\", ylab = \"Variance Explained (%)\",      main = \"Choosing SBHM Rank\") abline(h = 95, col = \"red\", lty = 2) grid() text(8, 97, \"95% threshold\", col = \"red\", pos = 3)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"library-coverage-intuition","dir":"Articles","previous_headings":"Step 1: Building the Shared Basis","what":"Library Coverage Intuition","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Guidelines choosing r: - Start r=6 gamma libraries, r=8-10 complex libraries - Aim 90-95% variance explained - Balance: larger r = better library coverage, parameters fit - Typical range: r=4 (simple) r=12 (complex)","code":"# Visualize a subset of library shapes reconstructed from B and A H_hat <- sbhm$B %*% sbhm$A  # T×K (rank-r reconstruction) K <- ncol(H_hat) sel <- unique(round(seq(1, K, length.out = min(K, 12)))) matplot(sbhm$tgrid, H_hat[, sel, drop = FALSE], type = \"l\", lty = 1,         col = colorRampPalette(c(\"#6baed6\", \"#08519c\"))(length(sel)), lwd = 1.5,         xlab = \"Time (s)\", ylab = \"Amplitude\",         main = \"Sample of Library HRFs (rank-r reconstruction)\") abline(h = 0, col = \"gray80\", lty = 2) grid()"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"understanding-the-reference-hrf","dir":"Articles","previous_headings":"Step 1: Building the Shared Basis","what":"Understanding the Reference HRF","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"ref parameter sbhm_build() determines reference HRF used : 1. Shrinkage target matching (pulls noisy estimates towards reference) 2. Orientation alignment via orient_ref (flips coefficients match reference polarity) use : - “mean” (recommended): Let library define typical response - “spmg1”: compatibility SPM analyses library exploratory reference stored sbhm$ref$alpha_ref (r-dimensional coordinates).","code":"# Option 1: Mean of library (default, data-driven) sbhm_mean <- sbhm_build(..., ref = \"mean\") # Reference = average of all library HRFs  # Option 2: SPM canonical HRF (theory-driven) sbhm_spm <- sbhm_build(..., ref = \"spmg1\") # Reference = SPM's double-gamma canonical HRF"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"step-2-generating-synthetic-data-with-known-hrf-variation","dir":"Articles","previous_headings":"","what":"Step 2: Generating Synthetic Data with Known HRF Variation","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"demonstrate SBHM’s ability recover voxel-specific HRFs, ’ll create synthetic data different voxels different HRF shapes library.","code":"# Experimental design n_voxels <- n_voxels_default n_trials <- n_trials_default # Place trial onsets safely within the acquisition window so HRFs are observed safe_end <- max(sbhm$tgrid) - 30  # leave HRF span margin (~30s) onsets <- seq(20, safe_end, length.out = n_trials)  # Assign each voxel a random HRF from the library set.seed(456) true_hrf_idx <- sample(ncol(sbhm$A), n_voxels, replace = TRUE)  # Generate trial-wise design using SBHM basis design_spec <- list(   sframe = sframe,   cond = list(onsets = onsets, duration = 0, span = 30) )  hrf_B <- sbhm_hrf(sbhm$B, sbhm$tgrid, sbhm$span) rr <- regressor(   onsets = onsets,   hrf = hrf_B,   duration = 0,   span = 30,   summate = FALSE ) # Build per-trial regressors explicitly (one K-column block per trial) regressors_by_trial <- vector(\"list\", n_trials) for (t in 1:n_trials) {   rr_t <- regressor(onsets = onsets[t], hrf = hrf_B, duration = 0, span = 30, summate = FALSE)   X_t  <- evaluate(rr_t, grid = sbhm$tgrid, precision = 0.1, method = \"conv\")   regressors_by_trial[[t]] <- X_t  # T×K }  # Create signal: each voxel uses its assigned HRF's coordinates Y <- matrix(rnorm(n_time * n_voxels, sd = 0.5), n_time, n_voxels) true_amplitudes <- matrix(rnorm(n_trials * n_voxels, mean = 2, sd = 0.5),                           n_trials, n_voxels)  for (v in 1:n_voxels) {   # Get the true HRF coordinates for this voxel   alpha_true <- sbhm$A[, true_hrf_idx[v]]    # Add signal: X_trials * alpha_true gives the per-trial regressors   # Each trial's amplitude scales this regressor   for (t in 1:n_trials) {     regressor_t <- regressors_by_trial[[t]] %*% alpha_true     Y[, v] <- Y[, v] + true_amplitudes[t, v] * regressor_t   } }  cat(\"Data generated:\\n\") #> Data generated: cat(\"  Y dimensions:\", dim(Y), \"\\n\") #>   Y dimensions: 160 20 cat(\"  True HRF assignments:\", length(unique(true_hrf_idx)), \"unique HRFs\\n\") #>   True HRF assignments: 9 unique HRFs"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"step-3-running-the-complete-sbhm-pipeline","dir":"Articles","previous_headings":"","what":"Step 3: Running the Complete SBHM Pipeline","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Now ’ll use lss_sbhm() recover voxel-specific HRFs trial amplitudes.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"prewhitening-for-autocorrelated-noise","dir":"Articles","previous_headings":"Step 3: Running the Complete SBHM Pipeline","what":"Prewhitening for Autocorrelated Noise","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"fMRI data temporally correlated noise (common short TR < 2s), prewhitening improves parameter estimates: use prewhitening: - TR < 2s (high temporal resolution) - Visual inspection shows autocorrelated residuals - Improved model fit critical (e.g., single-subject studies) Note: Prewhitening applied prepass stage. combined data_fac (PCA factorization).","code":"# Example with AR(1) prewhitening via fmriAR res_prewhiten <- lss_sbhm(   Y = Y, sbhm = sbhm, design_spec = design_spec,   prewhiten = list(method = \"ar\", p = 1L, pooling = \"global\", exact_first = \"ar1\"),   ... )"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"basic-sbhm-run","dir":"Articles","previous_headings":"Step 3: Running the Complete SBHM Pipeline","what":"Basic SBHM Run","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# Run end-to-end SBHM (no prewhitening for this synthetic example) res_sbhm <- lss_sbhm(   Y = Y,   sbhm = sbhm,   design_spec = design_spec,   Nuisance = NULL,   prewhiten = NULL,  # Set to list(method=\"ar\", p=1L, exact_first=\"ar1\") for short TR   prepass = list(     ridge = list(mode = \"fractional\", lambda = 0.01)  # Small ridge for stability   ),   match = list(     shrink = list(tau = 0, ref = sbhm$ref$alpha_ref),     topK = 1,     whiten = TRUE,      # Whiten by singular values before matching     orient_ref = TRUE   # Flip coefficient sign if anti-correlated with reference                         # Ensures consistent polarity across voxels   ),   oasis = list(     ridge_mode = \"fractional\",  # Scale ridge by mean eigenvalue (recommended)     ridge_x = 0.01,   # Regularize design matrix (X'X + ridge_x*I) for stability     ridge_b = 0.01    # Shrink coefficients towards zero for variance reduction   ),   return = \"both\"  # Return both amplitudes and coefficients )  cat(\"SBHM results:\\n\") #> SBHM results: cat(\"  Amplitude dimensions:\", dim(res_sbhm$amplitude), \"\\n\") #>   Amplitude dimensions: 6 20 cat(\"  Coefficients dimensions:\", dim(res_sbhm$coeffs_r), \"\\n\") #>   Coefficients dimensions: 6 6 20 cat(\"  Matched HRF indices:\", length(res_sbhm$matched_idx), \"\\n\") #>   Matched HRF indices: 20"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"step-4-evaluating-hrf-recovery","dir":"Articles","previous_headings":"","what":"Step 4: Evaluating HRF Recovery","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Let’s assess well SBHM recovered true HRF assignments.","code":"# Check matching accuracy matched_idx <- res_sbhm$matched_idx accuracy <- mean(matched_idx == true_hrf_idx)  cat(\"HRF Matching Accuracy:\", round(100 * accuracy, 1), \"%\\n\") #> HRF Matching Accuracy: 15 % cat(\"Confused voxels:\", sum(matched_idx != true_hrf_idx), \"/\", n_voxels, \"\\n\\n\") #> Confused voxels: 17 / 20  # Analyze matching confidence via margin (top1 - top2 score) cat(\"Matching confidence (margin):\\n\") #> Matching confidence (margin): cat(\"  Mean:\", round(mean(res_sbhm$margin), 3), \"\\n\") #>   Mean: 0.21 cat(\"  Median:\", round(median(res_sbhm$margin), 3), \"\\n\") #>   Median: 0.2 cat(\"  Range:\", round(range(res_sbhm$margin), 3), \"\\n\") #>   Range: 0.002 0.317  # Low margin indicates ambiguity low_confidence <- which(res_sbhm$margin < median(res_sbhm$margin)) cat(\"  Low-confidence voxels (<median):\", length(low_confidence), \"\\n\") #>   Low-confidence voxels (<median): 10"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"visualizing-matching-confidence","dir":"Articles","previous_headings":"Step 4: Evaluating HRF Recovery","what":"Visualizing Matching Confidence","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Interpreting margin: - High margin (>0.1): Clear winner, high confidence - Medium margin (0.05-0.1): Moderate confidence, top 2 candidates similar - Low margin (<0.05): Ambiguous, consider averaging top-K candidates Note top-K: lss_sbhm() uses hard assignment (top-1) internally. soft assignment, call sbhm_prepass() sbhm_match(topK = K) directly obtain weights topK_idx, combine candidate coordinates manually projecting via sbhm_project().","code":"hist(res_sbhm$margin, breaks = 20, col = \"skyblue\", border = \"white\",      main = \"Distribution of Matching Confidence (Margin)\",      xlab = \"Margin (Top1 - Top2 cosine score)\",      ylab = \"Number of Voxels\") abline(v = median(res_sbhm$margin), col = \"red\", lwd = 2, lty = 2) text(median(res_sbhm$margin), par(\"usr\")[4] * 0.9,      \"Median\", pos = 4, col = \"red\") grid()"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"comparing-recovered-vs--true-amplitudes","dir":"Articles","previous_headings":"Step 4: Evaluating HRF Recovery","what":"Comparing Recovered vs. True Amplitudes","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# Correlation between estimated and true amplitudes cor_amp <- cor(as.vector(res_sbhm$amplitude), as.vector(true_amplitudes))  plot(as.vector(true_amplitudes), as.vector(res_sbhm$amplitude),      pch = 19, col = adjustcolor(\"navy\", alpha.f = 0.3),      xlab = \"True Amplitude\", ylab = \"SBHM Estimated Amplitude\",      main = paste0(\"Amplitude Recovery (r = \", round(cor_amp, 3), \")\")) abline(0, 1, col = \"red\", lwd = 2, lty = 2) grid()"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"library-manifold-and-matches","dir":"Articles","previous_headings":"Step 4: Evaluating HRF Recovery","what":"Library Manifold and Matches","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# PCA of library coordinates (columns of A), overlay matched HRFs A_t <- t(sbhm$A)  # K×r pca <- prcomp(A_t, center = TRUE, scale. = TRUE) pc <- pca$x[, 1:2, drop = FALSE] plot(pc, pch = 16, col = \"gray70\",      xlab = \"PC1\", ylab = \"PC2\",      main = \"Library Coordinate Space (PCA)\")  # Highlight true and matched HRFs points(pc[unique(true_hrf_idx), , drop = FALSE], pch = 1, col = \"#2c7fb8\", lwd = 2) points(pc[unique(res_sbhm$matched_idx), , drop = FALSE], pch = 4, col = \"#d95f02\", lwd = 2) legend(\"topright\", bty = \"n\",        legend = c(\"Library\", \"True HRFs\", \"Matched HRFs\"),         pch = c(16, 1, 4), col = c(\"gray60\", \"#2c7fb8\", \"#d95f02\")) grid()"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"matched-vs--true-hrf-shapes","dir":"Articles","previous_headings":"Step 4: Evaluating HRF Recovery","what":"Matched vs. True HRF Shapes","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# Compare matched HRF shapes to true HRFs for a few voxels H_hat <- sbhm$B %*% sbhm$A vox_show <- head(order(-res_sbhm$margin), n = min(6, n_voxels))  # confident voxels par(mfrow = c(2, 3), mar = c(3, 3, 2, 1)) for (v in vox_show) {   h_true <- H_hat[, true_hrf_idx[v]]   h_match <- H_hat[, res_sbhm$matched_idx[v]]   rng <- range(c(h_true, h_match))   plot(sbhm$tgrid, h_true, type = \"l\", col = \"#2c7fb8\", lwd = 2,        main = paste0(\"Voxel \", v, \" (margin=\", round(res_sbhm$margin[v], 2), \")\"),        xlab = \"Time (s)\", ylab = \"HRF\", ylim = rng)   lines(sbhm$tgrid, h_match, col = \"#d95f02\", lwd = 2, lty = 2)   abline(h = 0, col = \"gray80\", lty = 2)   legend(\"topright\", bty = \"n\", cex = 0.9,          legend = c(\"True\", \"Matched\"), lty = c(1, 2), lwd = 2,          col = c(\"#2c7fb8\", \"#d95f02\"))   grid() }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"parameter-quick-reference","dir":"Articles","previous_headings":"Understanding SBHM Parameters","what":"Parameter Quick Reference","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"table summarizes SBHM parameters recommended starting values: Minimal call (uses defaults except required arguments):","code":"res <- lss_sbhm(Y = Y, sbhm = sbhm, design_spec = design_spec) # Uses: small fractional ridge (0.01), topK=1, whiten=TRUE, orient_ref=TRUE, no prewhitening"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"prepass-ridge-regularization","dir":"Articles","previous_headings":"Understanding SBHM Parameters","what":"Prepass Ridge Regularization","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"prepass fits aggregate coefficients per voxel. Ridge helps : - Collinear basis functions (library redundancy) - Low SNR data - Preventing extreme coefficients Guidelines: - Start lambda = 0.01 (small fractional ridge) - Increase 0.02–0.05 unstable low SNR - Use mode = \"fractional\" automatic scaling design energy","code":"# Example: varying ridge strength prepass = list(   ridge = list(     mode = \"fractional\",    # Scale by design energy     lambda = 0.01,          # 1% of mean eigenvalue     alpha_ref = sbhm$ref$alpha_ref  # Shrink towards reference   ) )"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"matching-shrinkage","dir":"Articles","previous_headings":"Understanding SBHM Parameters","what":"Matching Shrinkage","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Shrinkage pulls noisy voxel estimates towards reference matching, reducing influence noise HRF assignment. use: - Low SNR data: tau = 0.1-0.2 (stronger shrinkage) - High SNR data: tau = 0 (shrinkage) - Adaptive shrinkage: provide per-voxel SNR estimates","code":"match = list(   shrink = list(     tau = 0.1,              # Shrinkage strength (0 = none, 1 = full shrinkage to ref)     ref = sbhm$ref$alpha_ref,  # Shrinkage target (library mean by default)     snr = NULL              # Optional: per-voxel SNR for adaptive shrinkage   ) )"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"adaptive-shrinkage-with-snr","dir":"Articles","previous_headings":"Understanding SBHM Parameters > Matching Shrinkage","what":"Adaptive Shrinkage with SNR","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"heterogeneous SNR across voxels, adaptive shrinkage adjusts tau per voxel: Adaptive formula: voxel v, effective tau = base_tau / (1 + snr[v]), base_tau calibrated internally.","code":"# Compute per-voxel SNR from prepass (higher SNR = less shrinkage) # Simple proxy: ratio of aggregate fit variance to residual variance prepass_result <- sbhm_prepass(Y, sbhm, design_spec) # Proxy SNR: variance explained by aggregate fit vs. residual variance fit <- prepass_result$A_agg %*% prepass_result$beta_bar  # in residualized space signal_var <- apply(fit, 2, stats::var) total_var  <- apply(Y,   2, stats::var) residual_var <- pmax(total_var - signal_var, .Machine$double.eps) snr_voxel <- pmax(signal_var, .Machine$double.eps) / residual_var  match = list(   shrink = list(     tau = NULL,  # NULL triggers adaptive mode     ref = sbhm$ref$alpha_ref,     snr = snr_voxel  # Higher SNR voxels get less shrinkage   ) )"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"whitening","dir":"Articles","previous_headings":"Understanding SBHM Parameters","what":"Whitening","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Whitening divides coefficients singular values matching, equalizing importance basis directions. Effect: Without whitening, first basis (largest S) dominates matching. whitening, r dimensions contribute equally cosine score.","code":"match = list(   whiten = TRUE  # Recommended: divides by S before L2 normalization )"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"working-with-top-k-matches","dir":"Articles","previous_headings":"Advanced Use Cases","what":"Working with Top-K Matches","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Instead hard assignment single best HRF, can get top-K candidates weights.","code":"# Demonstrate top-K matching directly via sbhm_prepass + sbhm_match pre_small <- sbhm_prepass(   Y = Y[, 1:10],  # Subset for speed   sbhm = sbhm,   design_spec = design_spec ) m_top3 <- sbhm_match(   beta_bar = pre_small$beta_bar,   S = sbhm$S,   A = sbhm$A,   topK = 3,   whiten = TRUE )  cat(\"Top-3 matching (subset):\\n\") #> Top-3 matching (subset): cat(\"  Top indices dims:\", dim(m_top3$topK_idx), \"\\n\") #>   Top indices dims: 3 10 cat(\"  Weights dims:\", dim(m_top3$weights), \"\\n\") #>   Weights dims: 3 10  # Examine one voxel's top-3 matches v <- 1 cat(\"\\nVoxel\", v, \"top-3:\\n\") #>  #> Voxel 1 top-3: for (k in 1:3) {   cat(\"  Rank\", k, \": HRF\", m_top3$topK_idx[k, v],       \"(weight =\", round(m_top3$weights[k, v], 3), \")\\n\") } #>   Rank 1 : HRF 2 (weight = 0.373 ) #>   Rank 2 : HRF 5 (weight = 0.321 ) #>   Rank 3 : HRF 8 (weight = 0.307 )"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"soft-assignment-end-to-end","dir":"Articles","previous_headings":"Advanced Use Cases","what":"Soft Assignment End-to-End","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Hard assignment uses top-1 HRF per voxel. Soft assignment blends top-K candidates using softmax weights, projects trial-wise coefficients onto blended coordinate produce amplitudes. can reduce variance ambiguous voxels (small margin) cost bias. Conceptually: - Prepass: estimate per-voxel basis coefficients beta_bar. - Match: compute cosine scores whitened, L2-normalized space; convert top-K scores softmax weights (temperature = 1) per voxel. - Blend: compute weighted average unwhitened library coordinates [:, idx] using weights form alpha_soft (r×V). - LSS: obtain trial-wise r-dimensional coefficients (K=r) OASIS. - Project: amplitudes = inner product trial-wise coefficients alpha_soft.","code":"# 1) Prepass on all voxels pre_all <- sbhm_prepass(Y = Y, sbhm = sbhm, design_spec = design_spec)  # 2) Top-K matching with softmax weights topK <- 3 m_soft <- sbhm_match(   beta_bar = pre_all$beta_bar,   S = sbhm$S,   A = sbhm$A,   topK = topK,   whiten = TRUE,   orient_ref = TRUE )  # 3) Build blended coordinates alpha_soft (r×V) V <- ncol(Y) r <- nrow(pre_all$beta_bar) alpha_soft <- matrix(0, nrow = r, ncol = V) for (v in seq_len(V)) {   idx <- m_soft$topK_idx[, v]   w   <- m_soft$weights[, v]   # Weighted combination in unwhitened coordinate space   alpha_soft[, v] <- as.numeric(sbhm$A[, idx, drop = FALSE] %*% w) }  # 4) Trial-wise coefficients with OASIS (K=r) hrf_B <- sbhm_hrf(sbhm$B, sbhm$tgrid, sbhm$span) spec  <- design_spec; spec$cond$hrf <- hrf_B BetaMat <- lss(   Y = Y, X = NULL, Z = NULL, Nuisance = NULL,   method = \"oasis\",   oasis = list(design_spec = spec, K = r),   prewhiten = NULL ) stopifnot(nrow(BetaMat) %% r == 0) ntrials <- nrow(BetaMat) / r beta_rt <- array(BetaMat, dim = c(r, ntrials, V))  # 5) Soft-assignment amplitudes amps_soft <- sbhm_project(beta_rt, alpha_soft)  cat(\"Soft-assignment amplitudes dims:\", dim(amps_soft), \"\\n\") #> Soft-assignment amplitudes dims: 6 20  # Optional: compare against hard-assignment amplitudes from lss_sbhm() res_hard <- lss_sbhm(Y, sbhm, design_spec, return = \"amplitude\",                      prepass = list(ridge = list(mode = \"fractional\", lambda = 0.01)))  cor_soft_hard <- cor(as.vector(amps_soft), as.vector(res_hard$amplitude)) cat(\"Correlation (soft vs hard amplitudes):\", round(cor_soft_hard, 3), \"\\n\") #> Correlation (soft vs hard amplitudes): 0.912  # Focus on ambiguous voxels (low margin) where soft can help ambig <- which(res_hard$margin < median(res_hard$margin)) if (length(ambig) >= 3) {   cat(\"Ambiguous voxels (n):\", length(ambig), \"\\n\") } #> Ambiguous voxels (n): 10"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"built-in-soft-assignment-convenience","dir":"Articles","previous_headings":"Advanced Use Cases > Soft Assignment End-to-End","what":"Built-in Soft Assignment (Convenience)","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"convenience, lss_sbhm() can also perform soft blending internally avoid manual steps. Set match = list(topK = 3, soft_blend = TRUE) optionally blend_margin blend ambiguous voxels. Best practices soft assignment: - Use margin small (ambiguous matches). Consider blending voxels margin < threshold keep hard assignment otherwise. - weights softmax cosine scores (temperature = 1). sharpen smooth, can post-process scores custom temperature exponentiation. - Blending trades interpretability (single HRF per voxel) stability. Report matched index whether soft blending applied.","code":"res_soft <- lss_sbhm(   Y = Y,   sbhm = sbhm,   design_spec = design_spec,   prepass = list(ridge = list(mode = \"fractional\", lambda = 0.01)),   match = list(topK = 3, soft_blend = TRUE, blend_margin = median(res_hard$margin)),   return = \"amplitude\" )  cor_built <- cor(as.vector(res_soft$amplitude), as.vector(res_hard$amplitude)) cat(\"Correlation (built-in soft vs hard):\", round(cor_built, 3), \"\\n\") #> Correlation (built-in soft vs hard): 0.96"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"returning-coefficients-for-custom-analysis","dir":"Articles","previous_headings":"Advanced Use Cases","what":"Returning Coefficients for Custom Analysis","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# Get trial-wise coefficients in basis space res_coeffs <- lss_sbhm(   Y = Y[, 1:5],   sbhm = sbhm,   design_spec = design_spec,   return = \"coefficients\"  # Don't project to amplitudes )  cat(\"Coefficient array dimensions:\", dim(res_coeffs$coeffs_r), \"\\n\") #> Coefficient array dimensions: 6 6 5 cat(\"  [1] = basis dimension (r)\\n\") #>   [1] = basis dimension (r) cat(\"  [2] = number of trials\\n\") #>   [2] = number of trials cat(\"  [3] = number of voxels\\n\") #>   [3] = number of voxels  # You can now do custom projections or analyses in coefficient space"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"computational-cost","dir":"Articles","previous_headings":"Performance Considerations","what":"Computational Cost","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"SBHM cost scales : - Build: O(T²·K) library SVD (one-time) - Prepass: O(T²·r + T·r·V) per voxel aggregate fit - Match: O(r·K·V) cosine scores - LSS: O(T·r·N·V) N trials, r-dimensional design Compared alternatives: - vs. Global grid search: ~2-3x slower (r fits vs. 1 fit per candidate) - vs. Voxel-wise FIR: ~10-50x faster (r parameters vs. K·N parameters) - vs. Unconstrained per-voxel: ~5-20x faster","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"memory-usage","dir":"Articles","previous_headings":"Performance Considerations","what":"Memory Usage","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Peak memory scales : - Data: T·V (input fMRI data) - Design: T·r·N (trial-wise basis design) - Coefficients: r·N·V (trial-wise basis coefficients) T=300, V=100k, r=6, N=100: ~2GB coefficients.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"data-factorization-for-whole-brain-analysis","dir":"Articles","previous_headings":"Performance Considerations > Optimization Tips","what":"1. Data Factorization for Whole-Brain Analysis","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"large datasets (V > 50,000 voxels), use PCA factorization reduce memory computation: Trade-offs: - Speedup: ~V/q times faster prepass (e.g., 100x V=100k, q=100) - Accuracy: Loses information discarded components (minor q=100-200) - Limitation: use prewhiten (incompatible operations) use: V > 50,000 memory limited, prepass bottleneck.","code":"# Compute PCA decomposition: Y ≈ scores × loadings' pca <- prcomp(Y, center = TRUE, rank. = 100)  # Keep q=100 components Y_pca <- list(   scores = pca$x,       # T×q (time × components)   loadings = pca$rotation  # q×V (components × voxels) )  # Run SBHM on factored data (fits q \"meta-voxels\" instead of V voxels) res_sbhm <- lss_sbhm(   Y = Y_pca$scores,  # Pass scores as Y   sbhm = sbhm,   design_spec = design_spec,   prepass = list(     data_fac = list(       scores = Y_pca$scores,    # T×q       loadings = Y_pca$loadings # q×V (transposed internally)     )   ) )"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"process-in-roi-chunks","dir":"Articles","previous_headings":"Performance Considerations > Optimization Tips","what":"2. Process in ROI Chunks","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# For targeted analyses, process specific brain regions roi_mask <- my_roi_definition  # Logical vector Y_roi <- Y[, roi_mask] res_roi <- lss_sbhm(Y_roi, sbhm, design_spec)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"reduce-library-size-if-matching-is-slow","dir":"Articles","previous_headings":"Performance Considerations > Optimization Tips","what":"3. Reduce Library Size if Matching is Slow","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"","code":"# SBHM matching is O(r·K·V), so K matters for large V # Option: Cluster library in parameter space and use centroids param_clusters <- kmeans(param_grid, centers = 50) pgrid_reduced <- param_grid[unique(param_clusters$cluster), ]"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"library-augmentation-with-time-shifts-experimental","dir":"Articles","previous_headings":"Performance Considerations > Optimization Tips","what":"4. Library Augmentation with Time Shifts (Experimental)","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"shifts parameter augments library time-shifting HRF: Use cases: - Uncertain event timing (e.g., subject-paced tasks) - Modeling temporal jitter HRF onset - Exploratory analyses Caution: Increases library size (computational cost) risk overfitting.","code":"# Add shifted versions of each HRF (e.g., ±1s shifts) sbhm_shifted <- sbhm_build(   library_spec = list(fun = gamma_fun, pgrid = param_grid, span = 32),   r = 8,  # May need higher rank for shifted library   sframe = sframe,   shifts = c(-1, 0, 1),  # Create 3 versions: 1s early, on-time, 1s late   normalize = TRUE ) # Library size increases by length(shifts) factor (K → K×3)"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"low-matching-accuracy","dir":"Articles","previous_headings":"Troubleshooting","what":"Low Matching Accuracy","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Symptoms: Many voxels assigned incorrect HRFs, low correlation ground truth Possible causes: 1. Library doesn’t cover true HRF shapes 2. Low SNR making coefficient estimates noisy 3. Insufficient trials stable prepass fit Solutions: - Expand library cover parameter space - Increase shrinkage: tau = 0.1-0.2 - Add ridge regularization prepass - Increase number trials experiment","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"high-variance-in-amplitudes","dir":"Articles","previous_headings":"Troubleshooting","what":"High Variance in Amplitudes","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Symptoms: Amplitudes high trial--trial variability, low test-retest reliability Possible causes: 1. Matched HRF wrong (using wrong projection) 2. Low SNR data 3. Insufficient ridge OASIS step Solutions: - Check matching confidence via margin - Increase OASIS ridge: ridge_x = 0.05-0.1 - Use top-K averaging instead hard assignment","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"slow-computation","dir":"Articles","previous_headings":"Troubleshooting","what":"Slow Computation","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Symptoms: SBHM takes much longer expected Possible causes: 1. large library (K > 200) 2. High rank (r > 12) many voxels 3. Dense event designs many trials Solutions: - Reduce library size via clustering parameter space - Lower rank: start r=6 - Process ROIs separately instead whole-brain","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"Mumford et al. (2012). “Deconvolving BOLD activation event-related designs multivoxel pattern classification analyses.” NeuroImage. Lindquist et al. (2009). “Modeling hemodynamic response function fMRI.” NeuroImage. Friston et al. (1998). “Event-related fMRI: Characterizing differential responses.” NeuroImage.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/sbhm.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation","text":"SBHM provides efficient, interpretable approach voxel-specific HRF estimation : Learning shared basis physiologically plausible library Matching voxels library members via cosine similarity coefficient space Estimating trial-wise activations per-voxel HRF shapes Key advantages: - Computational efficiency (fit r parameters per voxel, K·N) - Built-regularization via library constraint - Interpretable HRF assignments confidence scores - Integrates seamlessly OASIS LSS framework Next steps: - See ?sbhm_build library construction details - See ?lss_sbhm full parameter documentation - See “Voxel-wise HRF” vignette unconstrained alternatives - See “OASIS Method” vignette related approaches","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"why-hrf-variability-matters","dir":"Articles","previous_headings":"","what":"Why HRF Variability Matters","title":"Voxel-wise HRF Modeling with fmrilss","text":"Voxel-wise HRF modeling addresses spatial subject-specific differences hemodynamic response can bias trial-wise estimates. Vascular properties, neurovascular coupling, acquisition protocols influence HRF, relying single canonical shape can misrepresent activation patterns. vignette shows estimate voxel-specific HRFs incorporate LSS analyses. Familiarity core LSS workflow fmrihrf basics assumed. Alternative approach: library-constrained voxel-specific HRF estimation computational efficiency, see Shared-Basis HRF Matching (SBHM) dedicated SBHM vignette (vignette(\"sbhm\", package = \"fmrilss\")). SBHM provides interpretable per-voxel HRF assignments 5-20x faster fully unconstrained voxel-wise estimation.","code":"library(fmrilss) library(fmrihrf) set.seed(123)  # Helper function to create design matrix using fmrihrf API # (design_matrix is not exported from fmrihrf, so we create a wrapper) design_matrix <- function(sframe, conditions, tr_per_trial = FALSE) {   # Get block info from sframe   n_blocks <- length(fmrihrf::blocklens(sframe))    if (tr_per_trial) {     # Create trial-wise design (one column per trial)     X_list <- list()     for (cond in conditions) {       n_trials <- length(cond$onsets)       # Each trial gets its own regressor       X_trial <- fmrihrf::regressor_design(         onsets = cond$onsets,         fac = factor(1:n_trials),  # Each trial is its own level         block = rep(1, n_trials),  # Assume single block for simplicity         sframe = sframe,         hrf = cond$hrf,         duration = if (!is.null(cond$duration)) cond$duration else 0,         span = if (!is.null(cond$span)) cond$span else 30,         precision = 0.1,         method = \"conv\",         summate = FALSE  # Don't sum across trials       )       X_list[[length(X_list) + 1]] <- X_trial     }     X <- do.call(cbind, X_list)   } else {     # Create aggregate design (one column per condition)     X_list <- list()     for (cond in conditions) {       n_trials <- length(cond$onsets)       # All trials in same condition get summed       X_cond <- fmrihrf::regressor_design(         onsets = cond$onsets,         fac = factor(rep(1, n_trials)),  # All trials same level         block = rep(1, n_trials),         sframe = sframe,         hrf = cond$hrf,         duration = if (!is.null(cond$duration)) cond$duration else 0,         span = if (!is.null(cond$span)) cond$span else 30,         precision = 0.1,         method = \"conv\",         summate = TRUE  # Sum across trials in condition       )       X_list[[length(X_list) + 1]] <- X_cond     }     X <- do.call(cbind, X_list)   }    list(X = as.matrix(X)) }"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"building-intuition-through-simulation","dir":"Articles","previous_headings":"","what":"Building Intuition Through Simulation","title":"Voxel-wise HRF Modeling with fmrilss","text":"best way understand impact HRF variability see action. Let’s create controlled simulation know ground truth can observe different analysis approaches perform.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"creating-data-with-variable-hrfs","dir":"Articles","previous_headings":"Building Intuition Through Simulation","what":"Creating Data with Variable HRFs","title":"Voxel-wise HRF Modeling with fmrilss","text":"’ll simulate experiment rapid stimulus presentation, different voxels subtly different HRF characteristics. mimics might happen analyzing data regions different vascular properties:","code":"# Simulation parameters n_time <- 200      # Time points n_vox  <- 5        # Number of voxels TR     <- 1.0      # Repetition time  # Create sampling frame sframe <- fmrihrf::sampling_frame(blocklens = n_time, TR = TR)  # Rapid, jittered event train with ISI ~ U(3, 9) seconds set.seed(123) start_time <- 10 isi <- runif(200, min = 3, max = 9)                   # generous upper bound onsets_cont <- c(start_time, start_time + cumsum(isi)) onsets <- onsets_cont[onsets_cont < (n_time - 20)]     # leave tail for HRF span n_trials <- length(onsets)  events <- data.frame(   onset = onsets,   duration = rep(1, n_trials),   condition = rep(\"task\", n_trials) )  # Generate voxel-specific HRF parameters # Each voxel has slightly different HRF characteristics voxel_hrfs <- list() for (v in 1:n_vox) {   # Vary peak time and width across voxels   peak_shift <- (v - 3) * 0.5  # Range: -1 to +1 seconds   width_scale <- 1 + (v - 3) * 0.1  # Range: 0.8 to 1.2    # Create modified HRF parameters   # Use SPMG1 as base and modify peak and undershoot   base_hrf <- HRF_SPMG1    # Modify the HRF parameters   # SPMG1 uses double gamma with peak around 5s and undershoot around 15s   voxel_hrfs[[v]] <- fmrihrf::HRF(     fun = function(t) {       # Shift the peak time       t_shifted <- t - peak_shift       # Apply width scaling (preserve area by multiplying by scale)       base_response <- base_hrf(t_shifted / width_scale) * width_scale       base_response     },     name = paste0(\"voxel_\", v, \"_hrf\"),     span = attr(base_hrf, \"span\"),     nbasis = attr(base_hrf, \"nbasis\")   ) }  # Generate true betas for each trial and voxel true_betas <- matrix(rnorm(n_trials * n_vox, mean = 1, sd = 0.3),                      nrow = n_trials, ncol = n_vox)  # Create time series data Y <- matrix(0, n_time, n_vox)  # For each voxel, create signal with voxel-specific HRF for (v in 1:n_vox) {   # Create design matrix for this voxel   dm <- design_matrix(     sframe = sframe,     conditions = list(       list(onsets = events$onset,            hrf = voxel_hrfs[[v]],            name = \"task\")     ),     tr_per_trial = TRUE   )    # Generate signal for this voxel   Y[, v] <- dm$X %*% true_betas[, v] }  # Add realistic noise with AR(1) structure noise_sd <- 0.5 ar_coef <- 0.3 for (v in 1:n_vox) {   # Generate independent innovations   innovations <- rnorm(n_time, sd = noise_sd)    # Apply AR(1) process   noise <- numeric(n_time)   noise[1] <- innovations[1]   for (t in 2:n_time) {     noise[t] <- ar_coef * noise[t-1] + sqrt(1 - ar_coef^2) * innovations[t]   }    Y[, v] <- Y[, v] + noise }  # Name the voxels colnames(Y) <- paste0(\"V\", 1:n_vox)  cat(\"Created synthetic data:\\n\") #> Created synthetic data: cat(\"  Time points:\", n_time, \"\\n\") #>   Time points: 200 cat(\"  Trials:\", n_trials, \"\\n\") #>   Trials: 26 cat(\"  Voxels:\", n_vox, \"\\n\") #>   Voxels: 5 cat(\"  Signal-to-noise ratio:\", round(var(Y[,1] - noise) / var(noise), 2), \"\\n\") #>   Signal-to-noise ratio: 5.76"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"the-standard-approach-and-its-limitations","dir":"Articles","previous_headings":"","what":"The Standard Approach and Its Limitations","title":"Voxel-wise HRF Modeling with fmrilss","text":"apply standard LSS canonical HRF data actually contains HRF variability, ’re making assumption may hold. Let’s see happens: Standard LSS treats every voxel shared HRF, deviations template translate biased betas.","code":"# Create design matrix with canonical HRF dm_standard <- design_matrix(   sframe = sframe,   conditions = list(     list(onsets = events$onset,          hrf = HRF_SPMG1,  # Canonical HRF for all voxels          name = \"task\")   ),   tr_per_trial = TRUE )  # Run standard LSS standard_betas <- lss(Y, dm_standard$X, method = \"r_optimized\")  cat(\"Standard LSS beta estimates (first 3 trials, all voxels):\\n\") #> Standard LSS beta estimates (first 3 trials, all voxels): print(round(standard_betas[1:3, ], 2)) #>           V1   V2   V3   V4   V5 #> Trial_1 0.41 0.56 0.74 0.73 0.39 #> Trial_2 1.08 1.27 1.05 1.38 1.21 #> Trial_3 1.02 0.51 1.13 0.47 0.89"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"estimating-voxel-specific-hrfs","dir":"Articles","previous_headings":"","what":"Estimating Voxel-Specific HRFs","title":"Voxel-wise HRF Modeling with fmrilss","text":"account HRF variability, need first estimate voxel’s HRF characteristics. One powerful approach uses multi-basis set can capture different aspects HRF variation. SPM software popularized three-component basis set consisting canonical HRF, temporal derivative (capturing shifts peak time), dispersion derivative (capturing changes width): basis weights describe latency width shifts relative canonical HRF: temporal derivative tracks peak timing dispersion derivative tracks response width.","code":"# Step 1: Estimate voxel-specific HRF using multi-basis approach # We'll use SPMG3 which includes canonical HRF plus temporal and dispersion derivatives  # Create multi-basis design matrix dm_multibasis <- design_matrix(   sframe = sframe,   conditions = list(     list(onsets = events$onset,          hrf = HRF_SPMG3,  # 3-basis set          name = \"task\")   ),   tr_per_trial = FALSE  # Aggregate for HRF estimation )  # Estimate HRF basis weights for each voxel hrf_weights <- matrix(NA, 3, n_vox)  # 3 basis functions  for (v in 1:n_vox) {   # Simple GLM to estimate basis weights   fit <- lm(Y[, v] ~ dm_multibasis$X - 1)   hrf_weights[, v] <- coef(fit) }  cat(\"Estimated HRF basis weights (3 bases x\", n_vox, \"voxels):\\n\") #> Estimated HRF basis weights (3 bases x 5 voxels): print(round(hrf_weights, 2)) #>       [,1]  [,2]  [,3]  [,4]  [,5] #> [1,]  1.31  1.34  1.49  1.47  1.38 #> [2,] -1.33 -1.21 -1.01 -1.25 -1.01 #> [3,]  1.23  1.61  1.32  1.68  1.31  # Normalize weights (optional, for interpretation) hrf_weights_norm <- sweep(hrf_weights, 2, hrf_weights[1,], \"/\") cat(\"\\nNormalized weights (relative to canonical):\\n\") #>  #> Normalized weights (relative to canonical): print(round(hrf_weights_norm, 2)) #>       [,1] [,2]  [,3]  [,4]  [,5] #> [1,]  1.00  1.0  1.00  1.00  1.00 #> [2,] -1.02 -0.9 -0.68 -0.85 -0.73 #> [3,]  0.94  1.2  0.89  1.14  0.95"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"applying-voxel-specific-hrfs-in-lss","dir":"Articles","previous_headings":"","what":"Applying Voxel-Specific HRFs in LSS","title":"Voxel-wise HRF Modeling with fmrilss","text":"HRF estimates hand, can now perform LSS using voxel’s specific hemodynamic response profile. two-stage approach first characterizes HRF, uses characterization accurate trial-wise estimation:","code":"# For demonstration, we'll use a simplified approach # In practice, you might use lss_with_hrf() with the appropriate backend  voxel_betas <- matrix(NA, n_trials, n_vox)  for (v in 1:n_vox) {   # Create voxel-specific design matrix using estimated weights   # Weight the basis functions by the estimated coefficients   X_voxel <- matrix(0, n_time, n_trials)    for (trial in 1:n_trials) {     # Create trial-specific regressors for each basis     dm_trial <- design_matrix(       sframe = sframe,       conditions = list(         list(onsets = events$onset[trial],              hrf = HRF_SPMG3,              name = \"trial\")       ),       tr_per_trial = FALSE     )      # Combine bases using voxel-specific weights     X_voxel[, trial] <- dm_trial$X %*% hrf_weights[, v]   }    # Run LSS for this voxel with its specific HRF   voxel_betas[, v] <- lss(Y[, v, drop = FALSE], X_voxel, method = \"r_optimized\") }  cat(\"Voxel-wise HRF LSS beta estimates (first 3 trials, all voxels):\\n\") #> Voxel-wise HRF LSS beta estimates (first 3 trials, all voxels): print(round(voxel_betas[1:3, ], 2)) #>      [,1] [,2] [,3] [,4] [,5] #> [1,] 0.61 0.78 0.73 0.86 0.55 #> [2,] 1.01 1.15 0.73 0.96 0.95 #> [3,] 0.90 0.64 0.98 0.57 0.84"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"the-oasis-alternative","dir":"Articles","previous_headings":"","what":"The OASIS Alternative","title":"Voxel-wise HRF Modeling with fmrilss","text":"OASIS method provides elegant alternative can handle HRF estimation LSS unified framework. Rather requiring separate stages, OASIS incorporates HRF flexibility directly estimation process, often improved computational efficiency: OASIS handles HRF expansion LSS solve one pass, limits redundant projections dense event-related designs.","code":"# OASIS can automatically handle HRF estimation and LSS in one step oasis_betas <- lss(   Y = Y,   X = NULL,   method = \"oasis\",   oasis = list(     design_spec = list(       sframe = sframe,       cond = list(         onsets = events$onset,         hrf = HRF_SPMG3,  # Multi-basis HRF         span = 30       )     ),     ridge_mode = \"fractional\",     ridge_x = 0.01,  # Small ridge for stability     ridge_b = 0.01   ) )  # OASIS returns results for each basis function # Extract canonical component (first basis) oasis_canonical <- oasis_betas[seq(1, nrow(oasis_betas), by = 3), ]  cat(\"OASIS beta estimates (canonical component, first 3 trials):\\n\") #> OASIS beta estimates (canonical component, first 3 trials): print(round(oasis_canonical[1:3, ], 2)) #>           V1   V2   V3   V4   V5 #> Trial_1 0.69 0.92 0.98 1.23 0.71 #> Trial_4 1.14 1.45 0.95 1.29 1.03 #> Trial_7 1.36 0.92 1.34 0.88 1.20"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"evaluating-the-approaches","dir":"Articles","previous_headings":"","what":"Evaluating the Approaches","title":"Voxel-wise HRF Modeling with fmrilss","text":"Let’s quantitatively compare well method recovers true beta values used generate synthetic data:  Points closer diagonal line indicate better recovery true betas.","code":"# Calculate correlations with true betas cor_standard <- cor(as.vector(standard_betas), as.vector(true_betas)) cor_voxel <- cor(as.vector(voxel_betas), as.vector(true_betas)) cor_oasis <- cor(as.vector(oasis_canonical), as.vector(true_betas))  # Calculate RMSE rmse_standard <- sqrt(mean((standard_betas - true_betas)^2)) rmse_voxel <- sqrt(mean((voxel_betas - true_betas)^2)) rmse_oasis <- sqrt(mean((oasis_canonical - true_betas)^2))  # Create comparison table comparison <- data.frame(   Method = c(\"Standard LSS\", \"Voxel-wise HRF\", \"OASIS\"),   Correlation = round(c(cor_standard, cor_voxel, cor_oasis), 3),   RMSE = round(c(rmse_standard, rmse_voxel, rmse_oasis), 3) )  print(comparison) #>           Method Correlation  RMSE #> 1   Standard LSS       0.592 0.385 #> 2 Voxel-wise HRF       0.740 0.233 #> 3          OASIS       0.684 0.473  # Visualization par(mfrow = c(1, 3), mar = c(4, 4, 3, 1))  # Standard LSS plot(true_betas, standard_betas,      xlab = \"True Betas\", ylab = \"Estimated Betas\",      main = paste(\"Standard LSS\\n(r =\", round(cor_standard, 2), \")\"),      pch = 19, col = rep(1:n_vox, each = n_trials),      xlim = range(true_betas), ylim = range(true_betas)) abline(0, 1, lty = 2, col = \"gray\")  # Voxel-wise HRF plot(true_betas, voxel_betas,      xlab = \"True Betas\", ylab = \"Estimated Betas\",      main = paste(\"Voxel-wise HRF\\n(r =\", round(cor_voxel, 2), \")\"),      pch = 19, col = rep(1:n_vox, each = n_trials),      xlim = range(true_betas), ylim = range(true_betas)) abline(0, 1, lty = 2, col = \"gray\")  # OASIS plot(true_betas, oasis_canonical,      xlab = \"True Betas\", ylab = \"Estimated Betas\",      main = paste(\"OASIS\\n(r =\", round(cor_oasis, 2), \")\"),      pch = 19, col = rep(1:n_vox, each = n_trials),      xlim = range(true_betas), ylim = range(true_betas)) abline(0, 1, lty = 2, col = \"gray\")  legend(\"topleft\", legend = paste(\"Voxel\", 1:n_vox),        col = 1:n_vox, pch = 19, cex = 0.8)"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"when-to-consider-voxel-wise-hrf-modeling","dir":"Articles","previous_headings":"","what":"When to Consider Voxel-wise HRF Modeling","title":"Voxel-wise HRF Modeling with fmrilss","text":"Use voxel-wise HRF modeling : - regions differ vascular architecture (e.g., motor vs. visual cortex); - cohorts show altered neurovascular coupling (aging, clinical populations, medication effects); - high-resolution acquisitions expose layer column specific responses; - sessions long enough HRF characteristics drift time.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"computational-strategies-for-large-scale-analysis","dir":"Articles","previous_headings":"","what":"Computational Strategies for Large-Scale Analysis","title":"Voxel-wise HRF Modeling with fmrilss","text":"Whole-brain analyses expensive, match backend workload:","code":"# C++ backend for medium-sized data betas_cpp <- lss(Y, X, method = \"cpp_optimized\")  # For very large data with multiple cores # Removed example of parallel backend to reduce clutter  # OASIS method is often fastest for complex designs betas_oasis <- lss(Y, X = NULL, method = \"oasis\",                    oasis = list(design_spec = design_spec))"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Voxel-wise HRF Modeling with fmrilss","text":"Choose backend according data size available hardware: R implementations work pilot subsets, whole-brain studies typically require optimized C++ path OASIS.","code":"#> R version 4.5.1 (2025-06-13) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] fmrihrf_0.1.0.9000 fmrilss_0.1.0      #>  #> loaded via a namespace (and not attached): #>  [1] Matrix_1.7-3        gtable_0.3.6        jsonlite_2.0.0      #>  [4] compiler_4.5.1      Rcpp_1.1.0          assertthat_0.2.1    #>  [7] jquerylib_0.1.4     splines_4.5.1       systemfonts_1.3.1   #> [10] scales_1.4.0        textshaping_1.0.4   uuid_1.2-1          #> [13] yaml_2.3.10         fastmap_1.2.0       lattice_0.22-7      #> [16] ggplot2_4.0.0       R6_2.6.1            labeling_0.4.3      #> [19] knitr_1.50          desc_1.4.3          bslib_0.9.0         #> [22] RColorBrewer_1.1-3  rlang_1.1.6         cachem_1.1.0        #> [25] xfun_0.53           fs_1.6.6            sass_0.4.10         #> [28] S7_0.2.0            viridisLite_0.4.2   memoise_2.0.1       #> [31] cli_3.6.5           withr_3.0.2         magrittr_2.0.4      #> [34] pkgdown_2.1.3       digest_0.6.37       grid_4.5.1          #> [37] bigmemory.sri_0.1.8 lifecycle_1.0.4     bigmemory_4.6.4     #> [40] vctrs_0.6.5         evaluate_1.0.5      glue_1.8.0          #> [43] farver_2.1.2        numDeriv_2016.8-1.1 fmriAR_0.1.0        #> [46] ragg_1.5.0          purrr_1.1.0         rmarkdown_2.30      #> [49] tools_4.5.1         htmltools_0.5.8.1"},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"choosing-the-right-method-for-your-study","dir":"Articles","previous_headings":"","what":"Choosing the Right Method for Your Study","title":"Voxel-wise HRF Modeling with fmrilss","text":"Method selection guidelines: - lss() canonical HRF sufficient responses expected homogeneous compute needs modest; - voxel-wise HRF LSS prioritizes accuracy anatomy pathology implies heterogeneous responses; - OASIS preferred rapid-event designs want single-step solve HRF flexibility ridge control.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"practical-recommendations","dir":"Articles","previous_headings":"","what":"Practical Recommendations","title":"Voxel-wise HRF Modeling with fmrilss","text":"Practical recommendations: - validate HRF assumptions independent data held-runs; - check signal--noise ratio supports estimation additional HRF parameters; - add complexity simpler models prove insufficient; - record HRF modeling choices reproducibility.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"looking-forward","dir":"Articles","previous_headings":"","what":"Looking Forward","title":"Voxel-wise HRF Modeling with fmrilss","text":"Voxel-wise HRF modeling improves trial-wise beta estimation accounting spatial variability hemodynamic response. fmrilss pipeline designed incorporate newer HRF estimation strategies appear literature.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/articles/voxel-wise-hrf.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further Reading","title":"Voxel-wise HRF Modeling with fmrilss","text":"vignette(\"getting_started\") LSS basics vignette(\"oasis_method\") advanced OASIS features ?fmrihrf HRF model options Mumford et al. (2012) core LSS theory","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Brad Buchsbaum. Maintainer.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Buchsbaum B (2025). fmrilss: Least Squares Separate (LSS) Analysis fMRI Data. R package version 0.1.0, https://bbuchsbaum.github.io/fmrilss/.","code":"@Manual{,   title = {fmrilss: Least Squares Separate (LSS) Analysis for fMRI Data},   author = {Brad Buchsbaum},   year = {2025},   note = {R package version 0.1.0},   url = {https://bbuchsbaum.github.io/fmrilss/}, }"},{"path":"https://bbuchsbaum.github.io/fmrilss/index.html","id":"fmrilss","dir":"","previous_headings":"","what":"Least Squares Separate (LSS) Analysis for fMRI Data","title":"Least Squares Separate (LSS) Analysis for fMRI Data","text":"Least Squares Separate (LSS) Analysis fMRI Data.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Least Squares Separate (LSS) Analysis for fMRI Data","text":"fmrilss package provides efficient flexible implementation Least Squares Separate (LSS) method fMRI analysis, proposed Mumford et al. (2012). approach models trial separate GLM, making powerful technique multivariate pattern analysis (MVPA) connectivity studies trial-specific estimates needed. package offers multiple backends, simple reference implementation highly optimized, parallel C++ engine, accessible clean, unified interface.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Least Squares Separate (LSS) Analysis for fMRI Data","text":"Five Implementations: Includes highly optimized C++ version, optimized R version, standard vectorized R C++ versions, simple R loop testing validation. Parallel Processing: optimized C++ version uses OpenMP multi-threaded execution maximize performance modern hardware. Flexible & Modern Interface: clean lss(Y, X, Z, Nuisance) signature powerful intuitive. Nuisance Regression: Built-support projecting nuisance regressors (e.g., motion parameters, physiological noise) LSS analysis. CRAN-Compliant: Built portable configurations suitable CRAN submission.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Least Squares Separate (LSS) Analysis for fMRI Data","text":"can install development version fmrilss GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"bbuchsbaum/fmrilss\")"},{"path":"https://bbuchsbaum.github.io/fmrilss/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Least Squares Separate (LSS) Analysis for fMRI Data","text":"primary function lss(), takes data Y, trial design X, experimental regressors Z, optional nuisance regressors.","code":"library(fmrilss)  # 1. Generate synthetic data set.seed(123) n_timepoints <- 120 n_trials <- 15 n_voxels <- 50  # Trial design matrix (X): one column per trial X <- matrix(0, n_timepoints, n_trials) onsets <- seq(from = 5, to = n_timepoints - 10, length.out = n_trials) for(i in 1:n_trials) {   X[onsets[i]:(onsets[i] + 4), i] <- 1 } colnames(X) <- paste0(\"Trial_\", 1:n_trials)  # Experimental regressors (Z): intercept and condition-specific effects # These are experimental regressors we want to model and get beta estimates for, # but not trial-wise (e.g., condition differences, block effects) Z <- cbind(Intercept = 1, LinearTrend = scale(1:n_timepoints, center = TRUE, scale = FALSE))  # Nuisance regressors: e.g., 6 motion parameters Nuisance <- matrix(rnorm(n_timepoints * 6), n_timepoints, 6)  # Data (Y): timepoints x voxels # (Simulate some effects for demonstration) true_betas <- matrix(rnorm(n_trials * n_voxels, 0, 1.5), n_trials, n_voxels) Y <- Z %*% matrix(c(10, -0.2), 2, n_voxels) +       X %*% true_betas +      Nuisance %*% matrix(rnorm(6 * n_voxels, 0, 2), 6, n_voxels) +      matrix(rnorm(n_timepoints * n_voxels, 0, 0.8), n_timepoints, n_voxels) colnames(Y) <- paste0(\"Voxel_\", 1:n_voxels)   # 2. Run LSS analysis  # Example 1: Basic LSS with default intercept # If Z is NULL, an intercept is automatically added. beta_estimates <- lss(Y, X)  # Example 2: LSS with experimental regressors (intercept + condition effects) beta_fixed <- lss(Y, X, Z = Z)  # Example 3: LSS with experimental regressors and nuisance regression beta_clean <- lss(Y, X, Z = Z, Nuisance = Nuisance)  # Example 4: Use the super-fast, parallelized C++ implementation beta_fast <- lss(Y, X, Z = Z, Nuisance = Nuisance, method = \"cpp_optimized\")  # The result is a (trials x voxels) matrix of beta estimates print(dim(beta_fast)) #> [1] 15 50 print(beta_fast[1:5, 1:4])"},{"path":"https://bbuchsbaum.github.io/fmrilss/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Least Squares Separate (LSS) Analysis for fMRI Data","text":"Mumford, J. ., Turner, B. O., Ashby, F. G., & Poldrack, R. . (2012). Deconvolving BOLD activation event-related designs multivoxel pattern classification analyses. NeuroImage, 59(3), 2636-2643.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Least Squares Separate (LSS) Analysis for fMRI Data","text":"GPL-3","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/LSSBeta.html","id":null,"dir":"Reference","previous_headings":"","what":"LSSBeta object — LSSBeta","title":"LSSBeta object — LSSBeta","text":"Simple list-based S3 class returned lss_with_hrf containing trial-wise beta estimates.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/MixedWorkspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Mixed Model Workspace for Optimized Computation — MixedWorkspace","title":"Mixed Model Workspace for Optimized Computation — MixedWorkspace","text":"Stores precomputed matrices decompositions can reused across multiple voxels avoid repeated expensive computations.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/VoxelHRF.html","id":null,"dir":"Reference","previous_headings":"","what":"VoxelHRF object — VoxelHRF","title":"VoxelHRF object — VoxelHRF","text":"Simple list-based S3 class returned estimate_voxel_hrf containing voxel-wise HRF basis coefficients related metadata.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/calculate_recovery_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate HRF Recovery Metrics — calculate_recovery_metrics","title":"Calculate HRF Recovery Metrics — calculate_recovery_metrics","text":"Evaluates well method recovered true HRF","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/calculate_recovery_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate HRF Recovery Metrics — calculate_recovery_metrics","text":"","code":"calculate_recovery_metrics(results, true_hrf)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/calculate_recovery_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate HRF Recovery Metrics — calculate_recovery_metrics","text":"results Output compare_hrf_recovery true_hrf Ground truth HRF","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/calculate_recovery_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate HRF Recovery Metrics — calculate_recovery_metrics","text":"Data frame recovery metrics","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/compare_hrf_recovery.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare HRF Recovery Methods — compare_hrf_recovery","title":"Compare HRF Recovery Methods — compare_hrf_recovery","text":"Compares OASIS, SPMG1, SPMG3, FIR HRF recovery","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/compare_hrf_recovery.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare HRF Recovery Methods — compare_hrf_recovery","text":"","code":"compare_hrf_recovery(data, hrf_grid = NULL)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/compare_hrf_recovery.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare HRF Recovery Methods — compare_hrf_recovery","text":"data Synthetic data generate_lwu_data hrf_grid Optional pre-computed HRF grid OASIS","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/compare_hrf_recovery.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare HRF Recovery Methods — compare_hrf_recovery","text":"List results methods","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/create_lwu_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Create LWU HRF Grid for OASIS Search — create_lwu_grid","title":"Create LWU HRF Grid for OASIS Search — create_lwu_grid","text":"Generates grid LWU HRF models varying parameters","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/create_lwu_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create LWU HRF Grid for OASIS Search — create_lwu_grid","text":"","code":"create_lwu_grid(   tau_range = c(4, 8),   sigma_range = c(1.5, 3.5),   rho_range = c(0.1, 0.6),   n_tau = 5,   n_sigma = 3,   n_rho = 3 )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/create_lwu_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create LWU HRF Grid for OASIS Search — create_lwu_grid","text":"tau_range Range tau values test sigma_range Range sigma values test rho_range Range rho values test n_tau Number tau values grid n_sigma Number sigma values grid n_rho Number rho values grid","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/create_lwu_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create LWU HRF Grid for OASIS Search — create_lwu_grid","text":"List HRF models parameters","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-convert_legacy_whiten.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert old-style whitening options to new format — .convert_legacy_whiten","title":"Convert old-style whitening options to new format — .convert_legacy_whiten","text":"Internal helper maintain backward compatibility old oasis$whiten = \"ar1\" syntax converting new prewhiten format.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-convert_legacy_whiten.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert old-style whitening options to new format — .convert_legacy_whiten","text":"","code":".convert_legacy_whiten(oasis_opts)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-convert_legacy_whiten.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert old-style whitening options to new format — .convert_legacy_whiten","text":"oasis_opts List OASIS options potentially containing whiten field","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-convert_legacy_whiten.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert old-style whitening options to new format — .convert_legacy_whiten","text":"List prewhiten options NULL whitening","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-lss_oasis.html","id":null,"dir":"Reference","previous_headings":"","what":"OASIS backend for fmrilss::lss (internal entry) — .lss_oasis","title":"OASIS backend for fmrilss::lss (internal entry) — .lss_oasis","text":"Add `method=\"oasis\"` fmrilss::lss(). path:   - (optionally) builds trial-wise design X via fmrihrf   - residualizes Y (X downstream) confounds + Z + -condition aggregates   - computes trial betas one batched pass via closed-form LSS (exact; ridge-LSS)   - optionally returns per-trial SEs design diagnostics","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-lss_oasis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"OASIS backend for fmrilss::lss (internal entry) — .lss_oasis","text":"","code":".lss_oasis(   Y,   X = NULL,   Z = NULL,   Nuisance = NULL,   oasis = list(),   prewhiten = NULL )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-lss_oasis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"OASIS backend for fmrilss::lss (internal entry) — .lss_oasis","text":"Y (T x V) numeric matrix X (T x N_trials) trial-wise design (NULL, use oasis$design_spec build) Z (T x K) fixed experimental regressors projected Nuisance (T x P) confounds (intercept, motion, drift, aCompCor, ...) oasis list options: - design_spec: list describing events/HRF build X via fmrihrf - K: explicit basis dimension (auto-detected provided) - ridge_mode: \"absolute\" (default) \"fractional\" - ridge_x, ridge_b: nonnegative ridge [a_j, b_j] Gram (default 0 -> exact LSS) - block_cols: voxel block size (default 4096) - return_se: logical (default FALSE) - return_diag: logical (default FALSE) - whiten: \"none\" | \"ar1\" (default \"none\"); \"ar1\", prewhiten Y design first (DEPRECATED: use prewhiten parameter) prewhiten list prewhitening options using fmriAR (see ?lss details)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-lss_oasis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"OASIS backend for fmrilss::lss (internal entry) — .lss_oasis","text":"default: (N_trials x V) matrix betas; `return_se` `return_diag`, list","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-mixed_solve_cpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Mixed Model Solver using C++ — .mixed_solve_cpp","title":"Mixed Model Solver using C++ — .mixed_solve_cpp","text":"C++ implementation mixed model solver. function typically called main `mixed_solve` function rather directly.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-mixed_solve_cpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mixed Model Solver using C++ — .mixed_solve_cpp","text":"","code":".mixed_solve_cpp(   Y,   X = NULL,   Z = NULL,   K = NULL,   method = \"REML\",   bounds = c(1e-09, 1e+09),   SE = FALSE,   return_Hinv = FALSE )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-mixed_solve_cpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mixed Model Solver using C++ — .mixed_solve_cpp","text":"Y Response vector. X Design matrix fixed effects (default: intercept ). Z Design matrix random effects (default: identity matrix). K Kinship matrix (default: identity matrix). method Optimization method, either \"REML\" \"ML\". bounds Bounds optimizer. SE Logical, whether return standard errors. return_Hinv Logical, whether return inverse H.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-mixed_solve_cpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mixed Model Solver using C++ — .mixed_solve_cpp","text":"list mixed model results.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-needs_advanced_prewhitening.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if advanced prewhitening is needed — .needs_advanced_prewhitening","title":"Check if advanced prewhitening is needed — .needs_advanced_prewhitening","text":"Determines whether use fmriAR (advanced) keep simple AR(1) based requested features.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-needs_advanced_prewhitening.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if advanced prewhitening is needed — .needs_advanced_prewhitening","text":"","code":".needs_advanced_prewhitening(prewhiten)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-needs_advanced_prewhitening.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if advanced prewhitening is needed — .needs_advanced_prewhitening","text":"prewhiten List prewhitening options","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-needs_advanced_prewhitening.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if advanced prewhitening is needed — .needs_advanced_prewhitening","text":"Logical: TRUE fmriAR needed, FALSE simple AR(1)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-prewhiten_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prewhiten fMRI data using AR/ARMA models — .prewhiten_data","title":"Prewhiten fMRI data using AR/ARMA models — .prewhiten_data","text":"Internal function providing unified interface prewhitening fMRI data using fmriAR package. Supports various AR/ARMA models flexible parameter estimation strategies.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-prewhiten_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prewhiten fMRI data using AR/ARMA models — .prewhiten_data","text":"","code":".prewhiten_data(Y, X = NULL, Z = NULL, Nuisance = NULL, prewhiten = list())"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-prewhiten_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prewhiten fMRI data using AR/ARMA models — .prewhiten_data","text":"Y Numeric matrix (timepoints x voxels) fMRI data X Optional design matrix trials (timepoints x trials) Z Optional experimental design matrix (timepoints x regressors) Nuisance Optional nuisance regressors (timepoints x nuisance) prewhiten List prewhitening options: method Character: \"ar\" (default), \"arma\", \"none\" p Integer \"auto\": AR order (default \"auto\") q Integer: MA order ARMA (default 0) p_max Integer: Maximum AR order p=\"auto\" (default 6) pooling Character: \"global\" (default), \"voxel\", \"run\", \"parcel\" runs Integer vector: Run identifiers run-aware estimation parcels Integer vector: Parcel memberships parcel-based pooling exact_first Character: \"ar1\" \"none\" exact AR(1) scaling compute_residuals Logical: Whether compute residuals first (default TRUE)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/dot-prewhiten_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prewhiten fMRI data using AR/ARMA models — .prewhiten_data","text":"List containing: Y_whitened Whitened data matrix X_whitened Whitened trial design matrix (provided) Z_whitened Whitened experimental design (provided) Nuisance_whitened Whitened nuisance regressors (provided) whiten_plan fmriAR plan object diagnostics applied Logical: Whether whitening applied","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/estimate_voxel_hrf.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate Voxel-wise HRF Basis Coefficients — estimate_voxel_hrf","title":"Estimate Voxel-wise HRF Basis Coefficients — estimate_voxel_hrf","text":"Fits GLM estimate HRF basis coefficients every voxel.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/estimate_voxel_hrf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate Voxel-wise HRF Basis Coefficients — estimate_voxel_hrf","text":"","code":"estimate_voxel_hrf(Y, events, basis, nuisance_regs = NULL)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/estimate_voxel_hrf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate Voxel-wise HRF Basis Coefficients — estimate_voxel_hrf","text":"Y Numeric matrix BOLD data (time x voxels). events Data frame onset, duration condition columns. basis HRF object fmrihrf package. nuisance_regs Optional numeric matrix nuisance regressors.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/estimate_voxel_hrf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate Voxel-wise HRF Basis Coefficients — estimate_voxel_hrf","text":"VoxelHRF object containing least: coefficients Matrix HRF basis coefficients. basis HRF basis object used. conditions Character vector modeled conditions.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/estimate_voxel_hrf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate Voxel-wise HRF Basis Coefficients — estimate_voxel_hrf","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(1) Y <- matrix(rnorm(100), 50, 2) events <- data.frame(onset = c(5, 25), duration = 1,                      condition = \"A\") basis <- fmrihrf::hrf_gamma() sframe <- fmrihrf::sampling_frame(blocklens = nrow(Y), TR = 1) times <- fmrihrf::samples(sframe, global = TRUE) rset <- fmrihrf::regressor_set(onsets = events$onset,                                fac = factor(1:nrow(events)),                                hrf = basis, duration = events$duration,                                span = 30) X <- fmrihrf::evaluate(rset, grid = times, precision = 0.1, method = \"conv\") coef <- matrix(rnorm(ncol(X) * ncol(Y)), ncol(X), ncol(Y)) Y <- X %*% coef + Y * 0.1 est <- estimate_voxel_hrf(Y, events, basis) str(est) } # }"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fast_reml_lambda.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast analytical REML estimation for single variance component — fast_reml_lambda","title":"Fast analytical REML estimation for single variance component — fast_reml_lambda","text":"single variance component model, REML estimate λ = σe²/σu² closed-form solution can computed efficiently.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fast_reml_lambda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast analytical REML estimation for single variance component — fast_reml_lambda","text":"omega Transformed response vector Q'y theta Transformed eigenvalues tol Convergence tolerance Newton iterations max_iter Maximum Newton iterations","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fast_reml_lambda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast analytical REML estimation for single variance component — fast_reml_lambda","text":"Estimated variance ratio λ","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fit_oasis_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit OASIS with HRF Grid Search — fit_oasis_grid","title":"Fit OASIS with HRF Grid Search — fit_oasis_grid","text":"Fits OASIS models different HRF parameters selects best","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fit_oasis_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit OASIS with HRF Grid Search — fit_oasis_grid","text":"","code":"fit_oasis_grid(Y, onsets, sframe, hrf_grid, ridge_x = 0.01, ridge_b = 0.01)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fit_oasis_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit OASIS with HRF Grid Search — fit_oasis_grid","text":"Y Data matrix (time x voxels) onsets Event onset times sframe Sampling frame hrf_grid List HRF models test ridge_x Ridge parameter design matrix ridge_b Ridge parameter aggregator","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fit_oasis_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit OASIS with HRF Grid Search — fit_oasis_grid","text":"List best HRF index, parameters, beta estimates","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fmrilss-package.html","id":null,"dir":"Reference","previous_headings":"","what":"fmrilss: Least Squares Separate (LSS) Analysis for fMRI Data — fmrilss-package","title":"fmrilss: Least Squares Separate (LSS) Analysis for fMRI Data — fmrilss-package","text":"package implements efficient least squares separate (LSS) analysis functional magnetic resonance imaging (fMRI) data. LSS used estimate trial--trial activation patterns event-related fMRI designs.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fmrilss-package.html","id":"main-functions","dir":"Reference","previous_headings":"","what":"Main functions","title":"fmrilss: Least Squares Separate (LSS) Analysis for fMRI Data — fmrilss-package","text":"lss: Main function performing LSS analysis lss_naive: Naive LSS implementation reference project_confounds: R implementation projecting confounds project_confounds_cpp: Fast C++ confound projection lss_beta_cpp: Vectorized C++ LSS beta computation get_data_matrix: Helper function data extraction","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fmrilss-package.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"fmrilss: Least Squares Separate (LSS) Analysis for fMRI Data — fmrilss-package","text":"Optimized C++ implementation using vectorized matrix algebra Memory-efficient projection without forming Q matrices Cholesky decomposition numerical stability Fallback R implementation QR decomposition Support various design matrix configurations Robust numerical handling edge cases OpenMP support multi-core processing","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/fmrilss-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"fmrilss: Least Squares Separate (LSS) Analysis for fMRI Data — fmrilss-package","text":"Name <.email@example.com>","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_lwu_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Synthetic fMRI Data with LWU HRF — generate_lwu_data","title":"Generate Synthetic fMRI Data with LWU HRF — generate_lwu_data","text":"Creates synthetic fMRI time series using specified LWU HRF parameters","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_lwu_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Synthetic fMRI Data with LWU HRF — generate_lwu_data","text":"","code":"generate_lwu_data(   onsets,   tau = 6,   sigma = 2.5,   rho = 0.35,   TR = 1,   total_time = 300,   n_voxels = 10,   amplitudes = NULL,   noise_sd = 0.2,   seed = NULL )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_lwu_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Synthetic fMRI Data with LWU HRF — generate_lwu_data","text":"onsets Vector event onset times seconds tau LWU tau parameter (time--peak) sigma LWU sigma parameter (width) rho LWU rho parameter (undershoot amplitude) TR Repetition time seconds total_time Total scan time seconds n_voxels Number voxels simulate amplitudes Event amplitudes (scalar vector) noise_sd Standard deviation noise seed Random seed","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_lwu_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Synthetic fMRI Data with LWU HRF — generate_lwu_data","text":"List Y (data matrix), true_hrf, true_betas, design info","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_rapid_design.html","id":null,"dir":"Reference","previous_headings":"","what":"OASIS HRF Recovery Testing Functions — generate_rapid_design","title":"OASIS HRF Recovery Testing Functions — generate_rapid_design","text":"Functions test OASIS's ability recover HRF parameters rapid event-related designs overlapping HRFs. Generate Rapid Event-Related Design","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_rapid_design.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"OASIS HRF Recovery Testing Functions — generate_rapid_design","text":"","code":"generate_rapid_design(   n_events = 25,   total_time = 300,   min_isi = 2,   max_isi = 4,   seed = NULL )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_rapid_design.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"OASIS HRF Recovery Testing Functions — generate_rapid_design","text":"n_events Number events generate total_time Total time seconds min_isi Minimum inter-stimulus interval seconds max_isi Maximum inter-stimulus interval seconds seed Random seed reproducibility","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_rapid_design.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"OASIS HRF Recovery Testing Functions — generate_rapid_design","text":"Numeric vector event onset times","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/generate_rapid_design.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"OASIS HRF Recovery Testing Functions — generate_rapid_design","text":"Creates rapid event-related design specified ISI range","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/get_data_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Data Matrix from Dataset — get_data_matrix","title":"Extract Data Matrix from Dataset — get_data_matrix","text":"Helper function extract data matrix various dataset formats. placeholder customized based data format.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/get_data_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Data Matrix from Dataset — get_data_matrix","text":"","code":"get_data_matrix(dset)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/get_data_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Data Matrix from Dataset — get_data_matrix","text":"dset Dataset object (format depends specific use case)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/get_data_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Data Matrix from Dataset — get_data_matrix","text":"numeric matrix rows timepoints columns voxels","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/list_to_workspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert R list back to MixedWorkspace — list_to_workspace","title":"Convert R list back to MixedWorkspace — list_to_workspace","text":"Convert R list back MixedWorkspace","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lsa.html","id":null,"dir":"Reference","previous_headings":"","what":"Least Squares All (LSA) Analysis — lsa","title":"Least Squares All (LSA) Analysis — lsa","text":"Performs standard multiple regression analysis trial regressors fitted simultaneously. provides reference comparison Least Squares Separate (LSS) approach.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lsa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Least Squares All (LSA) Analysis — lsa","text":"","code":"lsa(Y, X, Z = NULL, Nuisance = NULL, method = c(\"r\", \"cpp\"))"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lsa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Least Squares All (LSA) Analysis — lsa","text":"Y numeric matrix rows timepoints columns voxels/features. dependent variable data. X numeric matrix rows timepoints columns trial-specific regressors. column represents single trial event. Z numeric matrix nuisance regressors (e.g., motion parameters, drift terms). Defaults NULL. Nuisance alias Z, provided consistency LSS interface. Z Nuisance provided, Z takes precedence. method Character string specifying computational method: \"r\" - Pure R implementation using lm.fit \"cpp\" - C++ implementation better performance","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lsa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Least Squares All (LSA) Analysis — lsa","text":"numeric matrix size T × V containing beta estimates   trial regressor (rows) voxel (columns).","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lsa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Least Squares All (LSA) Analysis — lsa","text":"LSA fits model: Y = X*beta + Z*gamma + error, trial regressors X estimated simultaneously. contrast LSS, fits trial separately treating trials nuisance regressors.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lsa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Least Squares All (LSA) Analysis — lsa","text":"","code":"# Generate example data n_timepoints <- 100 n_trials <- 10 n_voxels <- 50  # Create trial design matrix X <- matrix(0, n_timepoints, n_trials) for(i in 1:n_trials) {   start <- (i-1) * 8 + 1   if(start + 5 <= n_timepoints) {     X[start:(start+5), i] <- 1   } }  # Create data with some signal Y <- matrix(rnorm(n_timepoints * n_voxels), n_timepoints, n_voxels) true_betas <- matrix(rnorm(n_trials * n_voxels, 0, 0.5), n_trials, n_voxels) for(i in 1:n_trials) {   Y <- Y + X[, i] %*% matrix(true_betas[i, ], 1, n_voxels) }  # Run LSA analysis beta_estimates <- lsa(Y, X)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss.html","id":null,"dir":"Reference","previous_headings":"","what":"Least Squares Separate (LSS) Analysis — lss","title":"Least Squares Separate (LSS) Analysis — lss","text":"Computes trial-wise beta estimates using Least Squares Separate approach Mumford et al. (2012). method fits separate GLM trial, trial interest trials separate regressors.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Least Squares Separate (LSS) Analysis — lss","text":"","code":"lss(   Y,   X,   Z = NULL,   Nuisance = NULL,   method = c(\"r_optimized\", \"cpp_optimized\", \"r_vectorized\", \"cpp\", \"naive\", \"oasis\"),   block_size = 96,   oasis = list(),   prewhiten = NULL )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Least Squares Separate (LSS) Analysis — lss","text":"Y numeric matrix size n × V n number timepoints V number voxels/variables X numeric matrix size n × T T number trials. column represents design one trial Z numeric matrix size n × F representing experimental regressors include trial-wise models. regressors want model get beta estimates , trial-wise (e.g., intercept, condition effects, block effects). NULL, intercept-design used. Defaults NULL Nuisance numeric matrix size n × N representing nuisance regressors projected LSS analysis (e.g., motion parameters, physiological noise). NULL, nuisance projection performed. Defaults NULL method Character string specifying implementation use. Options : \"r_optimized\" - Optimized R implementation (recommended, default) \"cpp_optimized\" - Optimized C++ implementation parallel support \"r_vectorized\" - Standard R vectorized implementation \"cpp\" - Standard C++ implementation \"naive\" - Simple loop-based R implementation (testing) \"oasis\" - OASIS method HRF support ridge regularization block_size integer specifying voxel block size parallel processing, applicable `method = \"cpp_optimized\"`. Defaults 96. oasis list options OASIS method. See Details available options. prewhiten list prewhitening options using fmriAR. See Details available options.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Least Squares Separate (LSS) Analysis — lss","text":"numeric matrix size T × V containing trial-wise beta estimates.   Note: Currently returns estimates trial regressors (X). Beta   estimates experimental regressors (Z) computed returned.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Least Squares Separate (LSS) Analysis — lss","text":"LSS approach fits separate GLM trial, model includes: trial interest (column X) trials combined (sum columns X) Experimental regressors (Z matrix) - modeled get beta estimates trial-wise Nuisance regressors provided, first projected Y X using standard linear regression residualization. using method=\"oasis\", following options available oasis list: design_spec: list building trial-wise designs event onsets using fmrihrf.     Must contain: sframe (sampling frame), cond (list onsets,     hrf, optionally span), optionally others (list conditions     modeled nuisances). provided, X can NULL constructed automatically. K: Explicit basis dimension multi-basis HRF models (e.g., 3 SPMG3).     provided, auto-detected X dimensions defaults 1 single-basis HRFs. ridge_mode: Either \"absolute\" (default) \"fractional\". absolute mode,     ridge_x ridge_b used directly regularization parameters. fractional mode,     represent fractions maximum eigenvalue adaptive regularization. ridge_x: Ridge parameter trial-specific regressors (default 0). Controls     regularization strength individual trial estimates. ridge_b: Ridge parameter aggregator regressor (default 0). Controls     regularization strength sum trials. return_se: Logical, whether return standard errors (default FALSE). TRUE,     returns list beta (trial estimates) se (standard errors) components. return_diag: Logical, whether return design diagnostics (default FALSE).     TRUE, includes diagnostic information design matrix structure. block_cols: Integer, voxel block size memory-efficient processing (default 4096).     Larger values use memory may faster systems sufficient RAM. whiten: Logical, whether apply AR(1) whitening (default FALSE). TRUE,     estimates AR(1) coefficients pre-whitens data account temporal autocorrelation. ntrials: Explicit number trials (used K > 1 determine output dimensions).     provided, calculated ncol(X) / K. hrf_grid: Vector HRF indices grid-based HRF selection (advanced use).     Allows testing multiple HRF shapes simultaneously. using prewhiten parameter, following options available: method: Character, \"ar\" (default), \"arma\", \"none\" noise model type. p: Integer \"auto\" AR order (default \"auto\"). q: Integer MA order ARMA models (default 0). p_max: Maximum AR order p=\"auto\" (default 6). pooling: Character, \"global\" (default), \"voxel\", \"run\", \"parcel\" parameter estimation strategy. runs: Integer vector run identifiers run-aware estimation. parcels: Integer vector parcel memberships parcel-based pooling. exact_first: Character, \"ar1\" \"none\" exact AR(1) scaling segment starts. Prewhitening applied LSS analysis account temporal autocorrelation fMRI time series. fmriAR package provides flexible AR/ARMA modeling various pooling strategies. backward compatibility, old oasis$whiten = \"ar1\" syntax still supported converted equivalent prewhiten settings. OASIS method provides mathematically equivalent computationally optimized version standard LSS. reformulates per-trial GLM fitting single matrix operation, eliminating redundant computations. particularly beneficial designs many trials processing large datasets. K > 1 (multi-basis HRFs), output K*ntrials rows, basis functions trial arranged sequentially.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Least Squares Separate (LSS) Analysis — lss","text":"Mumford, J. ., Turner, B. O., Ashby, F. G., & Poldrack, R. . (2012). Deconvolving BOLD activation event-related designs multivoxel pattern classification analyses. NeuroImage, 59(3), 2636-2643.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Least Squares Separate (LSS) Analysis — lss","text":"","code":"# Generate example data n_timepoints <- 100 n_trials <- 10 n_voxels <- 50  # Create trial design matrix X <- matrix(0, n_timepoints, n_trials) for(i in 1:n_trials) {   start <- (i-1) * 8 + 1   if(start + 5 <= n_timepoints) {     X[start:(start+5), i] <- 1   } }  # Create data with some signal Y <- matrix(rnorm(n_timepoints * n_voxels), n_timepoints, n_voxels) true_betas <- matrix(rnorm(n_trials * n_voxels, 0, 0.5), n_trials, n_voxels) for(i in 1:n_trials) {   Y <- Y + X[, i] %*% matrix(true_betas[i, ], 1, n_voxels) }  # Run LSS analysis beta_estimates <- lss(Y, X)  # With experimental regressors (intercept + condition effects) Z <- cbind(1, scale(1:n_timepoints)) beta_estimates_with_regressors <- lss(Y, X, Z = Z)  # With nuisance regression (motion parameters) Nuisance <- matrix(rnorm(n_timepoints * 6), n_timepoints, 6) beta_estimates_clean <- lss(Y, X, Z = Z, Nuisance = Nuisance)  if (FALSE) { # \\dontrun{ # Using OASIS method with ridge regularization beta_oasis <- lss(Y, X, method = \"oasis\",                   oasis = list(ridge_x = 0.1, ridge_b = 0.1,                               ridge_mode = \"fractional\"))  # OASIS with standard errors result_with_se <- lss(Y, X, method = \"oasis\",                      oasis = list(return_se = TRUE)) beta_estimates <- result_with_se$beta standard_errors <- result_with_se$se  # Building design from event onsets using fmrihrf   sframe <- sampling_frame(blocklens = 200, TR = 1.0)    # OASIS with automatic design construction   beta_auto <- lss(Y, X = NULL, method = \"oasis\",                    oasis = list(                      design_spec = list(                        sframe = sframe,                        cond = list(                          onsets = c(10, 30, 50, 70, 90, 110, 130, 150),                          hrf = HRF_SPMG1,                          span = 25                        ),                        others = list(                          list(onsets = c(20, 40, 60, 80, 100, 120, 140))                        )                      )                    ))    # Multi-basis HRF example (3 basis functions per trial)   beta_multibasis <- lss(Y, X = NULL, method = \"oasis\",                         oasis = list(                           design_spec = list(                             sframe = sframe,                             cond = list(                               onsets = c(10, 30, 50, 70, 90),                               hrf = HRF_SPMG3,  # 3-basis HRF                               span = 30                             )                           ),                           K = 3  # Explicit basis dimension                         ))   # Returns 15 rows (5 trials * 3 basis functions) } # }"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_beta_cpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Vectorized LSS Beta Computation Using C++ — lss_beta_cpp","title":"Vectorized LSS Beta Computation Using C++ — lss_beta_cpp","text":"Fast C++ implementation least squares separate (LSS) beta estimation using vectorized matrix operations. Computes trial betas single pass without loops.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_beta_cpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vectorized LSS Beta Computation Using C++ — lss_beta_cpp","text":"","code":"lss_beta_cpp(C_projected, Y_projected)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_beta_cpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vectorized LSS Beta Computation Using C++ — lss_beta_cpp","text":"C_projected Projected trial regressors (n x T) project_confounds_cpp Y_projected Projected data (n x V) project_confounds_cpp","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_beta_cpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vectorized LSS Beta Computation Using C++ — lss_beta_cpp","text":"Beta matrix (T x V) LSS estimates trial voxel","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_beta_cpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Vectorized LSS Beta Computation Using C++ — lss_beta_cpp","text":"vectorized implementation computes LSS betas simultaneously using matrix algebra. significantly faster per-trial loops automatically benefits BLAS multithreading. algorithm handles numerical edge cases setting problematic denominators NaN. best performance large datasets, ensure R installation uses optimized BLAS (like OpenBLAS Intel MKL).","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_beta_cpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Vectorized LSS Beta Computation Using C++ — lss_beta_cpp","text":"","code":"if (FALSE) { # \\dontrun{ # After projecting out confounds result <- project_confounds_cpp(X_confounds, Y_data, C_trials) betas <- lss_beta_cpp(result$Q_dmat_ran, result$residual_data) } # }"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_cpp_optimized.html","id":null,"dir":"Reference","previous_headings":"","what":"A wrapper for the optimized C++ LSS implementation — lss_cpp_optimized","title":"A wrapper for the optimized C++ LSS implementation — lss_cpp_optimized","text":"wrapper optimized C++ LSS implementation","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_cpp_optimized.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A wrapper for the optimized C++ LSS implementation — lss_cpp_optimized","text":"","code":"lss_cpp_optimized(Y, bdes)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_cpp_optimized.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A wrapper for the optimized C++ LSS implementation — lss_cpp_optimized","text":"Y voxel time data matrix bdes block design list created block_design","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_cpp_optimized.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A wrapper for the optimized C++ LSS implementation — lss_cpp_optimized","text":"matrix beta estimates","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_fused_optim_cpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Fused Single-Pass LSS Solver (C++) — lss_fused_optim_cpp","title":"Fused Single-Pass LSS Solver (C++) — lss_fused_optim_cpp","text":"function computes Least Squares-Separate (LSS) beta estimates using memory-efficient, single-pass algorithm. fuses projection estimation steps, processing voxels parallel blocks maximize cache efficiency.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_fused_optim_cpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fused Single-Pass LSS Solver (C++) — lss_fused_optim_cpp","text":"","code":"lss_fused_optim_cpp(X, Y, C, block_size = 96L)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_fused_optim_cpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fused Single-Pass LSS Solver (C++) — lss_fused_optim_cpp","text":"X nuisance regressor matrix (confounds). Y data matrix (e.g., fMRI data). C trial-wise design matrix. block_size number voxels process parallel block.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_fused_optim_cpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fused Single-Pass LSS Solver (C++) — lss_fused_optim_cpp","text":"matrix LSS beta estimates.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_naive.html","id":null,"dir":"Reference","previous_headings":"","what":"Naive Least Squares Separate (LSS) Analysis — lss_naive","title":"Naive Least Squares Separate (LSS) Analysis — lss_naive","text":"Performs LSS analysis using naive approach trial model fit separately. conceptually simplest implementation less efficient optimized lss function.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_naive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Naive Least Squares Separate (LSS) Analysis — lss_naive","text":"","code":"lss_naive(Y = NULL, bdes, dset = NULL)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_naive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Naive Least Squares Separate (LSS) Analysis — lss_naive","text":"Y numeric matrix rows timepoints columns voxels/features. NULL, function attempt extract data dset. bdes list containing design matrices components: dmat_base: Base design matrix (e.g., intercept, drift terms) dmat_fixed: Fixed effects design matrix (optional) dmat_ran: Random/trial design matrix LSS analysis fixed_ind: Indices fixed effects (optional) dset Optional dataset object. provided Y NULL, data extracted using get_data_matrix.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_naive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Naive Least Squares Separate (LSS) Analysis — lss_naive","text":"numeric matrix dimensions (n_events x n_voxels) containing LSS beta estimates trial voxel.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_naive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Naive Least Squares Separate (LSS) Analysis — lss_naive","text":"function implements naive LSS approach trial, separate GLM fitted includes: base regressors (intercept, drift, etc.) fixed effects regressors () current trial's regressor trial design matrix less efficient optimized lss function, implementation conceptually simpler can serve reference validation purposes.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_naive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Naive Least Squares Separate (LSS) Analysis — lss_naive","text":"","code":"if (FALSE) { # \\dontrun{ # Using same setup as lss() examples beta_estimates_naive <- lss_naive(Y = Y, bdes = bdes)  # Compare with optimized version beta_estimates_fast <- lss(Y = Y, bdes = bdes) max(abs(beta_estimates_naive - beta_estimates_fast)) } # }"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_optimized.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimized LSS Analysis (Pure R) — lss_optimized","title":"Optimized LSS Analysis (Pure R) — lss_optimized","text":"optimized version LSS analysis avoids creating large intermediate matrices, providing significant speedup lower memory usage pure R implementation.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_optimized.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimized LSS Analysis (Pure R) — lss_optimized","text":"","code":"lss_optimized(Y = NULL, bdes, dset = NULL, use_cpp = TRUE)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_optimized.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimized LSS Analysis (Pure R) — lss_optimized","text":"Y numeric matrix rows timepoints columns voxels/features. bdes list containing design matrices. dset Optional dataset object. use_cpp Logical. TRUE (default), uses C++ implementation. FALSE, uses new optimized R implementation.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_optimized.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimized LSS Analysis (Pure R) — lss_optimized","text":"numeric matrix LSS beta estimates.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform LSS using Voxel-wise HRFs — lss_with_hrf","title":"Perform LSS using Voxel-wise HRFs — lss_with_hrf","text":"Computes trial-wise beta estimates using voxel-specific HRFs.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform LSS using Voxel-wise HRFs — lss_with_hrf","text":"","code":"lss_with_hrf(   Y,   events,   hrf_estimates,   nuisance_regs = NULL,   engine = \"R\",   chunk_size = 5000,   verbose = TRUE,   backing_dir = NULL )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform LSS using Voxel-wise HRFs — lss_with_hrf","text":"Y Numeric matrix BOLD data (time x voxels). events Data frame onset, duration condition columns. hrf_estimates VoxelHRF object returned estimate_voxel_hrf. nuisance_regs Optional numeric matrix nuisance regressors. engine Computational engine: \"R\" pure R implementation (default), \"C++\" optimized C++ (experimental). chunk_size Number voxels process per batch (C++ engine ). verbose Logical; display progress bar. backing_dir Directory bigmemory backing files. NULL, temporary directory used (C++ engine ).","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform LSS using Voxel-wise HRFs — lss_with_hrf","text":"object class LSSBeta C++ engine, numeric matrix   (n_trials x n_vox) R engine.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform LSS using Voxel-wise HRFs — lss_with_hrf","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(1) Y <- matrix(rnorm(100), 50, 2) events <- data.frame(onset = c(5, 25), duration = 1,                      condition = \"A\") basis <- fmrihrf::hrf_gamma() sframe <- fmrihrf::sampling_frame(blocklens = nrow(Y), TR = 1) times <- fmrihrf::samples(sframe, global = TRUE) rset <- fmrihrf::regressor_set(onsets = events$onset,                                fac = factor(1:nrow(events)),                                hrf = basis, duration = events$duration,                                span = 30) X <- fmrihrf::evaluate(rset, grid = times, precision = 0.1, method = \"conv\") coef <- matrix(rnorm(ncol(X) * ncol(Y)), ncol(X), ncol(Y)) Y <- X %*% coef + Y * 0.1 est <- estimate_voxel_hrf(Y, events, basis) betas <- lss_with_hrf(Y, events, est, verbose = FALSE, engine = \"R\") dim(betas) } # }"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf_pure_r.html","id":null,"dir":"Reference","previous_headings":"","what":"Least Squares Separate with voxel-wise HRF (basis-weighted) — lss_with_hrf_pure_r","title":"Least Squares Separate with voxel-wise HRF (basis-weighted) — lss_with_hrf_pure_r","text":"Compute LSS trial-wise betas voxel HRF formed linear combination K basis kernels sampled TR grid.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf_pure_r.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Least Squares Separate with voxel-wise HRF (basis-weighted) — lss_with_hrf_pure_r","text":"","code":"lss_with_hrf_pure_r(   Y,   onset_idx,   durations = NULL,   hrf_basis_kernels,   coefficients,   Z = NULL,   Nuisance = NULL,   verbose = FALSE,   method = c(\"r\", \"cpp\", \"cpp_arma\", \"cpp_omp\") )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf_pure_r.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Least Squares Separate with voxel-wise HRF (basis-weighted) — lss_with_hrf_pure_r","text":"Y numeric matrix (n_time x n_vox) onset_idx integer vector (length n_trials), 1-based TR indices durations numeric vector (length n_trials), TRs; 0 means impulse. Uses inclusive indexing: duration = d, samples o:(o+d) 1. hrf_basis_kernels numeric matrix (L x K), K basis kernels TR grid coefficients numeric matrix (K x n_vox), voxel-wise HRF weights Z optional numeric matrix (n_time x F) experimental regressors; NULL, intercept (column 1s) used. Nuisance optional numeric matrix (n_time x q) confounds project verbose logical; print progress every 1000 voxels method character: \"r\" (default, pure R), \"cpp\" (C++ backend), \"cpp_arma\" (Armadillo backend), \"cpp_omp\" (OpenMP parallel backend). Falls back automatically: cpp_omp -> cpp_arma -> cpp -> r.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf_pure_r.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Least Squares Separate with voxel-wise HRF (basis-weighted) — lss_with_hrf_pure_r","text":"numeric matrix (n_trials x n_vox) trial-wise beta estimates","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf_pure_r.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Least Squares Separate with voxel-wise HRF (basis-weighted) — lss_with_hrf_pure_r","text":"**Design & nuisance handling match `lss()`**:   - trial--interest (Xi) sum trials (Xother)     included per-trial GLM.   - `Nuisance` supplied, projected **Y** trial     regressors LSS (standard residualization). Experimental regressors     `Z` ** residualized, matching `lss()` documentation.   - `Z` `NULL`, intercept-design used.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/lss_with_hrf_pure_r.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Least Squares Separate with voxel-wise HRF (basis-weighted) — lss_with_hrf_pure_r","text":"","code":"if (FALSE) { # \\dontrun{ # Minimal use (R backend): betas <- lss_with_hrf_pure_r(Y, onset_idx, durations, basis, coeffs, Z = NULL, Nuisance = NULL) # Or with C++ backend: betas <- lss_with_hrf_pure_r(Y, onset_idx, durations, basis, coeffs, method = \"cpp\") } # }"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_multi_voxel_cpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimized multi-voxel mixed model estimation — mixed_multi_voxel_cpp","title":"Optimized multi-voxel mixed model estimation — mixed_multi_voxel_cpp","text":"Uses precomputed workspace parallel processing efficiently estimate mixed models across many voxels.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_multi_voxel_cpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimized multi-voxel mixed model estimation — mixed_multi_voxel_cpp","text":"","code":"mixed_multi_voxel_cpp(Y, ws_list, compute_se = FALSE, n_threads = 0L)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_multi_voxel_cpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimized multi-voxel mixed model estimation — mixed_multi_voxel_cpp","text":"Y Response matrix (n × V) V number voxels ws_list Precomputed workspace (R list) compute_se Whether compute standard errors n_threads Number OpenMP threads (0 = auto)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_multi_voxel_cpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimized multi-voxel mixed model estimation — mixed_multi_voxel_cpp","text":"List matrices estimates across voxels","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_precompute.html","id":null,"dir":"Reference","previous_headings":"","what":"Precompute Workspace for Optimized Mixed Model — mixed_precompute","title":"Precompute Workspace for Optimized Mixed Model — mixed_precompute","text":"Performs expensive matrix computations depend response vector, allowing efficient reuse across multiple voxels.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_precompute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Precompute Workspace for Optimized Mixed Model — mixed_precompute","text":"","code":"mixed_precompute(X, Z, K = NULL)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_precompute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Precompute Workspace for Optimized Mixed Model — mixed_precompute","text":"X Fixed effects design matrix (n × p) Z Random effects design matrix (n × q) K Kinship/covariance matrix random effects (q × q)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_precompute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Precompute Workspace for Optimized Mixed Model — mixed_precompute","text":"Workspace object use mixed_solve_optimized","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_precompute_workspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Precompute workspace for mixed model optimization — mixed_precompute_workspace","title":"Precompute workspace for mixed model optimization — mixed_precompute_workspace","text":"Performs expensive computations depend response vector y, can reused across multiple voxels.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_precompute_workspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Precompute workspace for mixed model optimization — mixed_precompute_workspace","text":"","code":"mixed_precompute_workspace(X, Z, K)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_precompute_workspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Precompute workspace for mixed model optimization — mixed_precompute_workspace","text":"X Fixed effects design matrix (n × p) Z Random effects design matrix (n × q) K Kinship/covariance matrix random effects (q × q)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_precompute_workspace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Precompute workspace for mixed model optimization — mixed_precompute_workspace","text":"MixedWorkspace object containing precomputed matrices","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_single_voxel_cpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast single-voxel mixed model estimation using precomputed workspace — mixed_single_voxel_cpp","title":"Fast single-voxel mixed model estimation using precomputed workspace — mixed_single_voxel_cpp","text":"Fast single-voxel mixed model estimation using precomputed workspace","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_single_voxel_cpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast single-voxel mixed model estimation using precomputed workspace — mixed_single_voxel_cpp","text":"","code":"mixed_single_voxel_cpp(y, ws_list, compute_se = FALSE)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_single_voxel_cpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast single-voxel mixed model estimation using precomputed workspace — mixed_single_voxel_cpp","text":"y Response vector single voxel ws_list Precomputed workspace (R list) compute_se Whether compute standard errors","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_single_voxel_cpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast single-voxel mixed model estimation using precomputed workspace — mixed_single_voxel_cpp","text":"List beta, u, Vu, Ve, optionally standard errors","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve.html","id":null,"dir":"Reference","previous_headings":"","what":"Mixed Model Solver — mixed_solve","title":"Mixed Model Solver — mixed_solve","text":"Solves mixed models random effects using REML ML estimation. function provides unified interface mixed model estimation, similar lss/lsa functions package.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mixed Model Solver — mixed_solve","text":"","code":"mixed_solve(   Y,   X = NULL,   Z = NULL,   K = NULL,   Nuisance = NULL,   method = c(\"REML\", \"ML\"),   bounds = c(1e-09, 1e+09),   SE = FALSE,   return_Hinv = FALSE )  mixed_solve_cpp(   Y,   X = NULL,   Z = NULL,   K = NULL,   Nuisance = NULL,   method = c(\"REML\", \"ML\"),   bounds = c(1e-09, 1e+09),   SE = FALSE,   return_Hinv = FALSE )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mixed Model Solver — mixed_solve","text":"Y Response vector matrix. matrix, column treated separate response variable. X Design matrix fixed effects. NULL, defaults intercept . Z Design matrix random effects. NULL, defaults identity matrix. K Kinship matrix random effects. NULL, defaults identity matrix. Nuisance alias X, provided consistency lss/lsa interface. X Nuisance provided, X takes precedence. method Character string specifying estimation method: \"REML\" - Restricted Maximum Likelihood (default) \"ML\" - Maximum Likelihood bounds Numeric vector length 2 specifying bounds variance component optimization. Defaults c(1e-9, 1e9). SE Logical, whether compute return standard errors. Defaults FALSE. return_Hinv Logical, whether return inverse H matrix. Defaults FALSE.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mixed Model Solver — mixed_solve","text":"list containing: Vu Estimated variance component random effects. Ve Estimated variance component residuals. beta Estimated fixed effects coefficients. u Estimated random effects coefficients. LL Log-likelihood model. beta.SE Standard errors fixed effects coefficients (SE = TRUE). u.SE Standard errors random effects coefficients (SE = TRUE). Hinv Inverse H matrix (return_Hinv = TRUE).","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Mixed Model Solver — mixed_solve","text":"function fits mixed model: Y = X*beta + Z*u + error, u ~ N(0, Vu*K) error ~ N(0, Ve*). variance components Vu Ve estimated using REML ML.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mixed Model Solver — mixed_solve","text":"","code":"if (FALSE) { # \\dontrun{ # Example with random data set.seed(123) n <- 100 Y <- rnorm(n) Z <- matrix(rnorm(n * 5), n, 5) K <- diag(5) X <- matrix(1, n, 1)  # Fit mixed model result <- mixed_solve(Y, X, Z, K) } # }"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve_optimized.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimized Mixed Model Solver — mixed_solve_optimized","title":"Optimized Mixed Model Solver — mixed_solve_optimized","text":"optimized implementation mixed model estimation precomputes expensive matrix operations can reused across multiple voxels significant performance improvements.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve_optimized.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimized Mixed Model Solver — mixed_solve_optimized","text":"","code":"mixed_solve_optimized(   X,   Z,   Y,   K = NULL,   workspace = NULL,   compute_se = FALSE,   n_threads = 0 )"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve_optimized.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimized Mixed Model Solver — mixed_solve_optimized","text":"X Fixed effects design matrix (n × p) Z Random effects design matrix (n × q) Y Response data - can vector (single voxel) matrix (n × V multiple voxels) K Kinship/covariance matrix random effects (q × q). Defaults identity. workspace Precomputed workspace (optional, compute NULL) compute_se Whether compute standard errors (default: FALSE) n_threads Number OpenMP threads multi-voxel (0 = auto)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/mixed_solve_optimized.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimized Mixed Model Solver — mixed_solve_optimized","text":"List estimated parameters variance components","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/plot_hrf_comparison.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot HRF Recovery Comparison — plot_hrf_comparison","title":"Plot HRF Recovery Comparison — plot_hrf_comparison","text":"Creates visualization comparing true vs recovered HRFs","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/plot_hrf_comparison.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot HRF Recovery Comparison — plot_hrf_comparison","text":"","code":"plot_hrf_comparison(results, save_path = NULL)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/plot_hrf_comparison.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot HRF Recovery Comparison — plot_hrf_comparison","text":"results Output compare_hrf_recovery save_path Optional path save plot","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Project Out Confound Variables — project_confounds","title":"Project Out Confound Variables — project_confounds","text":"Computes orthogonal projection matrix Q = - X(X'X)^(-1)X' projects space spanned confound regressors X. useful advanced users want cache reuse projection matrices.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Project Out Confound Variables — project_confounds","text":"","code":"project_confounds(X)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Project Out Confound Variables — project_confounds","text":"X Confound design matrix (n x p) n number timepoints p number confound regressors","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Project Out Confound Variables — project_confounds","text":"Projection matrix Q (n x n) projects column space X","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Project Out Confound Variables — project_confounds","text":"function uses QR decomposition numerical stability instead computing Moore-Penrose pseudoinverse directly. resulting matrix Q can applied data remove influence confound regressors.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Project Out Confound Variables — project_confounds","text":"","code":"if (FALSE) { # \\dontrun{ # Create confound matrix (intercept + linear trend) n <- 100 X_confounds <- cbind(1, 1:n)  # Get projection matrix Q <- project_confounds(X_confounds)  # Apply to data to remove confounds Y_clean <- Q %*% Y_raw } # }"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds_cpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Project Out Confounds Using C++ — project_confounds_cpp","title":"Project Out Confounds Using C++ — project_confounds_cpp","text":"Fast C++ implementation projecting confound variables data trial design matrices. uses Cholesky decomposition numerical stability avoids creating large projection matrices.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds_cpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Project Out Confounds Using C++ — project_confounds_cpp","text":"","code":"project_confounds_cpp(X_confounds, Y_data, C_trials)"},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds_cpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Project Out Confounds Using C++ — project_confounds_cpp","text":"X_confounds Confound design matrix (n x k) Y_data Data matrix (n x V) V number voxels C_trials Trial design matrix (n x T) T number trials","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds_cpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Project Out Confounds Using C++ — project_confounds_cpp","text":"List projected data (residual_data) projected trials (Q_dmat_ran)","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds_cpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Project Out Confounds Using C++ — project_confounds_cpp","text":"function computes residuals Y - X(X'X)^(-1)X'Y C - X(X'X)^(-1)X'C without explicitly forming projection matrix Q = - X(X'X)^(-1)X'. approach uses ~100x less memory large n numerically stable.","code":""},{"path":"https://bbuchsbaum.github.io/fmrilss/reference/project_confounds_cpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Project Out Confounds Using C++ — project_confounds_cpp","text":"","code":"if (FALSE) { # \\dontrun{ n <- 200; k <- 5; V <- 1000; T <- 50 X_confounds <- cbind(1, 1:n, rnorm(n*3))  # intercept + trend + noise Y_data <- matrix(rnorm(n*V), n, V) C_trials <- matrix(rnorm(n*T), n, T)  result <- project_confounds_cpp(X_confounds, Y_data, C_trials) } # }"}]
