---
title: "Getting Started with fmrilss"
author: "fmrilss Development Team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with fmrilss}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Learning Objectives

After completing this vignette, you will be able to:

1. Understand the theoretical foundation of Least Squares Separate (LSS)
2. Implement LSS analysis using the `fmrilss` package
3. Choose appropriate computational backends for your data
4. Compare LSS with standard GLM approaches (LSA)
5. Handle nuisance regressors and experimental covariates
6. Optimize performance for large datasets

## Prerequisites

This vignette assumes basic familiarity with:
- fMRI analysis concepts (GLM, design matrices, HRF)
- R programming and matrix operations
- Basic statistics (linear regression)

## Introduction to Least Squares Separate (LSS)

The `fmrilss` package provides a fast and flexible implementation of the Least Squares Separate (LSS) modeling approach, first proposed by Mumford et al. (2012). LSS is a powerful method for analyzing event-related fMRI data, especially for studies involving multivariate pattern analysis (MVPA) or functional connectivity.

The core idea of LSS is to estimate a separate GLM for each trial. For a given trial, the model includes:

1.  A regressor for the **trial of interest**.
2.  A single regressor for **all other trials** combined.
3.  **Experimental regressors** that we want to model and estimate, but not trial-wise (e.g., intercept, condition effects, block effects).
4.  **Nuisance regressors** that we want to remove from the data before analysis (e.g., motion parameters, physiological noise).

This process is repeated for every trial, yielding a unique beta estimate for each one.

## The `lss()` Function

The main entry point to the package is the `lss()` function, which has a clean and modern interface:

```r
lss(Y, X, Z = NULL, Nuisance = NULL, method = "r_optimized")
```

-   `Y`: The data matrix (timepoints x voxels).
-   `X`: The trial design matrix (timepoints x trials).
-   `Z`: A matrix of experimental regressors we want to model and get beta estimates for, but not trial-wise (e.g., intercept, condition effects, block effects). Defaults to an intercept if `NULL`.
-   `Nuisance`: A matrix of nuisance regressors to be projected out before analysis (e.g., motion parameters, physiological noise).
-   `method`: The computational backend to use. Options range from a simple `naive` R loop to a highly optimized and parallelized `"cpp_optimized"` engine.

## A Practical Example

Let's walk through a complete example, from generating data to running the analysis.

### 1. Load the Package and Prepare Data

First, we load `fmrilss` and set up our parameters.

```{r setup}
library(fmrilss)

set.seed(42)
n_timepoints <- 150
n_trials <- 12
n_voxels <- 25
```

### 2. Create Design Matrices

Next, we create the core components of our model.

-   **Trial Matrix (`X`)**: A matrix where each column represents the predicted BOLD response for a single trial.
-   **Experimental Regressors (`Z`)**: A matrix for experimental regressors that we want to model and get beta estimates for, but not trial-wise. These might include condition effects, block effects, or other experimental factors that vary across the session but not trial-by-trial.
-   **Nuisance Matrix**: Regressors we want to remove from the data before modeling, such as motion parameters or physiological noise.

```{r design}
# Trial design matrix (X)
X <- matrix(0, n_timepoints, n_trials)
onsets <- seq(from = 10, to = n_timepoints - 12, length.out = n_trials)
for(i in 1:n_trials) {
  X[onsets[i]:(onsets[i] + 5), i] <- 1
}

# Experimental regressors (Z) - intercept and condition effects
# These are regressors we want to model and get estimates for, but not trial-wise
Z <- cbind(Intercept = 1, LinearTrend = scale(1:n_timepoints, center = TRUE, scale = FALSE))

# Nuisance regressors - e.g., 6 motion parameters
Nuisance <- matrix(rnorm(n_timepoints * 6), n_timepoints, 6)
```

### 3. Simulate Data (`Y`)

Now, we'll generate a synthetic data matrix `Y` that includes signals from our regressors plus some random noise.

```{r data}
# Simulate effects for each component
true_trial_betas <- matrix(rnorm(n_trials * n_voxels, 0, 1.2), n_trials, n_voxels)
true_fixed_effects <- matrix(rnorm(2 * n_voxels, c(5, -0.1), 0.5), 2, n_voxels)
true_nuisance_effects <- matrix(rnorm(6 * n_voxels, 0, 2), 6, n_voxels)

# Combine signals and add noise
Y <- (Z %*% true_fixed_effects) + 
     (X %*% true_trial_betas) +
     (Nuisance %*% true_nuisance_effects) +
     matrix(rnorm(n_timepoints * n_voxels, 0, 1), n_timepoints, n_voxels)
```

### 4. Run the LSS Analysis

With our data prepared, we can now run the `lss` analysis.

#### Basic Usage
If you only provide the data `Y` and the trial matrix `X`, the function will automatically include an intercept.

```{r lss_basic}
beta_basic <- lss(Y, X)
# The result is a trials-by-voxels matrix
dim(beta_basic)
```

#### Including Experimental Regressors and Nuisance Regressors
For a more realistic analysis, we include our `Z` and `Nuisance` matrices. The nuisance regressors are projected out of both `Y` and `X` before the trial-wise GLMs are estimated, while the experimental regressors in `Z` are included in every trial-wise GLM to get their beta estimates.

```{r lss_advanced}
beta_full <- lss(Y, X, Z = Z, Nuisance = Nuisance)
# The output dimensions remain the same
dim(beta_full)
```

### 5. Choosing a High-Performance Method

While the default R implementation is well-optimized, the C++ backend offers a significant speedup, especially for large datasets. It is also parallelized with OpenMP to use multiple CPU cores. To use it, simply set `method = "cpp_optimized"`.

```{r lss_cpp, eval=require("Rcpp") && require("RcppArmadillo")}
# Run the same analysis with the high-performance C++ engine
beta_fast <- lss(Y, X, Z = Z, Nuisance = Nuisance, method = "cpp_optimized")

# The results are numerically identical to the R version
all.equal(beta_full, beta_fast, tolerance = 1e-8)
```

This makes it easy to switch between a readable R implementation and a high-performance C++ engine without changing any other code.

## Comparing LSS with LSA (Least Squares All)

While LSS estimates separate models for each trial, the traditional Least Squares All (LSA) approach estimates all trials simultaneously in a single model. Let's compare these approaches:

```{r lsa-comparison}
# LSA: Standard GLM with all trials in one model
beta_lsa <- lsa(Y, X, Z = Z, Nuisance = Nuisance)

# Compare dimensions
cat("LSS beta dimensions:", dim(beta_full), "\n")
cat("LSA beta dimensions:", dim(beta_lsa), "\n")

# Compare variance in beta estimates
var_lss <- apply(beta_full, 2, var)
var_lsa <- apply(beta_lsa, 2, var)

cat("\nMean variance across voxels:\n")
cat("  LSS:", mean(var_lss), "\n")
cat("  LSA:", mean(var_lsa), "\n")

# Plot comparison
par(mfrow = c(1, 2))
hist(beta_full[, 1], main = "LSS: Beta distribution (Voxel 1)", 
     xlab = "Beta values", col = "lightblue")
hist(beta_lsa[, 1], main = "LSA: Beta distribution (Voxel 1)", 
     xlab = "Beta values", col = "lightgreen")
```

### When to Use LSS vs LSA

| Scenario | Recommended Method | Reason |
|----------|-------------------|---------|
| MVPA analysis | LSS | Reduces collinearity between trial estimates |
| Rapid event-related designs | LSS | Better handles overlapping HRFs |
| Block designs | LSA | Sufficient trial separation |
| Connectivity analysis | LSS | Provides trial-specific connectivity estimates |
| Group-level contrasts | LSA | More stable for averaging |

## Introduction to OASIS Method

The OASIS (Optimized Acquisition and Stimulus Sequencing) method is a powerful extension that provides:
- Automatic HRF estimation
- Ridge regularization for stability
- Efficient computation for complex designs
- Built-in support for multi-basis HRFs

```{r oasis-intro}
# Basic OASIS usage
beta_oasis <- lss(
  Y = Y,
  X = NULL,  # OASIS builds design internally
  method = "oasis",
  oasis = list(
    design_spec = list(
      sframe = sampling_frame(blocklens = nrow(Y), TR = 1),
      cond = list(
        onsets = seq(10, 290, by = 30),  # Event onsets
        hrf = HRF_SPMG1,  # HRF model
        span = 30
      )
    ),
    ridge_mode = "fractional",
    ridge_x = 0.01,
    ridge_b = 0.01
  )
)

cat("OASIS beta dimensions:", dim(beta_oasis), "\n")
```

## Working with Different Backends

The package provides multiple computational backends optimized for different scenarios:

```{r backend-comparison, eval=FALSE}
# Benchmark different methods
library(microbenchmark)

methods <- c("naive", "r_vectorized", "r_optimized", "cpp_optimized")
timings <- list()

for (m in methods) {
  timings[[m]] <- system.time({
    lss(Y, X, method = m)
  })[3]
}

# Display timing comparison
timing_df <- data.frame(
  Method = methods,
  Time = unlist(timings)
)
print(timing_df)

# For large datasets, use parallel processing
if (require("parallel")) {
  n_cores <- detectCores() - 1
  beta_parallel <- lss(Y, X, method = "r_optimized", parallel = TRUE, ncores = n_cores)
}
```

## Handling Complex Experimental Designs

### Multiple Conditions

```{r multiple-conditions}
# Create design with multiple conditions
n_cond <- 3
X_multi <- matrix(0, n_time, n_trials * n_cond)

for (c in 1:n_cond) {
  trial_idx <- ((c-1) * n_trials + 1):(c * n_trials)
  for (i in 1:n_trials) {
    onset <- 10 + (trial_idx[i] - 1) * 20
    if (onset + 9 <= n_time) {
      X_multi[onset:(onset + 9), trial_idx[i]] <- hrf
    }
  }
}

# Add condition labels as experimental regressors
Z_cond <- matrix(0, n_time, n_cond)
for (c in 1:n_cond) {
  trial_idx <- ((c-1) * n_trials + 1):(c * n_trials)
  Z_cond[, c] <- rowSums(X_multi[, trial_idx, drop = FALSE])
}

# Run LSS with condition regressors
beta_multi <- lss(Y, X_multi, Z = Z_cond, method = "r_optimized")
```

### Parametric Modulations

```{r parametric-modulation}
# Add parametric modulator (e.g., reaction time, stimulus intensity)
modulator <- rnorm(n_trials, mean = 0, sd = 1)

# Create parametrically modulated design
X_param <- X
for (i in 1:n_trials) {
  X_param[, i] <- X[, i] * modulator[i]
}

# Run LSS with parametric modulation
beta_param <- lss(Y, X_param, Z = Z, method = "r_optimized")

# Compare with unmodulated
cor_unmod <- cor(as.vector(beta_full), as.vector(Y %*% t(X)))
cor_param <- cor(as.vector(beta_param), as.vector(Y %*% t(X_param)))

cat("Correlation with signal:\n")
cat("  Unmodulated:", round(cor_unmod, 3), "\n")
cat("  Parametric:", round(cor_param, 3), "\n")
```

## Performance Optimization Tips

### 1. Memory Management

```{r memory-tips, eval=FALSE}
# For very large datasets, process in chunks
chunk_size <- 1000
n_chunks <- ceiling(ncol(Y) / chunk_size)

beta_chunks <- list()
for (chunk in 1:n_chunks) {
  voxel_idx <- ((chunk - 1) * chunk_size + 1):min(chunk * chunk_size, ncol(Y))
  beta_chunks[[chunk]] <- lss(Y[, voxel_idx], X, method = "cpp_optimized")
}

# Combine results
beta_full <- do.call(cbind, beta_chunks)
```

### 2. Preprocessing for Speed

```{r preprocessing}
# Project out nuisance before LSS (when appropriate)
Y_clean <- project_confounds(Y, Nuisance)

# This can be faster than including Nuisance in each LSS iteration
beta_preprocessed <- lss(Y_clean, X, Z = Z, method = "r_optimized")
```

### 3. Choose the Right Method

```{r method-selection, eval=FALSE}
# Decision tree for method selection
n_voxels <- ncol(Y)
n_timepoints <- nrow(Y)

if (n_voxels < 100) {
  method <- "r_optimized"  # Fast enough for small data
} else if (n_voxels < 10000) {
  method <- "cpp_optimized"  # C++ for medium data
} else {
  method <- "oasis"  # OASIS for large data
}

cat("Recommended method for", n_voxels, "voxels:", method, "\n")
```

## Troubleshooting Common Issues

### Issue 1: Singular Design Matrix

```{r troubleshoot-singular, eval=FALSE}
# Check for collinearity
cor_matrix <- cor(X)
high_cor <- which(abs(cor_matrix) > 0.9 & cor_matrix != 1, arr.ind = TRUE)

if (nrow(high_cor) > 0) {
  warning("High correlation between regressors detected")
  # Consider using ridge regression via OASIS
  beta_ridge <- lss(Y, X, method = "oasis", 
                    oasis = list(ridge_mode = "absolute", ridge_x = 0.1))
}
```

### Issue 2: Memory Errors

```{r troubleshoot-memory, eval=FALSE}
# Monitor memory usage
mem_required <- object.size(Y) * n_trials * 2  # Rough estimate
mem_available <- memory.limit()  # Windows only

if (mem_required > mem_available * 0.8) {
  warning("May run out of memory. Consider chunking or using OASIS.")
}
```

### Issue 3: Slow Performance

```{r troubleshoot-speed, eval=FALSE}
# Profile code to find bottlenecks
Rprof("lss_profile.out")
beta_slow <- lss(Y, X, method = "naive")
Rprof(NULL)
summaryRprof("lss_profile.out")
```

## Advanced Topics

For more advanced usage, see:
- `vignette("voxel-wise-hrf")` - Voxel-specific HRF modeling
- `vignette("oasis_method")` - Complete OASIS documentation
- `vignette("mixed_models")` - Mixed effects models with LSS

## Summary

This vignette covered:
1. Basic LSS implementation and theory
2. Comparison with LSA
3. Introduction to OASIS method
4. Different computational backends
5. Complex experimental designs
6. Performance optimization
7. Troubleshooting

The `fmrilss` package provides a comprehensive toolkit for trial-wise beta estimation with options ranging from simple to highly optimized implementations.

## References

Mumford, J. A., Turner, B. O., Ashby, F. G., & Poldrack, R. A. (2012). Deconvolving BOLD activation in event-related designs for multivoxel pattern classification analyses. *NeuroImage*, 59(3), 2636-2643. 