---
title: 'Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific HRF Estimation'
author: fmrilss Development Team
date: '`r Sys.Date()`'
output:
  rmarkdown::html_vignette:
    mathjax: default
vignette: '%\VignetteIndexEntry{Shared-Basis HRF Matching (SBHM): Efficient Voxel-Specific
  HRF Estimation} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}'
params:
  family: red
css: albers.css
resource_files:
- albers.css
- albers.js
includes:
  in_header: |-
    <script src="albers.js"></script>
    <script>document.addEventListener('DOMContentLoaded',function(){document.body.classList.add('palette-red');});</script>

---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6,
  dev = "png",
  dpi = 150,
  warning = FALSE,
  message = FALSE
)

# Make vignette render in pkgdown by using a lightweight fast mode
fast_mode <- identical(Sys.getenv("IN_PKGDOWN"), "true") || identical(Sys.getenv("CI"), "true")

# Always evaluate chunks, but keep computations small in fast mode
eval_chunks <- TRUE

# Global settings used throughout
TR <- 1.0
n_time <- if (fast_mode) 160 else 200
set.seed(123)
```

## The Problem

Real fMRI data shows substantial HRF variability across brain regions. A single
canonical HRF often fits poorly, but estimating fully unconstrained voxel-wise
HRFs is expensive and noisy. SBHM offers a middle path: learn a shared basis
from a library of plausible HRFs, then match each voxel to its best library
member in a low-dimensional space.

The pipeline has four steps:

1. **Build** a shared basis from an HRF library via SVD (`sbhm_build()`)
2. **Prepass** aggregate model fit per voxel (`sbhm_prepass()`)
3. **Match** voxels to library members by cosine similarity (`sbhm_match()`)
4. **Project** trial-wise coefficients to scalar amplitudes (`sbhm_project()`)

In practice, `lss_sbhm()` runs all four steps in a single call.

```{r setup, message=FALSE, warning=FALSE}
if (requireNamespace("ggplot2", quietly = TRUE) && requireNamespace("albersdown", quietly = TRUE)) ggplot2::theme_set(albersdown::theme_albers(params$family))
library(fmrihrf)
library(fmrilss)

# Derived convenience defaults for examples
n_voxels_default <- if (fast_mode) 20 else 50
n_trials_default <- if (fast_mode) 6 else 10
ranks_default    <- if (fast_mode) 2:6 else 2:10
```

## Step 1: Build the Shared Basis

We start by defining a grid of gamma HRF parameters spanning physiological
variability in peak time and width.

```{r param-grid, eval=eval_chunks}
shapes <- if (fast_mode) seq(6, 10, by = 2) else seq(5, 11, by = 1.5)
rates  <- if (fast_mode) seq(0.8, 1.2, by = 0.2) else seq(0.7, 1.3, by = 0.15)
param_grid <- expand.grid(shape = shapes, rate = rates)
cat("Library size:", nrow(param_grid), "HRFs\n")
```

Each grid row becomes a gamma HRF. We wrap this in a factory function that
`sbhm_build()` can evaluate across the grid.

```{r gamma-fun, eval=eval_chunks}
gamma_fun <- function(shape, rate) {
  f <- function(t) fmrihrf::hrf_gamma(t, shape = shape, rate = rate)
  fmrihrf::as_hrf(f, name = sprintf("gamma(s=%.2f,r=%.2f)", shape, rate), span = 32)
}
```

Now we build the SBHM basis. The SVD decomposes the library into a shared time
basis `B`, singular values `S`, and per-HRF coordinates `A`.

```{r build-sbhm, eval=eval_chunks}
sframe <- sampling_frame(blocklens = n_time, TR = TR)
sbhm <- sbhm_build(
  library_spec = list(fun = gamma_fun, pgrid = param_grid, span = 32,
                      precision = 0.1, method = "conv"),
  r = 6, sframe = sframe,
  baseline = c(0, 0.5), normalize = TRUE, ref = "mean"
)
```

```{r report-dims, eval=eval_chunks}
cat("B (time basis):", dim(sbhm$B), "\n")
cat("A (library coords):", dim(sbhm$A), "\n")
cat("Singular values:", round(sbhm$S, 2), "\n")
```

### Visualizing the Basis

Each basis function captures a principal mode of HRF variation: the main shape,
timing shifts, width differences, and undershoot features.

```{r visualize-basis, fig.width=10, fig.height=6, eval=eval_chunks}
par(mfrow = c(2, 3), mar = c(3, 3, 2, 1))
for (i in 1:ncol(sbhm$B)) {
  plot(sbhm$tgrid, sbhm$B[, i], type = "l", col = "navy", lwd = 2,
       main = paste0("Basis ", i, " (s=", round(sbhm$S[i], 2), ")"),
       xlab = "Time (s)", ylab = "Amplitude")
  abline(h = 0, col = "gray", lty = 2)
}
```

### Choosing the Rank

Aim for 90--95% variance explained. We can sweep ranks to find the elbow.

```{r choose-rank, fig.width=8, fig.height=5, eval=eval_chunks}
ranks <- ranks_default
ve <- sapply(ranks, function(ri) {
  sum(sbhm_build(library_spec = list(fun = gamma_fun, pgrid = param_grid, span = 32),
                 r = ri, sframe = sframe, normalize = TRUE)$S^2)
})
```

```{r plot-rank, fig.width=8, fig.height=5, eval=eval_chunks}
plot(ranks, ve / max(ve) * 100, type = "b", pch = 19, col = "navy",
     lwd = 2, xlab = "Rank (r)", ylab = "Variance Explained (%)",
     main = "Choosing SBHM Rank")
abline(h = 95, col = "red", lty = 2)
grid()
```

### Library Coverage

A sample of rank-r reconstructed HRFs shows the range of shapes the library
spans.

```{r library-coverage, fig.width=8, fig.height=5, eval=eval_chunks}
H_hat <- sbhm$B %*% sbhm$A
sel <- unique(round(seq(1, ncol(H_hat), length.out = min(ncol(H_hat), 12))))
matplot(sbhm$tgrid, H_hat[, sel, drop = FALSE], type = "l", lty = 1,
        col = colorRampPalette(c("#6baed6", "#08519c"))(length(sel)),
        lwd = 1.5, xlab = "Time (s)", ylab = "Amplitude",
        main = "Sample of Library HRFs (rank-r reconstruction)")
abline(h = 0, col = "gray80", lty = 2)
```

## Step 2: Generate Synthetic Data

To demonstrate recovery, we create data where each voxel uses a known HRF from
the library. First, set up the experimental design.

```{r design-setup, eval=eval_chunks}
n_voxels <- n_voxels_default
n_trials <- n_trials_default
safe_end <- max(sbhm$tgrid) - 30
onsets <- seq(20, safe_end, length.out = n_trials)
```

Assign each voxel a random library HRF and build per-trial regressors.

```{r assign-hrfs, eval=eval_chunks}
set.seed(456)
true_hrf_idx <- sample(ncol(sbhm$A), n_voxels, replace = TRUE)
design_spec <- list(
  sframe = sframe,
  cond = list(onsets = onsets, duration = 0, span = 30)
)
hrf_B <- sbhm_hrf(sbhm$B, sbhm$tgrid, sbhm$span)
```

```{r build-regressors, eval=eval_chunks}
regressors_by_trial <- lapply(onsets, function(ot) {
  rr_t <- regressor(onsets = ot, hrf = hrf_B, duration = 0, span = 30, summate = FALSE)
  evaluate(rr_t, grid = sbhm$tgrid, precision = 0.1, method = "conv")
})
```

Now generate the signal. Each voxel's response is the sum of per-trial
regressors projected through that voxel's HRF coordinates, scaled by a random
amplitude, plus noise.

```{r generate-signal, eval=eval_chunks}
Y <- matrix(rnorm(n_time * n_voxels, sd = 0.5), n_time, n_voxels)
true_amplitudes <- matrix(rnorm(n_trials * n_voxels, mean = 2, sd = 0.5),
                          n_trials, n_voxels)
for (v in 1:n_voxels) {
  alpha_true <- sbhm$A[, true_hrf_idx[v]]
  for (t in 1:n_trials)
    Y[, v] <- Y[, v] + true_amplitudes[t, v] * (regressors_by_trial[[t]] %*% alpha_true)
}
```

```{r data-summary, eval=eval_chunks}
cat("Y:", dim(Y), "\n")
cat("Unique true HRFs:", length(unique(true_hrf_idx)), "\n")
```

## Step 3: Run the SBHM Pipeline

A single call to `lss_sbhm()` runs the entire pipeline. All control lists
(`prepass`, `match`, `oasis`, `amplitude`) are optional; pass only what you want
to override. See `?lss_sbhm` for full parameter documentation.

```{r run-sbhm, eval=eval_chunks}
res_sbhm <- lss_sbhm(
  Y = Y, sbhm = sbhm, design_spec = design_spec,
  return = "both"
)
```

```{r sbhm-dims, eval=eval_chunks}
cat("Amplitudes:", dim(res_sbhm$amplitude), "\n")
cat("Coefficients:", dim(res_sbhm$coeffs_r), "\n")
cat("Matched indices:", length(res_sbhm$matched_idx), "\n")
```

If you use `fmridesign`, prefer `lss_sbhm_design()` for an even simpler
interface that mirrors `lss_design()`. See `?lss_sbhm_design`.

## Step 4: Evaluate Recovery

### Matching Accuracy

```{r evaluate-recovery, eval=eval_chunks}
accuracy <- mean(res_sbhm$matched_idx == true_hrf_idx)
cat("HRF matching accuracy:", round(100 * accuracy, 1), "%\n")
cat("Confused voxels:", sum(res_sbhm$matched_idx != true_hrf_idx),
    "/", n_voxels, "\n")
```

### Matching Confidence

The `margin` (top-1 minus top-2 cosine score) indicates how unambiguous each
assignment is. Higher is better.

```{r margin-stats, eval=eval_chunks}
cat("Margin -- mean:", round(mean(res_sbhm$margin), 3),
    " median:", round(median(res_sbhm$margin), 3), "\n")
```

```{r visualize-margin, fig.width=8, fig.height=5, eval=eval_chunks}
hist(res_sbhm$margin, breaks = 20, col = "skyblue", border = "white",
     main = "Matching Confidence (Margin)",
     xlab = "Margin (top1 - top2 cosine score)")
abline(v = median(res_sbhm$margin), col = "red", lwd = 2, lty = 2)
grid()
```

Rules of thumb: margin > 0.1 means high confidence; below 0.05 the top
candidates are nearly interchangeable, and soft blending (see below) may help.

### Amplitude Recovery

```{r compare-amplitudes, fig.width=8, fig.height=5, eval=eval_chunks}
cor_amp <- cor(as.vector(res_sbhm$amplitude), as.vector(true_amplitudes))
plot(as.vector(true_amplitudes), as.vector(res_sbhm$amplitude),
     pch = 19, col = adjustcolor("navy", alpha.f = 0.3),
     xlab = "True Amplitude", ylab = "Estimated Amplitude",
     main = paste0("Amplitude Recovery (r = ", round(cor_amp, 3), ")"))
abline(0, 1, col = "red", lwd = 2, lty = 2)
grid()
```

### Recovered HRF Shapes

For the most confidently matched voxels, the recovered and true HRFs should
overlap closely.

```{r hrf-prep, eval=eval_chunks}
H_hat <- sbhm$B %*% sbhm$A
vox_show <- head(order(-res_sbhm$margin), n = min(6, n_voxels))
```

```{r matched-vs-true, fig.width=10, fig.height=6, eval=eval_chunks}
par(mfrow = c(2, 3), mar = c(3, 3, 2, 1))
for (v in vox_show) {
  rng <- range(c(H_hat[, true_hrf_idx[v]], H_hat[, res_sbhm$matched_idx[v]]))
  plot(sbhm$tgrid, H_hat[, true_hrf_idx[v]], type = "l", col = "#2c7fb8",
       lwd = 2, ylim = rng, main = paste0("Voxel ", v), xlab = "Time (s)", ylab = "HRF")
  lines(sbhm$tgrid, H_hat[, res_sbhm$matched_idx[v]], col = "#d95f02", lwd = 2, lty = 2)
  legend("topright", bty = "n", cex = 0.8, legend = c("True", "Matched"),
         lty = 1:2, lwd = 2, col = c("#2c7fb8", "#d95f02"))
}
```

### Library Manifold

PCA of the library coordinates shows which HRFs were present (true) versus
selected (matched).

```{r pca-library, fig.width=8, fig.height=5, eval=eval_chunks}
pca <- prcomp(t(sbhm$A), center = TRUE, scale. = TRUE)
pc <- pca$x[, 1:2, drop = FALSE]
plot(pc, pch = 16, col = "gray70", xlab = "PC1", ylab = "PC2",
     main = "Library Coordinate Space (PCA)")
points(pc[unique(true_hrf_idx), , drop = FALSE], pch = 1, col = "#2c7fb8", lwd = 2)
points(pc[unique(res_sbhm$matched_idx), , drop = FALSE], pch = 4, col = "#d95f02", lwd = 2)
legend("topright", bty = "n",
       legend = c("Library", "True", "Matched"),
       pch = c(16, 1, 4), col = c("gray60", "#2c7fb8", "#d95f02"))
```

## Soft Assignment for Ambiguous Voxels

Hard assignment picks the single best HRF per voxel. When the margin is small,
blending the top-K candidates can reduce variance. The built-in approach
requires just two extra arguments.

```{r soft-blend, eval=eval_chunks}
res_soft <- lss_sbhm(
  Y = Y, sbhm = sbhm, design_spec = design_spec,
  match = list(topK = 3, soft_blend = TRUE,
               blend_margin = median(res_sbhm$margin)),
  return = "amplitude"
)
```

```{r soft-compare, eval=eval_chunks}
cor_sv <- cor(as.vector(res_soft$amplitude), as.vector(res_sbhm$amplitude))
cat("Soft vs hard amplitude correlation:", round(cor_sv, 3), "\n")
```

Blending only applies to voxels whose margin falls below `blend_margin`;
confident voxels keep their hard assignment. For manual control over the
blending weights, use `sbhm_prepass()`, `sbhm_match(topK = K)`, and
`sbhm_project()` directly. See `?sbhm_match` for details.

## Amplitude Policy

The final amplitude stage estimates per-trial scalars from the matched HRF. The
`amplitude$method` argument selects the estimator:
`"global_ls"` (default, fast, ridge-regularized GLM),
`"lss1"` (per-trial 2x2 LSS, better under strong overlap), or
`"oasis_voxel"` (full OASIS per voxel, heaviest but returns SEs).

```{r amplitude-policy, eval=FALSE}
out <- lss_sbhm(
  Y, sbhm, design_spec,
  amplitude = list(method = "global_ls",
                   ridge = list(mode = "fractional", lambda = 0.02),
                   cond_gate = list(metric = "rho", thr = 0.999, fallback = "lss1"))
)
```

See `?lss_sbhm` for guidance on choosing by ISI regime (slow, moderate, fast ER).

## Returning Coefficients for Custom Analysis

When you need the full r-dimensional trial-wise coefficients instead of scalar
amplitudes, request `return = "coefficients"`.

```{r coeffs-demo, eval=eval_chunks}
res_c <- lss_sbhm(Y = Y[, 1:5], sbhm = sbhm,
                   design_spec = design_spec, return = "coefficients")
cat("Coefficients:", dim(res_c$coeffs_r), " [r x trials x voxels]\n")
```

## Parameter Quick Reference

| Parameter | Default | Recommended | Notes |
|-----------|---------|-------------|-------|
| `r` (rank) | -- | 6--12 | Aim for 90--95% variance explained |
| `topK` | 1 | 1--5 | Use 3--5 with `soft_blend = TRUE` for ambiguous cases |
| `prepass$ridge` | NULL | `list(mode = "fractional", lambda = 0.01)` | Stabilizes noisy/collinear designs |
| `match$shrink$tau` | 0 | 0--0.2 | Increase for low SNR |
| `match$whiten` | TRUE | TRUE | Equalizes all basis dimensions in matching |
| `prewhiten` | NULL | `list(method = "ar", p = 1L)` | Use for TR < 2s |
| `amplitude$method` | `"global_ls"` | varies by ISI | See `?lss_sbhm` |

For full details on every parameter, see `?sbhm_build`, `?sbhm_match`, and
`?lss_sbhm`.

## Performance Considerations

SBHM's cost scales as O(T*r*N*V) for the LSS step, where N is the number of
trials. Compared to unconstrained voxel-wise HRF estimation this is typically
10--50x faster, since r << K*N.

For very large datasets (V > 50,000), PCA factorization reduces memory by
fitting the prepass on q "meta-voxels" instead of V voxels.

```{r factorized, eval=FALSE}
pca_Y <- prcomp(Y, center = TRUE, rank. = 100)
res <- lss_sbhm(
  Y = pca_Y$x, sbhm = sbhm, design_spec = design_spec,
  prepass = list(data_fac = list(scores = pca_Y$x, loadings = pca_Y$rotation))
)
```

For ROI-based analyses, simply subset columns of Y before calling `lss_sbhm()`.
A benchmark script is available via
`system.file("benchmarks", "bench_sbhm.R", package = "fmrilss")`.

## Prewhitening

If your TR is short (< 2s) or residuals show autocorrelation, add prewhitening
to the final OASIS step.

```{r prewhiten-example, eval=FALSE}
res_pw <- lss_sbhm(
  Y = Y, sbhm = sbhm, design_spec = design_spec,
  prewhiten = list(method = "ar", p = 1L, pooling = "global", exact_first = "ar1")
)
```

Note: the factorized prepass intentionally skips prewhitening for efficiency.
The final OASIS step still applies it when `prewhiten` is provided.

## References

- Mumford et al. (2012). "Deconvolving BOLD activation in event-related designs for multivoxel pattern classification analyses." *NeuroImage*.
- Lindquist et al. (2009). "Modeling the hemodynamic response function in fMRI." *NeuroImage*.

## Summary

SBHM provides efficient, interpretable voxel-specific HRF estimation by
learning a shared basis from a plausible library and matching each voxel in a
low-dimensional coefficient space. The `lss_sbhm()` function runs the full
pipeline in a single call, returning trial-wise amplitudes with per-voxel HRF
assignments and confidence scores.

**Next steps:**
see `?sbhm_build` for library construction,
`?lss_sbhm` for the end-to-end pipeline,
and the "Voxel-wise HRF" vignette for unconstrained alternatives.
